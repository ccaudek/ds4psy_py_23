<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Data Science per psicologi - 27&nbsp; Regressione lineare bivariata</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./053_reglin3.html" rel="next">
<link href="./051_reglin1.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"><div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Regressione lineare bivariata</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Data Science per psicologi</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/ccaudek/data_science/" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Benvenuti</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">Prefazione</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./basics.html" class="sidebar-item-text sidebar-link">Parte 1: Nozioni di base</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./001_key_notions.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Concetti chiave</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./005_measurement.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">La misurazione in psicologia</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./007_freq_distr.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Analisi esplorativa dei dati</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./011_loc_scale.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Indici di posizione e di scala</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./012_correlation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Le relazioni tra variabili</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./013_penguins.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Manipolazione e visualizzazione dei dati in <span class="math inline">\(\mathsf{R}\)</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./prob.html" class="sidebar-item-text sidebar-link">Parte 2: Il calcolo delle probabilità</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./015_prob_intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">La logica dell’incerto</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./016_conditional_prob.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Probabilità condizionata: significato, teoremi, eventi indipendenti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./017_bayes_theorem.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Il teorema di Bayes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./018_expval_var.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Indici di posizione, di varianza e di associazione di variabili casuali</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./019_joint_prob.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Probabilità congiunta</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./020_density_func.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">La densità di probabilità</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./distr.html" class="sidebar-item-text sidebar-link">Parte 3: Distribuzioni di v.c. discrete e continue</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./022_discr_rv_distr.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. discrete</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./023_cont_rv_distr.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. continue</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./024_likelihood.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">La funzione di verosimiglianza</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./bayes_inference.html" class="sidebar-item-text sidebar-link">Parte 4: Inferenza bayesiana</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./025_intro_bayes.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Credibilità, modelli e parametri</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./026_subj_prop.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Pensare ad una proporzione in termini soggettivi</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./029_conjugate_families.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./030_balance_prior_post.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">L’influenza della distribuzione a priori</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./036_posterior_sim.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Approssimazione della distribuzione a posteriori</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./040_beta_binomial_mod.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Il modello beta-binomiale in linguaggio Stan</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./041_mcmc_diagnostics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Diagnostica delle catene markoviane</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./045_summarize_posterior.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Sintesi a posteriori</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./046_bayesian_prediction.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">La predizione bayesiana</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./050_normal_normal_mod.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Inferenza sul parametro <span class="math inline">\(\mu\)</span> (media di una v.c. Normale)</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./regression.html" class="sidebar-item-text sidebar-link">Parte 5: Regressione lineare</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./051_reglin1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./052_reglin2.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Regressione lineare bivariata</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./053_reglin3.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Modello di regressione in linguaggio Stan</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./054_reglin4.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Inferenza sul modello lineare</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./055_reglin5.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Confronto tra due gruppi indipendenti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./056_pred_check.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Predictive checks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./060_anova.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Confronto tra le medie di tre o più gruppi</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./070_mod_hier.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Modello gerarchico</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./071_mod_hier_sim.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Modello gerarchico: simulazioni</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./entropy.html" class="sidebar-item-text sidebar-link">Parte 6: Entropia</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./090_entropy.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Entropia</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./091_kl.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">La divergenza di Kullback-Leibler</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./092_info_criterion.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Criterio di informazione e convalida incrociata</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./frequentist_inference.html" class="sidebar-item-text sidebar-link">Parte 7: Inferenza frequentista</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./220_intro_frequentist.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Legge dei grandi numeri</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./221_conf_interv.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Intervallo fiduciale</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./225_distr_camp_mean.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Distribuzione campionaria</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./226_test_ipotesi.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Significatività statistica</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./227_ttest.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Inferenza sulle medie</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./228_limiti_stat_frequentista.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Limiti dell’inferenza frequentista</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./999_refs.html" class="sidebar-item-text sidebar-link">Riferimenti bibliografici</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">Appendici</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a01_math_symbols.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Simbologia di base</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a02_number_sets.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Numeri binari, interi, razionali, irrazionali e reali</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a03_set_theory.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Insiemi</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a04_summation_notation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Simbolo di somma (sommatorie)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a05_calculus_notation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Per liberarvi dai terrori preliminari</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a10_markov_chains.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Le catene di Markov</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a15_stan_lang.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Programmare in Stan</span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><!-- margin-sidebar --><div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Sommario</h2>
   
  <ul>
<li>
<a href="#stima-dei-coefficienti-di-regressione" id="toc-stima-dei-coefficienti-di-regressione" class="nav-link active" data-scroll-target="#stima-dei-coefficienti-di-regressione"><span class="toc-section-number">27.1</span>  Stima dei coefficienti di regressione</a>
  <ul class="collapse">
<li><a href="#trasformazione-dei-dati" id="toc-trasformazione-dei-dati" class="nav-link" data-scroll-target="#trasformazione-dei-dati"><span class="toc-section-number">27.1.1</span>  Trasformazione dei dati</a></li>
  <li><a href="#il-metodo-dei-minimi-quadrati" id="toc-il-metodo-dei-minimi-quadrati" class="nav-link" data-scroll-target="#il-metodo-dei-minimi-quadrati"><span class="toc-section-number">27.1.2</span>  Il metodo dei minimi quadrati</a></li>
  <li><a href="#lerrore-standard-della-regressione" id="toc-lerrore-standard-della-regressione" class="nav-link" data-scroll-target="#lerrore-standard-della-regressione"><span class="toc-section-number">27.1.3</span>  L’errore standard della regressione</a></li>
  </ul>
</li>
  <li>
<a href="#indice-di-determinazione" id="toc-indice-di-determinazione" class="nav-link" data-scroll-target="#indice-di-determinazione"><span class="toc-section-number">27.2</span>  Indice di determinazione</a>
  <ul class="collapse">
<li><a href="#inferenza-sul-modello-di-regressione" id="toc-inferenza-sul-modello-di-regressione" class="nav-link" data-scroll-target="#inferenza-sul-modello-di-regressione"><span class="toc-section-number">27.2.1</span>  Inferenza sul modello di regressione</a></li>
  </ul>
</li>
  <li><a href="#commenti-e-considerazioni-finali" id="toc-commenti-e-considerazioni-finali" class="nav-link" data-scroll-target="#commenti-e-considerazioni-finali">Commenti e considerazioni finali</a></li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-regr-model-lm" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Regressione lineare bivariata</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Codice</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header><div class="cell">

</div>
<p>In questo Capitolo verrà discusso il modello di regressione bivariata, ovvero il modello che, mediante una relazione lineare, predice una variabile continua <span class="math inline">\(y\)</span> a partire da un unico predittore continuo <span class="math inline">\(x\)</span>. Ciò corrisponde ad adattare ai dati (<span class="math inline">\(x_i, y_i\)</span>) la retta di regressione <span class="math inline">\(y_i = a + bx_i + e_i\)</span>, con <span class="math inline">\(i=1, \dots, n\)</span>. Usando dei dati reali, vedremo come stimare i coefficienti di regressione <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span>, e come essi possono essere interpretati. Vedremo anche come si può descrivere la bontà di adattamento del modello ai dati.</p>
<p>Nell’esempio che discuteremo in questo Capitolo verranno usati i dati <code>kidiq</code>:</p>
<blockquote class="blockquote">
<p>Data from a survey of adult American women and their children (a subsample from the National Longitudinal Survey of Youth).</p>
</blockquote>
<blockquote class="blockquote">
<p>Source: Gelman and Hill (2007)</p>
</blockquote>
<blockquote class="blockquote">
<p>434 obs. of 4 variables</p>
</blockquote>
<blockquote class="blockquote">
<ul>
<li>kid_score Child’s IQ score</li>
<li>mom_hs Indicator for whether the mother has a high school degree</li>
<li>mom_iq Mother’s IQ score</li>
<li>mom_age Mother’s age</li>
</ul>
</blockquote>
<p>Nel presente Capitolo esaminerò la relazione tra l’intelligenza del bambino (<code>kid_score</code>) e l’intelligenza della madre (<code>mom_iq</code>). Mi chiederò se, e in che misura, l’intelligenza della madre sia in grado di predire l’intelligenza del bambino.</p>
<p>Per iniziare, leggo i dati in <span class="math inline">\(\mathsf{R}\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">kidiq</span> <span class="op">&lt;-</span> <span class="fu">rio</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/rio/man/import.html">import</a></span><span class="op">(</span><span class="fu">here</span><span class="fu">::</span><span class="fu"><a href="https://here.r-lib.org//reference/here.html">here</a></span><span class="op">(</span></span>
<span>  <span class="st">"data"</span>, <span class="st">"kidiq.dta"</span></span>
<span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu">glimpse</span><span class="op">(</span><span class="va">kidiq</span><span class="op">)</span></span>
<span><span class="co">#&gt; Rows: 434</span></span>
<span><span class="co">#&gt; Columns: 5</span></span>
<span><span class="co">#&gt; $ kid_score &lt;dbl&gt; 65, 98, 85, 83, 115, 98, 69, 106, 102, 95, 91, 58, 84, 78, 1…</span></span>
<span><span class="co">#&gt; $ mom_hs    &lt;dbl&gt; 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, …</span></span>
<span><span class="co">#&gt; $ mom_iq    &lt;dbl&gt; 121.11753, 89.36188, 115.44316, 99.44964, 92.74571, 107.9018…</span></span>
<span><span class="co">#&gt; $ mom_work  &lt;dbl&gt; 4, 4, 4, 3, 4, 1, 4, 3, 1, 1, 1, 4, 4, 4, 2, 1, 3, 3, 4, 3, …</span></span>
<span><span class="co">#&gt; $ mom_age   &lt;dbl&gt; 27, 25, 27, 25, 27, 18, 20, 23, 24, 19, 23, 24, 27, 26, 24, …</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>I dati rappresentati in un diagramma a dispersione suggeriscono che, in questo campione, sembra effettivamente esserci un’associazione positiva tra l’intelligenza del bambino (<code>kid_score</code>) e l’intelligenza della madre (<code>mom_iq</code>).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">kidiq</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">mom_iq</span>, y <span class="op">=</span> <span class="va">kid_score</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="052_reglin2_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Il modello di regressione lineare descrive questa associazione mediante una retta.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">kidiq</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">mom_iq</span>, y <span class="op">=</span> <span class="va">kid_score</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_smooth</span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, se <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="052_reglin2_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Ci sono infinite rette che, in linea di principio, possono essere usate per “approssimare” la nube di punti nel diagramma a dispersione. È dunque necessario introdurre dei vincoli per selezionare una di queste possibili rette. Un vincolo che viene introdotto dal modello di regressione è quello di costringere la retta a passare per il punto <span class="math inline">\((\bar{x}, \bar{y})\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">kidiq</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">mom_iq</span>, y <span class="op">=</span> <span class="va">kid_score</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_smooth</span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, se <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_point</span><span class="op">(</span></span>
<span>    <span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">mom_iq</span><span class="op">)</span>, y<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">kid_score</span><span class="op">)</span><span class="op">)</span>, </span>
<span>    colour<span class="op">=</span><span class="st">"red"</span>, size <span class="op">=</span> <span class="fl">4</span></span>
<span>  <span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="052_reglin2_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Una retta che passa per il punto <span class="math inline">\((\bar{x}, \bar{y})\)</span> (evidenziato in rosso nella figura precedente) ha delle desiderabili proprietà statistiche che verranno descritte in seguito.</p>
<p>Il campione è costituito da <span class="math inline">\(n\)</span> coppie di osservazioni (<span class="math inline">\(x, y\)</span>). Per ciascuna coppia di valori <span class="math inline">\(x_i, y_i\)</span>, il modello di regressione si aspetta che il valore <span class="math inline">\(y_i\)</span> sia associato al corrispondente valore <span class="math inline">\(x_i\)</span> come indicato dalla seguente equazione:</p>
<p><span class="math display">\[
y_i = a + b x_i + e_i.
\]</span></p>
<p>I valori <span class="math inline">\(y_i\)</span> corrispondono, nell’esempio che stiamo discutendo, alla variabile <code>kid_score</code>. I primi 10 valori della variabile <span class="math inline">\(y\)</span> sono i seguenti:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">kidiq</span><span class="op">$</span><span class="va">kid_score</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span></span>
<span><span class="co">#&gt;  [1]  65  98  85  83 115  98  69 106 102  95</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Per fare riferimento a ciascuna osservazione usiamo l’indice <span class="math inline">\(i\)</span>. Quindi, ad esempio, <span class="math inline">\(y_3\)</span> è uguale a</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">kidiq</span><span class="op">$</span><span class="va">kid_score</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span></span>
<span><span class="co">#&gt; [1] 85</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Nel caso presente, la variabile <span class="math inline">\(x\)</span> è <code>mom_iq</code>. I primi 10 valori di <span class="math inline">\(x\)</span> sono i seguenti:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">kidiq</span><span class="op">$</span><span class="va">mom_iq</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span></span>
<span><span class="co">#&gt;  [1] 121.11753  89.36188 115.44316  99.44964  92.74571 107.90184 138.89311</span></span>
<span><span class="co">#&gt;  [8] 125.14512  81.61953  95.07307</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In maniera corrispondente alla <span class="math inline">\(y\)</span>, uso un indice per fare riferimento ai singoli valori della variabile. Ad esempio, <span class="math inline">\(x_3\)</span> è</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">kidiq</span><span class="op">$</span><span class="va">mom_iq</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span></span>
<span><span class="co">#&gt; [1] 115.4432</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Il modello di regressione bivariata, descritto dall’equazione precedente, ci dice che ciascun valore <span class="math inline">\(y\)</span> è dato dalla somma di due componenti: una componente deterministica e una componente aleatoria. Consideriamo il primo valore <span class="math inline">\(y\)</span> del campione. Per esso, il modello di regressione diventa</p>
<p><span class="math display">\[
y_1 = a + b x_1 + e_1,
\]</span></p>
<p>laddove <span class="math inline">\(a + b x_1\)</span> è la componente deterministica, denotata con <span class="math inline">\(\hat{y}\)</span>, e <span class="math inline">\(e_1\)</span> è la componente aleatoria.</p>
<p>La componente deterministica è la <em>componente</em> di ciascun valore <span class="math inline">\(y_i\)</span> che è possibile prevedere conoscendo <span class="math inline">\(x_i\)</span>. Tuttavia, non è possibile prevedere <em>perfettamente</em> i valori <span class="math inline">\(y\)</span> – ciò si verificherebbe soltanto se tutti punti del diagramma a dispersione fossero disposti su una retta. Ma non lo sono mai nella pratica concreta: la retta è solo un’approssimazione della relazione lineare tra <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>. Pertanto, conoscendo <span class="math inline">\(x_i\)</span> possiamo solo prevedere una porzione (o “componente”) del corrispondente valore <span class="math inline">\(y_i\)</span>.</p>
<p>Cosa significa che possiamo prevedere solo una componente di ciascuna osservazione <span class="math inline">\(y_i\)</span>? Significa che il valore osservato <span class="math inline">\(y_i\)</span> sarà diverso dal valore <span class="math inline">\(\hat{y}_i\)</span> previsto dal modello. Ciascun valore osservato <span class="math inline">\(y_i\)</span> sarà dunque dato dalla seguente somma: <span class="math inline">\(y_i = \hat{y}_i + e_i\)</span>, laddove <span class="math inline">\(e_i\)</span>, detto “residuo” è la componente di <span class="math inline">\(y_i\)</span> non predicibile dal modello lineare.</p>
<p>Ci possiamo dunque porre due domande:</p>
<ul>
<li>come è possibile trovare i coefficienti <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> che consentono di predire una componente della <span class="math inline">\(y\)</span> conoscendo la <span class="math inline">\(x\)</span>?</li>
<li>quant’è grande la porzione della <span class="math inline">\(y\)</span> che può essere predetta conoscendo <span class="math inline">\(x\)</span>? In altre parole, quant’è accurata la predizione della <span class="math inline">\(y\)</span> fornita dal modello di regressione lineare?</li>
</ul>
<p>Rispondere a tali due domanda definisce i primi due obiettivi del modello statistico della regressione lineare. Il terzo obiettivo è quello dell’inferenza, ovvero quello di capire che relazioni ci sono tra la relazione tra <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> osservata nel campione e la la relazione tra le due variabili nella popolazione.</p>
<section id="stima-dei-coefficienti-di-regressione" class="level2" data-number="27.1"><h2 data-number="27.1" class="anchored" data-anchor-id="stima-dei-coefficienti-di-regressione">
<span class="header-section-number">27.1</span> Stima dei coefficienti di regressione</h2>
<p>Iniziamo con il primo obiettivo, ovvero quello di trovare i coefficienti <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> che consentono di predire una componente di ciascuna osservazione <span class="math inline">\(y\)</span> conoscendo <span class="math inline">\(x\)</span>. Quindi, nel caso presente, ci chiediamo quanto segue. Il primo bambino del campione ha un QI uguale a 65. Sua madre ha un QI di 121.12. Qual è la predizione migliore del QI del bambino che possiamo ottenere conoscendo il QI della madre?</p>
<p>È chiaro, guardando i dati del campione, che non c’è una corrispondenza perfetta tra QI della madre e QI del bambino, tutt’altro! Infatti, se guardiamo il diagramma di dispersione ci rendiamo conto che i punti sono piuttosto lontani dalla retta che abbiamo sovrapposto alla nube di punti <span class="math inline">\(x_i, y_i\)</span>. Tuttavia, il diagramma di dispersione ci suggerisce che, al di là del rumore, c’è comunque una relazione tra le due variabili. Il nostro obiettivo è trovare un metodo quantitativo per descrivere una tale relazione.</p>
<p>Abbiamo detto che è possibile prevedere una componente di <span class="math inline">\(y_i\)</span> conoscendo <span class="math inline">\(x_i\)</span>. La componente <span class="math inline">\(y_i\)</span> predicibile da <span class="math inline">\(x_i\)</span> viene denotata da <span class="math inline">\(\hat{y}_i\)</span> e, nei termini del modello di regressione lineare è uguale a</p>
<p><span class="math display">\[
\hat{y}_i = a_i + bx_i.
\]</span></p>
<p>L’equazione precedente è un’<em>equazione lineare</em> e, dal punto di vista geometrico, corrisponde ad una retta. Ci sono infinite equazioni che, in linea di principio, possiamo usare per descrivere la relazione tra <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>. Abbiamo scelto la relazione lineare perché è la più semplice. Se guardiamo il diagramma di dispersione, infatti, non ci sono ragioni per descrivere la relazione tra il QI del bambino e il QI della madre con qualche curva, anziché con una retta. In altri campioni, una curva potrebbe essere più sensata di una retta, quale descrizione della relazione <em>media</em> tra <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>, ma non nel caso presente. Ricordiamo il principio del rasoio di Occam (ovvero, il principio che sta alla base del pensiero scientifico moderno): se un modello semplice funziona, non c’è ragione di usare un modello più complesso.</p>
<p>Dunque, abbiamo capito che vogliamo descrivere la <em>relazione media</em> tra <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> con una retta, ovvero, mediante l’equazione lineare</p>
<p><span class="math display">\[
\hat{y}_i = a + b x_i.
\]</span></p>
<p>L’equazione precedente ci dice che il modello lineare <span class="math inline">\(a + b x_i\)</span> <em>non è in grado di prevedere completamente</em> i valori <span class="math inline">\(y_i\)</span>. Questo, in generale, non è mai possibile (ovvero, è possibile solo in un caso specifico che, nella realtà empirica, non si verifica mai). L’equazione precedente ci dice che possiamo prevedere solo una componente di ciascuna osservazione <span class="math inline">\(y_i\)</span>, ovvero quella componente che abbiamo denotato con <span class="math inline">\(\hat{y}_i\)</span>. La componente che non possiamo prevedere con l’equazione <span class="math inline">\(a + b x_i\)</span> viene detta <em>residuo</em> e si denota con <span class="math inline">\(e_i\)</span>:</p>
<p><span class="math display">\[
e_i = y_i - \hat{y}_i = y_i - (a + bx_i).
\]</span></p>
<p>Dal punto di vista geometrico, la componente erratica del modello, <span class="math inline">\(e_i\)</span>, corrisponde alla distanza verticale tra ciascun punto del diagramma a dispersione e la retta di regressione <span class="math inline">\(a + bx\)</span>. Diciamo che <em>scomponiamo</em> il valore di ciascuna osservazione <span class="math inline">\(y_i\)</span> in due componenti nel senso che</p>
<p><span class="math display">\[
y_i = \hat{y}_i + e_i = (a + bx_i) + e_i.
\]</span></p>
<p>Il primo obiettivo del modello di regressione è quello di trovare i coefficienti dell’equazione</p>
<p><span class="math display">\[
a + b x_i
\]</span></p>
<p>che consente di trovare <span class="math inline">\(\hat{y}_i\)</span> conoscendo <span class="math inline">\(x_i\)</span>. Questi due coefficienti sono detti <em>coefficienti di regressione</em>.</p>
<p>Per trovare i coefficienti di regressione dobbiamo introdurre dei vincoli per limitare lo spazio delle possibili soluzioni. Il primo di tali vincoli è stato introdotto in precedenza: vogliamo che la retta <span class="math inline">\(\hat{y}_i = a + b x_i\)</span> passi per il punto <span class="math inline">\((\bar{x}, \bar{y})\)</span>. Il punto <span class="math inline">\((\bar{x}, \bar{y})\)</span> corrisponde al <em>baricentro</em> del diagramma a dispersione.</p>
<p>Ci sono però infinite rette che passano per i punto <span class="math inline">\((\bar{x}, \bar{y})\)</span>. Tutte queste rette soddisfano la seguente proprietà:</p>
<p><span class="math display">\[
\sum_{i=1}^n e_i = 0,
\]</span></p>
<p>ovvero, fanno in modo che la somma dei residui (positivi, per i punti che si trovano al di sopra della retta di regressione, negativi, per punti che si trovano al di sotto della retta di regressione) sia uguale a zero.</p>
<p>Questo significa che non possiamo selezionare una tra le infinite rette che passano per il punto <span class="math inline">\((\bar{x}, \bar{y})\)</span> usando il criterio che ci porta a scegliere la retta che rende la più piccola possibile (ovvero, minimizza) la somma dei residui. Infatti, tutte le rette passanti per il punto <span class="math inline">\((\bar{x}, \bar{y})\)</span> soddisfano questo requisito (rendono uguale a zero la somma dei residui). Dunque, dobbiamo trovare qualche altri criterio per scegliere una tra le infinite rette che passano per il punto <span class="math inline">\((\bar{x}, \bar{y})\)</span>.</p>
<p>Il criterio che viene normalmente scelto è quello di <em>minimizzare la somma dei quadrati dei residui</em> <span class="math inline">\((y_i - \hat{y}_i)^2\)</span>. In altri termini, vogliamo trovare i coefficienti <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> tali per cui la quantità</p>
<p><span class="math display">\[
\sum_{i=1}^{n}{(y_i - (a + b x_i))^2}
\]</span></p>
<p>assume il suo valore minimo. I coefficienti <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> che soddisfano questa proprietà si chiamano <em>coefficienti dei minimi quadrati</em>.</p>
<p>Questo problema ha una soluzione analitica. La soluzione analitica si trova riconoscendo il fatto che l’equazione precedente definisce una superficie e il problema diventa quello di trovare il punto di minimo di questa superficie. Per trovare la soluzione ci si deve rendere conto che il punto cercato è quello per cui il piano tangente alla superficie (nelle due direzioni <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span>) è piatto (le tangenti nelle due direzioni sono uguali a zero). Rendere uguale a zero la tangente ad una curva significa porre uguali a zero la derivata della curva. Nel caso presente, abbiamo una superficie, dunque due tangenti ortogonali e quindi abbiamo il problema di rendere uguali a zero le derivate parziali rispetto ad <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span>. Così facendo si definisce un sistema di equazioni lineari con due incognite, <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span>. La soluzione di tali equazioni, che si chiamano <em>equazioni normali</em>, è la seguente:</p>
<p><span class="math display">\[
a = \bar{y} - b \bar{x},
\]</span></p>
<p><span class="math display">\[
b = \frac{\mbox{Cov}(x, y)}{\mbox{Var}(x)}.
\]</span></p>
<p>Le due precedenti equazioni corrispondono alla <em>stima dei minimi quadrati</em> dei coefficienti di regressione della retta che minimizza la somma dei quadrati dei residui.</p>
<p>Nel caso dell’esempio presente, tali coefficienti sono uguali a:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cov</a></span><span class="op">(</span><span class="va">kidiq</span><span class="op">$</span><span class="va">kid_score</span>, <span class="va">kidiq</span><span class="op">$</span><span class="va">mom_iq</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">kidiq</span><span class="op">$</span><span class="va">mom_iq</span><span class="op">)</span></span>
<span><span class="va">b</span></span>
<span><span class="co">#&gt; [1] 0.6099746</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">a</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">kidiq</span><span class="op">$</span><span class="va">kid_score</span><span class="op">)</span> <span class="op">-</span> <span class="va">b</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">kidiq</span><span class="op">$</span><span class="va">mom_iq</span><span class="op">)</span></span>
<span><span class="va">a</span></span>
<span><span class="co">#&gt; [1] 25.79978</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In <span class="math inline">\(\mathsf{R}\)</span> li possiamo facilmente trovare con la seguente funzione:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">fm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">kid_score</span> <span class="op">~</span> <span class="va">mom_iq</span>, data <span class="op">=</span> <span class="va">kidiq</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">fm</span><span class="op">)</span></span>
<span><span class="co">#&gt; (Intercept)      mom_iq </span></span>
<span><span class="co">#&gt;  25.7997778   0.6099746</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In precedenza abbiamo soltanto accennato al problema di come si possono trovano i coefficienti dei minimi quadrati; ritorneremo su questo punto in seguito, con una simulazione. Per ora, chiediamoci cosa significano i due coefficienti che abbiamo appena trovato.</p>
<p>Il coefficiente <span class="math inline">\(a\)</span> si chiama <em>intercetta</em>. L’intercetta, all’interno del diagramma a dispersione, specifica il punto in cui la retta di regressione interseca l’asse <span class="math inline">\(y\)</span> del sistema di assi cartesiani.</p>
<p>Nel caso presente questo valore non è di alcun interesse, perché corrisponde al valore della retta di regressione quando <span class="math inline">\(x = 0\)</span>, ovvero quando l’intelligenza della madre è uguale a 0. Vedremo in seguito come, trasformando i dati, è possibile assegnare al coefficiente <span class="math inline">\(a\)</span> un’interpretazione più utile. Per ora mi limito a fornire l’interpretazione del coefficiente.</p>
<p>Passando a <span class="math inline">\(b\)</span>, possiamo dire che questo secondo coefficiente va sotto il nome di <em>pendenza</em> della retta di regressione. Ovvero ci dice di quanto aumenta (se <span class="math inline">\(b\)</span> è positivo) o diminuisce (se <span class="math inline">\(b\)</span> è negativo) la retta di regressione in corrispondenza di un aumento di 1 punto della variabile <span class="math inline">\(x\)</span>.</p>
<p>Nel caso presente, il coefficiente <span class="math inline">\(b\)</span> ci dice che, se il QI delle madri aumenta di 1 punto, il QI dei bambini aumenta <strong>in media</strong> di 0.61 punti.</p>
<p>È importante capire cosa significa che, in base ai risultati della regressione, <span class="math inline">\(y\)</span> aumenta <em>in media</em> di <span class="math inline">\(b\)</span> punti per ciascun aumento unitario di <span class="math inline">\(x\)</span>.</p>
<p>Il modello statistico di regressione <em>ipotizza</em> che, per ciascun valore osservato <span class="math inline">\(x\)</span> (per esempio, il valore del QI della prima madre del campione, ovvero <span class="math inline">\(x = 121.11753\)</span>) ci sia una distribuzione di valori <span class="math inline">\(y\)</span> nella popolazione, di cui solo uno è stato osservato nel campione. Possiamo facilmente capire che, se consideriamo tutte le madri con QI di 121.12, il punteggio del QI dei loro figli non sia costante, ma assuma tanti valori possibili. Questa distribuzione di valori possibili si chiama distribuzione <span class="math inline">\(y\)</span> condizionata a <span class="math inline">\(x\)</span>, ovvero <span class="math inline">\(p(y \mid x_i)\)</span>.</p>
<p>Il modello statistico della regressione lineare non può in alcun modo prevedere il valore assunto da ciascuna delle possibili osservazioni che fanno parte della distribuzione <span class="math inline">\(p(y \mid x_i)\)</span>. Il modello della regressione lineare ha un obiettivo più limitato, ovvero si propone di prevedere <em>le medie</em> delle distribuzioni <span class="math inline">\(p(y \mid x_i)\)</span> conoscendo i valori <span class="math inline">\(x\)</span>.</p>
<p>Dunque, quando il coefficiente <span class="math inline">\(b\)</span> è uguale a 0.61, questo significa che il modello di regressione predice che <em>la medie</em> della distribuzione condizionata <span class="math inline">\(p(y \mid x_i)\)</span> aumenta di 0.61 punti se la variabile <span class="math inline">\(x\)</span> (QI delle madri) aumenta di un punto. Questo significa che il modello di regressione non fa una predizione sul punteggio di ciascun valore <span class="math inline">\(y_i\)</span> (in funzione di <span class="math inline">\(x\)</span>), ma solo della media delle distribuzioni condizionate <span class="math inline">\(p(y \mid x_i)\)</span> di cui il valore osservato <span class="math inline">\(y_i\)</span> è una realizzazione casuale.</p>
<p>Possiamo dire la stessa cosa con parole diverse dicendo che il modello di regressione fa delle predizioni sulla componente deterministica di ciascuna osservazione. È più semplice capire questo aspetto se rappresentiamo in maniera grafica la componente “deterministica” <span class="math inline">\(\hat{y}_i = a + b x_i\)</span> predetta dal modello di regressione.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">kidiq</span><span class="op">$</span><span class="va">yhat</span> <span class="op">&lt;-</span> <span class="va">fm</span><span class="op">$</span><span class="va">fitted.values</span></span>
<span><span class="va">kidiq</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">mom_iq</span>, y <span class="op">=</span> <span class="va">yhat</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_smooth</span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, se <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_point</span><span class="op">(</span></span>
<span>    <span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">mom_iq</span><span class="op">)</span>, y<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">kid_score</span><span class="op">)</span><span class="op">)</span>, </span>
<span>    colour<span class="op">=</span><span class="st">"red"</span>, size <span class="op">=</span> <span class="fl">4</span></span>
<span>  <span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="052_reglin2_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Il diagramma precedente presenta ciascun valore <span class="math inline">\(\hat{y}_i = a + b x_i\)</span> in funzione di <span class="math inline">\(x_i\)</span>. Si vede che i valori predetti dal modello di regressione sono i punti che stanno sulla retta di regressione.</p>
<p>In precedenza abbiamo detto che il residuo, ovvero la componente di ciascuna osservazione <span class="math inline">\(y_i\)</span> che non viene predetta dal modello di regressione, corrisponde alla <em>distanza verticale</em> tra il valore <span class="math inline">\(y_i\)</span> osservato e il valore <span class="math inline">\(\hat{y}_i\)</span> predetto dal modello di regressione:</p>
<p><span class="math display">\[
e_i = y_i - (a + b x_i).
\]</span></p>
<p>Nel caso nella prima osservazione, ad esempio abbiamo:</p>
<p><span class="math display">\[
y_1 = (a + b x_1) + e_1
\]</span></p>
<p>Abbiamo</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">kidiq</span><span class="op">$</span><span class="va">kid_score</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="co">#&gt; [1] 65</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Dunque</p>
<p><span class="math display">\[
e_1 = (a + b x_1) - y_1
\]</span></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">e_1</span> <span class="op">&lt;-</span> <span class="va">kidiq</span><span class="op">$</span><span class="va">kid_score</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="op">(</span><span class="va">a</span> <span class="op">+</span> <span class="va">b</span> <span class="op">*</span> <span class="va">kidiq</span><span class="op">$</span><span class="va">mom_iq</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">e_1</span></span>
<span><span class="co">#&gt; [1] -34.67839</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Ciò significa che il valore osservato <span class="math inline">\(y_1 = 65\)</span> viene scomposto dal modello di regressione in due componenti. La componente deterministica <span class="math inline">\(\hat{y}_1\)</span>, predicibile da <span class="math inline">\(x_1\)</span>, è</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">yhat_1</span> <span class="op">&lt;-</span> <span class="va">a</span> <span class="op">+</span> <span class="va">b</span> <span class="op">*</span> <span class="va">kidiq</span><span class="op">$</span><span class="va">mom_iq</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">yhat_1</span></span>
<span><span class="co">#&gt; [1] 99.67839</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La somma della componente deterministica e della componente erratica, ovviamente, riproduce il valore osservato.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">yhat_1</span> <span class="op">+</span> <span class="va">e_1</span></span>
<span><span class="co">#&gt; [1] 65</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Se sommiamo tutti i residui calcolati rispetto alla retta di regressione dei minimi quadrati otteniamo zero:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">fm</span><span class="op">$</span><span class="va">res</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 5.373479e-13</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="trasformazione-dei-dati" class="level3" data-number="27.1.1"><h3 data-number="27.1.1" class="anchored" data-anchor-id="trasformazione-dei-dati">
<span class="header-section-number">27.1.1</span> Trasformazione dei dati</h3>
<p>In generale, per variabili a livello di scala ad intervalli, non è possibile assegnare un’interpretazione utile all’intercetta del modello di regressione lineare. L’intercetta ci dice infatti qual è il valore atteso della <span class="math inline">\(y\)</span> quando <span class="math inline">\(x = 0\)</span>. Ma, se la variabile <span class="math inline">\(x\)</span> è misurata su scala ad intervalli, il valore <span class="math inline">\(x = 0\)</span> è arbitrario e non corrisponde “all’assenza di intensità” della variabile <span class="math inline">\(x\)</span>. Un valore pari a 0 del QI della madre non vuol dire che l’intelligenza della madre sia nulla (un’affermazione, questa, che è difficile da capire), ma semplicemente che il punteggio del test usato per misurare il QI della madre assume valore 0 (qualcosa che, comunque, in pratica non succederà mai). Quindi è di poco interesse sapere qual è il valore medio del QI del bambino quando test usato per misurare il QI della madre ha valore 0. Per potere fornire all’intercetta del modello di regressione un’interpretazione più utile dobbiamo trasformare le osservazioni <span class="math inline">\(x\)</span>.</p>
<p>Esprimiamo <span class="math inline">\(x\)</span> come differenza dalla media. Chiamiamo questa nuova variabile <span class="math inline">\(xd\)</span>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">kidiq</span><span class="op">$</span><span class="va">xd</span> <span class="op">&lt;-</span> <span class="va">kidiq</span><span class="op">$</span><span class="va">mom_iq</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">kidiq</span><span class="op">$</span><span class="va">mom_iq</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Se ora usiamo le coppie di osservazioni <span class="math inline">\(xd_i, y_i\)</span>, il diagramma a dispersione assume la forma seguente.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">kidiq</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">xd</span>, y <span class="op">=</span> <span class="va">kid_score</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_smooth</span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, se <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_point</span><span class="op">(</span></span>
<span>    <span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">xd</span><span class="op">)</span>, y<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">kid_score</span><span class="op">)</span><span class="op">)</span>, </span>
<span>    colour<span class="op">=</span><span class="st">"red"</span>, size <span class="op">=</span> <span class="fl">4</span></span>
<span>  <span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="052_reglin2_files/figure-html/unnamed-chunk-20-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Quello che abbiamo fatto è stato di <em>traslare rigidamente</em> la nube di punti sul piano cartesiano di una quantità pari alla distanza tra <span class="math inline">\(\bar{x}\)</span> e l’origine. Dunque, le <em>relazioni spaziali</em> tra i punti del diagramma a dispersione restano immutate. Di conseguenza, la pendenza della retta di regressione calcolata sui dati trasformati è uguale a quella che si trova nel caso dei dati non trasformati. Ciò che cambia è il valore dell’intercetta.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">fm1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">kid_score</span> <span class="op">~</span> <span class="va">xd</span>, data <span class="op">=</span> <span class="va">kidiq</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">fm1</span><span class="op">)</span></span>
<span><span class="co">#&gt; (Intercept)          xd </span></span>
<span><span class="co">#&gt;  86.7972350   0.6099746</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>L’intercetta corrisponde al punto sull’asse <span class="math inline">\(y\)</span> dove la retta di regressione interseca l’ordinata. Ma, nel caso dei dati trasformati, dato che abbiamo traslato i punti di una quantità pari a <span class="math inline">\(x - \bar{x}\)</span>, il valore <span class="math inline">\(xd = 0\)</span> corrisponde a <span class="math inline">\(x = \bar{x}\)</span> nel caso dei dati grezzi. Dunque, per i dati trasformati <span class="math inline">\(xd_i, y_i\)</span>, l’intercetta corrisponderà al valore atteso della <span class="math inline">\(y\)</span> in corrispondenza del valore medio della variabile <span class="math inline">\(x\)</span> sulla scala dei dati non trasformati (ovvero <span class="math inline">\(\bar{x}\)</span>). In altre parole, l’intercetta del modello di regressione lineare calcolata sui dati trasformati corrisponde al QI medio dei bambini in corrispondenza del QI medio delle madri.</p>
</section><section id="il-metodo-dei-minimi-quadrati" class="level3" data-number="27.1.2"><h3 data-number="27.1.2" class="anchored" data-anchor-id="il-metodo-dei-minimi-quadrati">
<span class="header-section-number">27.1.2</span> Il metodo dei minimi quadrati</h3>
<p>Ora che abbiamo visto come interpretare il coefficienti di regressione, chiediamoci come vengono calcolati. La procedura generale è stata brevemente descritta in precedenza. Vediamo ora come si giunge alla conclusione descritta sopra usando una simulazione.</p>
<p>Il problema è di trovare i valori <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> tali per cui la quantità <span class="math inline">\(\sum_{i=1}^{n}{(y_i - (a + b x_i))^2}\)</span> assume il valore minore possibile. Questo è un problema di minimizzazione rispetto a due parametri. Per dare un’idea di come si fa, semplifichiamo il problema e supponiamo che uno dei due parametri sia noto, ad esempio <span class="math inline">\(a\)</span>, così ci resta una sola incognita.</p>
<p>Credo una griglia di valori <code>b_grid</code> possibili, ad esempio:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">nrep</span> <span class="op">&lt;-</span> <span class="fl">1e5</span></span>
<span><span class="va">b_grid</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, length.out <span class="op">=</span> <span class="va">nrep</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Definisco una funzione che calcola la quantità <span class="math inline">\(\sum_{i=1}^{n}{(y_i - (a + b x_i))^2}\)</span>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">sse</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">a</span>, <span class="va">b</span>, <span class="va">x</span>, <span class="va">y</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">y</span> <span class="op">-</span> <span class="op">(</span><span class="va">a</span> <span class="op">+</span> <span class="va">b</span> <span class="op">*</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Calcolo la somma degli errori quadratici per ciascun possibile valore <code>b_grid</code>, fissando <span class="math inline">\(a = 25.79978\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">sse_res</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="cn">NA</span>, <span class="va">nrep</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">nrep</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">sse_res</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu">sse</span><span class="op">(</span>a <span class="op">=</span> <span class="fl">25.79978</span>, b <span class="op">=</span> <span class="va">b_grid</span><span class="op">[</span><span class="va">i</span><span class="op">]</span>, x <span class="op">=</span> <span class="va">kidiq</span><span class="op">$</span><span class="va">mom_iq</span>, y <span class="op">=</span> <span class="va">kidiq</span><span class="op">$</span><span class="va">kid_score</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Esaminiamo il risultato ottenuto.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span></span>
<span>  <span class="va">b_grid</span>, <span class="va">sse_res</span>, type <span class="op">=</span> <span class="st">'l'</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="052_reglin2_files/figure-html/unnamed-chunk-25-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Il risultato ottenuto con la simulazione</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">b_grid</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/which.min.html">which.min</a></span><span class="op">(</span><span class="va">sse_res</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="co">#&gt; [1] 0.6099761</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>riproduce quello ottenuto per via analitica:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">b</span></span>
<span><span class="co">#&gt; [1] 0.6099746</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Una simulazione simile, ma computazionalmente più complessa, può essere usata per stimare simultaneamente entrambi i parametri. Ci siamo limitati qui ad una <em>proof of concept</em> del caso più semplice.</p>
</section><section id="lerrore-standard-della-regressione" class="level3" data-number="27.1.3"><h3 data-number="27.1.3" class="anchored" data-anchor-id="lerrore-standard-della-regressione">
<span class="header-section-number">27.1.3</span> L’errore standard della regressione</h3>
<p>Il secondo obiettivo del modello statistico di regressione lineare è quello di stabilire <em>quanto sia grande la componente</em> <span class="math inline">\(y\)</span> predicibile da <span class="math inline">\(x\)</span>, per ciascuna osservazione.</p>
<p>Un indice assoluto della bontà di adattamento è fornito dalla deviazione standard dei residui, <span class="math inline">\(s_e\)</span>, chiamata anche <em>errore standard della stima</em>. Uno stimatore non distorto della varianza dei residui nella popolazione è dato da</p>
<p><span class="math display">\[
s^2_e = \frac{1}{n-2}\sum e_i^2
\]</span></p>
<p>e quindi l’errore standard della stima sarà</p>
<span class="math display">\[\begin{equation}
s_e = \sqrt{\frac{1}{n-2}\sum e_i^2}.
\end{equation}\]</span>
<p>Si noti che questa è la stessa formula della varianza (dato che la media dei residui è zero), tranne per il fatto che al denominatore abbiamo <span class="math inline">\(n-2\)</span>. Dato che, per calcolare <span class="math inline">\(\hat{y}\)</span> abbiamo usato due coefficienti (<span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span>), si dice che “abbiamo perso due gradi di libertà”.</p>
<p>Dato che <span class="math inline">\(s_e\)</span> possiede la stessa unità di misura della variabile <span class="math inline">\(y\)</span>, l’errore standard della stima può essere considerato come una sorta di “residuo medio.” – usando la stessa interpretazione che diamo alla deviazione standard in generale.</p>
<p>Si noti che la formula precedente non fornisce la “deviazione standard dei residui nel campione” (quella formula avrebbe <span class="math inline">\(n\)</span> al denominatore). Invece, fornisce una <em>stima</em> della deviazione standard dei residui nella popolazione da cui il campione è stato estratto.</p>
<p>Verifichiamo quanto detto con i dati a disposizione.</p>
<p>I residui possono essere trovati nel modo seguente.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">e</span> <span class="op">&lt;-</span> <span class="va">kidiq</span><span class="op">$</span><span class="va">kid_score</span> <span class="op">-</span> <span class="op">(</span><span class="va">a</span> <span class="op">+</span> <span class="va">b</span> <span class="op">*</span> <span class="va">kidiq</span><span class="op">$</span><span class="va">mom_iq</span><span class="op">)</span></span>
<span><span class="va">e</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span></span>
<span><span class="co">#&gt;  [1] -34.678390  17.691747 -11.217173  -3.461529  32.627697   6.382845</span></span>
<span><span class="co">#&gt;  [7] -41.521041   3.864881  26.414387  11.208068</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Oppure nel modo seguente.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">fm</span><span class="op">$</span><span class="va">residuals</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span></span>
<span><span class="co">#&gt;          1          2          3          4          5          6          7 </span></span>
<span><span class="co">#&gt; -34.678390  17.691747 -11.217173  -3.461529  32.627697   6.382845 -41.521041 </span></span>
<span><span class="co">#&gt;          8          9         10 </span></span>
<span><span class="co">#&gt;   3.864881  26.414387  11.208068</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Calcolo il residuo medio, prendendo il valore assoluto.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">e</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 14.4686</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>L’errore standard della regressione è</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">e</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">e</span><span class="op">)</span> <span class="op">-</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 18.26612</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>I due numeri non sono uguali, ma possiamo dire che hanno lo stesso ordine di grandezza.</p>
<p>Se usiamo la funzione <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> otteniamo lo stesso valore, chiamato <code>Residual standard error</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fm</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = kid_score ~ mom_iq, data = kidiq)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -56.753 -12.074   2.217  11.710  47.691 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) 25.79978    5.91741    4.36 1.63e-05 ***</span></span>
<span><span class="co">#&gt; mom_iq       0.60997    0.05852   10.42  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 18.27 on 432 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.201,  Adjusted R-squared:  0.1991 </span></span>
<span><span class="co">#&gt; F-statistic: 108.6 on 1 and 432 DF,  p-value: &lt; 2.2e-16</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section></section><section id="indice-di-determinazione" class="level2" data-number="27.2"><h2 data-number="27.2" class="anchored" data-anchor-id="indice-di-determinazione">
<span class="header-section-number">27.2</span> Indice di determinazione</h2>
<p>Un importante risultato dei minimi quadrati riguarda la cosiddetta <em>scomposizione della devianza</em> mediante la quale si definisce l’indice di determinazione, il quale fornisce una misura relativa della bontà di adattamento del modello di regressione ai dati del campione. Per una generica osservazione <span class="math inline">\(x_i, y_i\)</span>, la variazione di <span class="math inline">\(y_i\)</span> rispetto alla media <span class="math inline">\(\bar{y}\)</span> può essere descritta come la somma di due componenti: il residuo <span class="math inline">\(e_i=y_i- \hat{y}_i\)</span> e lo scarto di <span class="math inline">\(\hat{y}_i\)</span> rispetto alla media <span class="math inline">\(\bar{y}\)</span>:</p>
<p><span class="math display">\[
y_i - \bar{y} = (y_i- \hat{y}_i) + (\hat{y}_i - \bar{y}) = e_i + (\hat{y}_i - \bar{y}).
\]</span></p>
<p>Se consideriamo tutte le osservazioni, la devianza delle <span class="math inline">\(y\)</span> può essere scomposta nel seguente modo:</p>
<span class="math display">\[\begin{align}
\sum (y_i - \bar{y})^2 &amp;= \sum \left[ e_i + (\hat{y}_i - \bar{y})
\right]^2
= \sum e_i^2 + \sum (\hat{y}_i - \bar{y})^2 + 2 \sum e_i (\hat{y}_i -
\bar{y}) \notag
\end{align}\]</span>
<p>Per i vincoli imposti sul modello statistico di regressione, il doppio prodotto si annulla, infatti</p>
<span class="math display">\[\begin{align}
\sum e_i (\hat{y}_i - \bar{y}) &amp;= \sum e_i \hat{y}_i - \bar{y}\sum e_i = \sum e_i (a + b x_i) \notag \\
&amp;= a \sum e_i + b \sum e_i x_i = 0 \notag
\end{align}\]</span>
<p>Il termine <span class="math inline">\(b \sum e_i x_i\)</span> è uguale a zero perché, come vedremo in seguito, i coefficienti di regressione vengono calcolati in modo tale da rendere nulla <span class="math inline">\(\mbox{Cov}(e, x)\)</span>. Di conseguenza, il termine precedente deve essere nullo.</p>
<p>Possiamo dunque concludere che la devianza totale (<span class="math inline">\(\mbox{dev}_T\)</span>) si scompone nella somma di devianza d’errore (o devianza non spiegata) (<span class="math inline">\(\mbox{dev}_E\)</span>) e devianza di regressione (o devianza spiegata) (<span class="math inline">\(\mbox{dev}_T\)</span>):</p>
<span class="math display">\[\begin{align}
\underbrace{\sum_{i=1}^n (y_i - \bar{y})^2}_{\tiny{\text{Devianza
totale}}} &amp;= \underbrace{\sum_{i=1}^n e_i^2}_{\tiny{\text{Devianza
di dispersione}}} + \underbrace{\sum_{i=1}^n  (\hat{y}_i -
\bar{y})^2}_{\tiny{\text{Devianza di regressione}}} \notag
\end{align}\]</span>
<p>La devianza di regressione, <span class="math inline">\(\mbox{dev_R} \triangleq \mbox{dev_T} - \mbox{dev_E}\)</span>, indica dunque la riduzione degli errori al quadrato che è imputabile alla regressione lineare. Il rapporto <span class="math inline">\(\mbox{dev_R}/\mbox{dev_T}\)</span>, detto <em>indice di determinazione</em>, esprime tale riduzione degli errori in termini proporzionali e definisce il coefficiente di correlazione al quadrato:</p>
<span class="math display">\[\begin{equation}
R^2 \triangleq \frac{\mbox{dev_R}}{\mbox{dev_T}} = 1 - \frac{\mbox{dev_E}}{\mbox{dev_T}}.
\end{equation}\]</span>
<p>Quando l’insieme di tutte le deviazioni della <span class="math inline">\(y\)</span> dalla media è spiegato dall’insieme di tutte le deviazioni della variabile teorica <span class="math inline">\(\hat{y}\)</span> dalla media, si ha che l’adattamento (o accostamento) del modello al campione di dati è perfetto, la devianza residua è nulla ed <span class="math inline">\(r^2 = 1\)</span>; nel caso opposto, la variabilità totale coincide con quella residua, per cui <span class="math inline">\(r^2 = 0\)</span>. Tra questi due estremi, <span class="math inline">\(r\)</span> indica l’intensità della relazione lineare tra le due variabili e <span class="math inline">\(r^2\)</span>, con <span class="math inline">\(0 \leq r^2 \leq 1\)</span>, esprime la porzione della devianza totale della <span class="math inline">\(y\)</span> che è spiegata dalla regressione lineare sulla <span class="math inline">\(x\)</span>.</p>
<p>Per l’esempio in discussione abbiamo quanto segue. La devianza totale è</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">dev_t</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">kidiq</span><span class="op">$</span><span class="va">kid_score</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">kidiq</span><span class="op">$</span><span class="va">kid_score</span><span class="op">)</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">dev_t</span></span>
<span><span class="co">#&gt; [1] 180386.2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La devianza spiegata è</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">dev_r</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">fm</span><span class="op">$</span><span class="va">fitted.values</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">kidiq</span><span class="op">$</span><span class="va">kid_score</span><span class="op">)</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">dev_r</span></span>
<span><span class="co">#&gt; [1] 36248.82</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>L’indice di determinazione è</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">R2</span> <span class="op">&lt;-</span> <span class="va">dev_r</span> <span class="op">/</span> <span class="va">dev_t</span></span>
<span><span class="va">R2</span></span>
<span><span class="co">#&gt; [1] 0.2009512</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Nell’output di <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> un tale valore è chiamato <code>Multiple R-squared</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">fm</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = kid_score ~ mom_iq, data = kidiq)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -56.753 -12.074   2.217  11.710  47.691 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) 25.79978    5.91741    4.36 1.63e-05 ***</span></span>
<span><span class="co">#&gt; mom_iq       0.60997    0.05852   10.42  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 18.27 on 432 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.201,  Adjusted R-squared:  0.1991 </span></span>
<span><span class="co">#&gt; F-statistic: 108.6 on 1 and 432 DF,  p-value: &lt; 2.2e-16</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Il risultato ottenuto si può interpretare dicendo che circa il 20% della variabilità dei punteggi del QI dei bambini può essere predetto conoscendo il QI delle madri.</p>
<section id="inferenza-sul-modello-di-regressione" class="level3" data-number="27.2.1"><h3 data-number="27.2.1" class="anchored" data-anchor-id="inferenza-sul-modello-di-regressione">
<span class="header-section-number">27.2.1</span> Inferenza sul modello di regressione</h3>
<p>La discussione precedente era tutta basata sulla trattazione “classica” del modello lineare, ovvero una trattazione basata sulle stime di massima verosimiglianza (se <span class="math inline">\(y \sim \mathcal{N}(\alpha + \beta x, \sigma)\)</span>, allora le stime dei minimi quadrati coincidono con le stime di massima verosimiglianza). In altre parole, nella discussione precedente non abbiamo considerato in alcun modo le distribuzioni a priori dei parametri <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span>. I risultati precedenti si confermano, in un contesto bayesiano, se e solo se imponiamo sui parametri delle distribuzioni a priori non informative (cioè, uniformi). In tali circostanze, le stime di massima verosimiglianza risultano identiche al massimo a posteriori bayesiano.</p>
<p>Detto questo, il tema dell’inferenza viene trattato dall’approccio frequentista costruendo la “distribuzione campionaria” dei parametri (ovvero la distribuzione dei valori che i parametri otterrebbero in infiniti campioni casuali (<span class="math inline">\(x, y\)</span>) di ampiezza <span class="math inline">\(n\)</span> estratti dalla medesima popolazione) e poi calcolando gli errori standard dei parametri e gli intervalli di fiducia dei parametri. Una domanda frequente è, per esempio, se la pendenza della retta di regressione sia maggiore di zero. Per rispondere a tale domanda l’approccio frequentista calcola l’intervallo di fiducia al 95% per il parametro <span class="math inline">\(\beta\)</span>. Se tale intervallo non include lo zero, e se il limite inferiore di tale intervallo è maggiore di zero, allora si conclude, con un grado di confidenza del 95%, che il vero parametro <span class="math inline">\(\beta\)</span> nella popolazione è maggiore di zero. Ovvero, si conclude che vi sono evidenze di un’associazione lineare positiva tra <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>.</p>
<p>Alla stessa conclusione si può arrivare calcolando, in un ottica bayesiana, l’intervallo di credibilità al 95% per il parametro <span class="math inline">\(\beta\)</span>. I due intervalli sono identici se usiamo una distribuzione a priori piatta. Sono invece diversi se usiamo una distribuzione a priori debolmente informativa, oppure informativa.</p>
<p>Solitamente si usa una distribuzione a priori debolmente informativa centrata sullo zero. In tali circostanze, l’uso della distribuzione a priori ha solo un effetto di <em>regolarizzazione</em>, ovvero di riduzione del peso delle osservazioni estreme – un tale risultato statistico è molto desiderabile, ma è difficile da ottenere in un contesto frequentista. Vedremo nel prossimo capitolo come può essere svolta l’inferenza sui coefficienti del modello di regressione lineare in un contesto bayesiano.</p>
</section></section><section id="commenti-e-considerazioni-finali" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="commenti-e-considerazioni-finali">Commenti e considerazioni finali</h2>
<p>Il modello lineare bivariato viene usato per descrivere la relazione tra due variabili e per determinare il segno e l’intensità di tale relazione. Inoltre, il modello lineare ci consente di prevedere il valore della variabile dipendente in base al valore assunto dalla variabile indipendente.</p>


<!-- -->

</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./051_reglin1.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Introduzione</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./053_reglin3.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Modello di regressione in linguaggio Stan</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb36" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Regressione lineare bivariata {#sec-regr-model-lm}</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(<span class="st">"_common.R"</span>)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(<span class="st">"_stan_options.R"</span>)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>In questo Capitolo verrà discusso il modello di regressione bivariata, ovvero il modello che, mediante una relazione lineare, predice una variabile continua $y$ a partire da un unico predittore continuo $x$. Ciò corrisponde ad adattare ai dati ($x_i, y_i$) la retta di regressione $y_i = a + bx_i + e_i$, con $i=1, \dots, n$. Usando dei dati reali, vedremo come stimare i coefficienti di regressione $a$ e $b$, e come essi possono essere interpretati. Vedremo anche come si può descrivere la bontà di adattamento del modello ai dati.</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>Nell'esempio che discuteremo in questo Capitolo verranno usati i dati <span class="in">`kidiq`</span>:</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Data from a survey of adult American women and their children (a subsample from the National Longitudinal Survey of Youth).</span></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Source: Gelman and Hill (2007)</span></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 434 obs. of 4 variables</span></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; -   kid_score Child's IQ score</span></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; -   mom_hs Indicator for whether the mother has a high school degree</span></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; -   mom_iq Mother's IQ score</span></span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; -   mom_age Mother's age</span></span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>Nel presente Capitolo esaminerò la relazione tra l'intelligenza del bambino (<span class="in">`kid_score`</span>) e l'intelligenza della madre (<span class="in">`mom_iq`</span>). Mi chiederò se, e in che misura, l'intelligenza della madre sia in grado di predire l'intelligenza del bambino.</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a>Per iniziare, leggo i dati in $\mathsf{R}$.</span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>kidiq <span class="ot">&lt;-</span> rio<span class="sc">::</span><span class="fu">import</span>(here<span class="sc">::</span><span class="fu">here</span>(</span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a>  <span class="st">"data"</span>, <span class="st">"kidiq.dta"</span></span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(kidiq)</span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a>I dati rappresentati in un diagramma a dispersione suggeriscono che, in questo campione, sembra effettivamente esserci un'associazione positiva tra l'intelligenza del bambino (<span class="in">`kid_score`</span>) e l'intelligenza della madre (<span class="in">`mom_iq`</span>).</span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-41"><a href="#cb36-41" aria-hidden="true" tabindex="-1"></a>kidiq <span class="sc">%&gt;%</span> </span>
<span id="cb36-42"><a href="#cb36-42" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> mom_iq, <span class="at">y =</span> kid_score)) <span class="sc">+</span></span>
<span id="cb36-43"><a href="#cb36-43" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span>
<span id="cb36-44"><a href="#cb36-44" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-45"><a href="#cb36-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-46"><a href="#cb36-46" aria-hidden="true" tabindex="-1"></a>Il modello di regressione lineare descrive questa associazione mediante una retta.</span>
<span id="cb36-47"><a href="#cb36-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-50"><a href="#cb36-50" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-51"><a href="#cb36-51" aria-hidden="true" tabindex="-1"></a>kidiq <span class="sc">%&gt;%</span> </span>
<span id="cb36-52"><a href="#cb36-52" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> mom_iq, <span class="at">y =</span> kid_score)) <span class="sc">+</span> </span>
<span id="cb36-53"><a href="#cb36-53" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb36-54"><a href="#cb36-54" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>)</span>
<span id="cb36-55"><a href="#cb36-55" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-56"><a href="#cb36-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-57"><a href="#cb36-57" aria-hidden="true" tabindex="-1"></a>Ci sono infinite rette che, in linea di principio, possono essere usate per "approssimare" la nube di punti nel diagramma a dispersione. È dunque necessario introdurre dei vincoli per selezionare una di queste possibili rette. Un vincolo che viene introdotto dal modello di regressione è quello di costringere la retta a passare per il punto $(\bar{x}, \bar{y})$.</span>
<span id="cb36-58"><a href="#cb36-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-61"><a href="#cb36-61" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-62"><a href="#cb36-62" aria-hidden="true" tabindex="-1"></a>kidiq <span class="sc">%&gt;%</span> </span>
<span id="cb36-63"><a href="#cb36-63" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> mom_iq, <span class="at">y =</span> kid_score)) <span class="sc">+</span> </span>
<span id="cb36-64"><a href="#cb36-64" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb36-65"><a href="#cb36-65" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb36-66"><a href="#cb36-66" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(</span>
<span id="cb36-67"><a href="#cb36-67" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">x=</span><span class="fu">mean</span>(mom_iq), <span class="at">y=</span><span class="fu">mean</span>(kid_score)), </span>
<span id="cb36-68"><a href="#cb36-68" aria-hidden="true" tabindex="-1"></a>    <span class="at">colour=</span><span class="st">"red"</span>, <span class="at">size =</span> <span class="dv">4</span></span>
<span id="cb36-69"><a href="#cb36-69" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb36-70"><a href="#cb36-70" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-71"><a href="#cb36-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-72"><a href="#cb36-72" aria-hidden="true" tabindex="-1"></a>Una retta che passa per il punto $(\bar{x}, \bar{y})$ (evidenziato in rosso nella figura precedente) ha delle desiderabili proprietà statistiche che verranno descritte in seguito.</span>
<span id="cb36-73"><a href="#cb36-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-74"><a href="#cb36-74" aria-hidden="true" tabindex="-1"></a>Il campione è costituito da $n$ coppie di osservazioni ($x, y$). Per ciascuna coppia di valori $x_i, y_i$, il modello di regressione si aspetta che il valore $y_i$ sia associato al corrispondente valore $x_i$ come indicato dalla seguente equazione:</span>
<span id="cb36-75"><a href="#cb36-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-76"><a href="#cb36-76" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-77"><a href="#cb36-77" aria-hidden="true" tabindex="-1"></a>y_i = a + b x_i + e_i.</span>
<span id="cb36-78"><a href="#cb36-78" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-79"><a href="#cb36-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-80"><a href="#cb36-80" aria-hidden="true" tabindex="-1"></a>I valori $y_i$ corrispondono, nell'esempio che stiamo discutendo, alla variabile <span class="in">`kid_score`</span>. I primi 10 valori della variabile $y$ sono i seguenti:</span>
<span id="cb36-81"><a href="#cb36-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-84"><a href="#cb36-84" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-85"><a href="#cb36-85" aria-hidden="true" tabindex="-1"></a>kidiq<span class="sc">$</span>kid_score[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]</span>
<span id="cb36-86"><a href="#cb36-86" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-87"><a href="#cb36-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-88"><a href="#cb36-88" aria-hidden="true" tabindex="-1"></a>Per fare riferimento a ciascuna osservazione usiamo l'indice $i$. Quindi, ad esempio, $y_3$ è uguale a</span>
<span id="cb36-89"><a href="#cb36-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-92"><a href="#cb36-92" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-93"><a href="#cb36-93" aria-hidden="true" tabindex="-1"></a>kidiq<span class="sc">$</span>kid_score[<span class="dv">3</span>]</span>
<span id="cb36-94"><a href="#cb36-94" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-95"><a href="#cb36-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-96"><a href="#cb36-96" aria-hidden="true" tabindex="-1"></a>Nel caso presente, la variabile $x$ è <span class="in">`mom_iq`</span>. I primi 10 valori di $x$ sono i seguenti:</span>
<span id="cb36-97"><a href="#cb36-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-100"><a href="#cb36-100" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-101"><a href="#cb36-101" aria-hidden="true" tabindex="-1"></a>kidiq<span class="sc">$</span>mom_iq[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]</span>
<span id="cb36-102"><a href="#cb36-102" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-103"><a href="#cb36-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-104"><a href="#cb36-104" aria-hidden="true" tabindex="-1"></a>In maniera corrispondente alla $y$, uso un indice per fare riferimento ai singoli valori della variabile. Ad esempio, $x_3$ è</span>
<span id="cb36-105"><a href="#cb36-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-108"><a href="#cb36-108" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-109"><a href="#cb36-109" aria-hidden="true" tabindex="-1"></a>kidiq<span class="sc">$</span>mom_iq[<span class="dv">3</span>]</span>
<span id="cb36-110"><a href="#cb36-110" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-111"><a href="#cb36-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-112"><a href="#cb36-112" aria-hidden="true" tabindex="-1"></a>Il modello di regressione bivariata, descritto dall'equazione precedente, ci dice che ciascun valore $y$ è dato dalla somma di due componenti: una componente deterministica e una componente aleatoria. Consideriamo il primo valore $y$ del campione. Per esso, il modello di regressione diventa</span>
<span id="cb36-113"><a href="#cb36-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-114"><a href="#cb36-114" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-115"><a href="#cb36-115" aria-hidden="true" tabindex="-1"></a>y_1 = a + b x_1 + e_1,</span>
<span id="cb36-116"><a href="#cb36-116" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-117"><a href="#cb36-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-118"><a href="#cb36-118" aria-hidden="true" tabindex="-1"></a>laddove $a + b x_1$ è la componente deterministica, denotata con $\hat{y}$, e $e_1$ è la componente aleatoria.</span>
<span id="cb36-119"><a href="#cb36-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-120"><a href="#cb36-120" aria-hidden="true" tabindex="-1"></a>La componente deterministica è la *componente* di ciascun valore $y_i$ che è possibile prevedere conoscendo $x_i$. Tuttavia, non è possibile prevedere *perfettamente* i valori $y$ -- ciò si verificherebbe soltanto se tutti punti del diagramma a dispersione fossero disposti su una retta. Ma non lo sono mai nella pratica concreta: la retta è solo un'approssimazione della relazione lineare tra $x$ e $y$. Pertanto, conoscendo $x_i$ possiamo solo prevedere una porzione (o "componente") del corrispondente valore $y_i$.</span>
<span id="cb36-121"><a href="#cb36-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-122"><a href="#cb36-122" aria-hidden="true" tabindex="-1"></a>Cosa significa che possiamo prevedere solo una componente di ciascuna osservazione $y_i$? Significa che il valore osservato $y_i$ sarà diverso dal valore $\hat{y}_i$ previsto dal modello. Ciascun valore osservato $y_i$ sarà dunque dato dalla seguente somma: $y_i = \hat{y}_i + e_i$, laddove $e_i$, detto "residuo" è la componente di $y_i$ non predicibile dal modello lineare.</span>
<span id="cb36-123"><a href="#cb36-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-124"><a href="#cb36-124" aria-hidden="true" tabindex="-1"></a>Ci possiamo dunque porre due domande:</span>
<span id="cb36-125"><a href="#cb36-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-126"><a href="#cb36-126" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>come è possibile trovare i coefficienti $a$ e $b$ che consentono di predire una componente della $y$ conoscendo la $x$?</span>
<span id="cb36-127"><a href="#cb36-127" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>quant'è grande la porzione della $y$ che può essere predetta conoscendo $x$? In altre parole, quant'è accurata la predizione della $y$ fornita dal modello di regressione lineare?</span>
<span id="cb36-128"><a href="#cb36-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-129"><a href="#cb36-129" aria-hidden="true" tabindex="-1"></a>Rispondere a tali due domanda definisce i primi due obiettivi del modello statistico della regressione lineare. Il terzo obiettivo è quello dell'inferenza, ovvero quello di capire che relazioni ci sono tra la relazione tra $x$ e $y$ osservata nel campione e la la relazione tra le due variabili nella popolazione.</span>
<span id="cb36-130"><a href="#cb36-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-131"><a href="#cb36-131" aria-hidden="true" tabindex="-1"></a><span class="fu">## Stima dei coefficienti di regressione</span></span>
<span id="cb36-132"><a href="#cb36-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-133"><a href="#cb36-133" aria-hidden="true" tabindex="-1"></a>Iniziamo con il primo obiettivo, ovvero quello di trovare i coefficienti $a$ e $b$ che consentono di predire una componente di ciascuna osservazione $y$ conoscendo $x$. Quindi, nel caso presente, ci chiediamo quanto segue. Il primo bambino del campione ha un QI uguale a 65. Sua madre ha un QI di 121.12. Qual è la predizione migliore del QI del bambino che possiamo ottenere conoscendo il QI della madre?</span>
<span id="cb36-134"><a href="#cb36-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-135"><a href="#cb36-135" aria-hidden="true" tabindex="-1"></a>È chiaro, guardando i dati del campione, che non c'è una corrispondenza perfetta tra QI della madre e QI del bambino, tutt'altro! Infatti, se guardiamo il diagramma di dispersione ci rendiamo conto che i punti sono piuttosto lontani dalla retta che abbiamo sovrapposto alla nube di punti $x_i, y_i$. Tuttavia, il diagramma di dispersione ci suggerisce che, al di là del rumore, c'è comunque una relazione tra le due variabili. Il nostro obiettivo è trovare un metodo quantitativo per descrivere una tale relazione.</span>
<span id="cb36-136"><a href="#cb36-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-137"><a href="#cb36-137" aria-hidden="true" tabindex="-1"></a>Abbiamo detto che è possibile prevedere una componente di $y_i$ conoscendo $x_i$. La componente $y_i$ predicibile da $x_i$ viene denotata da $\hat{y}_i$ e, nei termini del modello di regressione lineare è uguale a</span>
<span id="cb36-138"><a href="#cb36-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-139"><a href="#cb36-139" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-140"><a href="#cb36-140" aria-hidden="true" tabindex="-1"></a>\hat{y}_i = a_i + bx_i.</span>
<span id="cb36-141"><a href="#cb36-141" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-142"><a href="#cb36-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-143"><a href="#cb36-143" aria-hidden="true" tabindex="-1"></a>L'equazione precedente è un'*equazione lineare* e, dal punto di vista geometrico, corrisponde ad una retta. Ci sono infinite equazioni che, in linea di principio, possiamo usare per descrivere la relazione tra $x$ e $y$. Abbiamo scelto la relazione lineare perché è la più semplice. Se guardiamo il diagramma di dispersione, infatti, non ci sono ragioni per descrivere la relazione tra il QI del bambino e il QI della madre con qualche curva, anziché con una retta. In altri campioni, una curva potrebbe essere più sensata di una retta, quale descrizione della relazione *media* tra $x$ e $y$, ma non nel caso presente. Ricordiamo il principio del rasoio di Occam (ovvero, il principio che sta alla base del pensiero scientifico moderno): se un modello semplice funziona, non c'è ragione di usare un modello più complesso.</span>
<span id="cb36-144"><a href="#cb36-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-145"><a href="#cb36-145" aria-hidden="true" tabindex="-1"></a>Dunque, abbiamo capito che vogliamo descrivere la *relazione media* tra $x$ e $y$ con una retta, ovvero, mediante l'equazione lineare</span>
<span id="cb36-146"><a href="#cb36-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-147"><a href="#cb36-147" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-148"><a href="#cb36-148" aria-hidden="true" tabindex="-1"></a>\hat{y}_i = a + b x_i.</span>
<span id="cb36-149"><a href="#cb36-149" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-150"><a href="#cb36-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-151"><a href="#cb36-151" aria-hidden="true" tabindex="-1"></a>L'equazione precedente ci dice che il modello lineare $a + b x_i$ *non è in grado di prevedere completamente* i valori $y_i$. Questo, in generale, non è mai possibile (ovvero, è possibile solo in un caso specifico che, nella realtà empirica, non si verifica mai). L'equazione precedente ci dice che possiamo prevedere solo una componente di ciascuna osservazione $y_i$, ovvero quella componente che abbiamo denotato con $\hat{y}_i$. La componente che non possiamo prevedere con l'equazione $a + b x_i$ viene detta *residuo* e si denota con $e_i$:</span>
<span id="cb36-152"><a href="#cb36-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-153"><a href="#cb36-153" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-154"><a href="#cb36-154" aria-hidden="true" tabindex="-1"></a>e_i = y_i - \hat{y}_i = y_i - (a + bx_i).</span>
<span id="cb36-155"><a href="#cb36-155" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-156"><a href="#cb36-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-157"><a href="#cb36-157" aria-hidden="true" tabindex="-1"></a>Dal punto di vista geometrico, la componente erratica del modello, $e_i$, corrisponde alla distanza verticale tra ciascun punto del diagramma a dispersione e la retta di regressione $a + bx$. Diciamo che *scomponiamo* il valore di ciascuna osservazione $y_i$ in due componenti nel senso che</span>
<span id="cb36-158"><a href="#cb36-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-159"><a href="#cb36-159" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-160"><a href="#cb36-160" aria-hidden="true" tabindex="-1"></a>y_i = \hat{y}_i + e_i = (a + bx_i) + e_i. </span>
<span id="cb36-161"><a href="#cb36-161" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-162"><a href="#cb36-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-163"><a href="#cb36-163" aria-hidden="true" tabindex="-1"></a>Il primo obiettivo del modello di regressione è quello di trovare i coefficienti dell'equazione</span>
<span id="cb36-164"><a href="#cb36-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-165"><a href="#cb36-165" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-166"><a href="#cb36-166" aria-hidden="true" tabindex="-1"></a> a + b x_i</span>
<span id="cb36-167"><a href="#cb36-167" aria-hidden="true" tabindex="-1"></a> $$</span>
<span id="cb36-168"><a href="#cb36-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-169"><a href="#cb36-169" aria-hidden="true" tabindex="-1"></a>che consente di trovare $\hat{y}_i$ conoscendo $x_i$. Questi due coefficienti sono detti *coefficienti di regressione*.</span>
<span id="cb36-170"><a href="#cb36-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-171"><a href="#cb36-171" aria-hidden="true" tabindex="-1"></a>Per trovare i coefficienti di regressione dobbiamo introdurre dei vincoli per limitare lo spazio delle possibili soluzioni. Il primo di tali vincoli è stato introdotto in precedenza: vogliamo che la retta $\hat{y}_i = a + b x_i$ passi per il punto $(\bar{x}, \bar{y})$. Il punto $(\bar{x}, \bar{y})$ corrisponde al *baricentro* del diagramma a dispersione.</span>
<span id="cb36-172"><a href="#cb36-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-173"><a href="#cb36-173" aria-hidden="true" tabindex="-1"></a>Ci sono però infinite rette che passano per i punto $(\bar{x}, \bar{y})$. Tutte queste rette soddisfano la seguente proprietà:</span>
<span id="cb36-174"><a href="#cb36-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-175"><a href="#cb36-175" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-176"><a href="#cb36-176" aria-hidden="true" tabindex="-1"></a>\sum_{i=1}^n e_i = 0,</span>
<span id="cb36-177"><a href="#cb36-177" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-178"><a href="#cb36-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-179"><a href="#cb36-179" aria-hidden="true" tabindex="-1"></a>ovvero, fanno in modo che la somma dei residui (positivi, per i punti che si trovano al di sopra della retta di regressione, negativi, per punti che si trovano al di sotto della retta di regressione) sia uguale a zero.</span>
<span id="cb36-180"><a href="#cb36-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-181"><a href="#cb36-181" aria-hidden="true" tabindex="-1"></a>Questo significa che non possiamo selezionare una tra le infinite rette che passano per il punto $(\bar{x}, \bar{y})$ usando il criterio che ci porta a scegliere la retta che rende la più piccola possibile (ovvero, minimizza) la somma dei residui. Infatti, tutte le rette passanti per il punto $(\bar{x}, \bar{y})$ soddisfano questo requisito (rendono uguale a zero la somma dei residui). Dunque, dobbiamo trovare qualche altri criterio per scegliere una tra le infinite rette che passano per il punto $(\bar{x}, \bar{y})$.</span>
<span id="cb36-182"><a href="#cb36-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-183"><a href="#cb36-183" aria-hidden="true" tabindex="-1"></a>Il criterio che viene normalmente scelto è quello di *minimizzare la somma dei quadrati dei residui* $(y_i - \hat{y}_i)^2$. In altri termini, vogliamo trovare i coefficienti $a$ e $b$ tali per cui la quantità</span>
<span id="cb36-184"><a href="#cb36-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-185"><a href="#cb36-185" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-186"><a href="#cb36-186" aria-hidden="true" tabindex="-1"></a>\sum_{i=1}^{n}{(y_i - (a + b x_i))^2}</span>
<span id="cb36-187"><a href="#cb36-187" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-188"><a href="#cb36-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-189"><a href="#cb36-189" aria-hidden="true" tabindex="-1"></a>assume il suo valore minimo. I coefficienti $a$ e $b$ che soddisfano questa proprietà si chiamano *coefficienti dei minimi quadrati*.</span>
<span id="cb36-190"><a href="#cb36-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-191"><a href="#cb36-191" aria-hidden="true" tabindex="-1"></a>Questo problema ha una soluzione analitica. La soluzione analitica si trova riconoscendo il fatto che l'equazione precedente definisce una superficie e il problema diventa quello di trovare il punto di minimo di questa superficie. Per trovare la soluzione ci si deve rendere conto che il punto cercato è quello per cui il piano tangente alla superficie (nelle due direzioni $a$ e $b$) è piatto (le tangenti nelle due direzioni sono uguali a zero). Rendere uguale a zero la tangente ad una curva significa porre uguali a zero la derivata della curva. Nel caso presente, abbiamo una superficie, dunque due tangenti ortogonali e quindi abbiamo il problema di rendere uguali a zero le derivate parziali rispetto ad $a$ e $b$. Così facendo si definisce un sistema di equazioni lineari con due incognite, $a$ e $b$. La soluzione di tali equazioni, che si chiamano *equazioni normali*, è la seguente:</span>
<span id="cb36-192"><a href="#cb36-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-193"><a href="#cb36-193" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-194"><a href="#cb36-194" aria-hidden="true" tabindex="-1"></a>a = \bar{y} - b \bar{x},</span>
<span id="cb36-195"><a href="#cb36-195" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-196"><a href="#cb36-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-197"><a href="#cb36-197" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-198"><a href="#cb36-198" aria-hidden="true" tabindex="-1"></a>b = \frac{\mbox{Cov}(x, y)}{\mbox{Var}(x)}.</span>
<span id="cb36-199"><a href="#cb36-199" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-200"><a href="#cb36-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-201"><a href="#cb36-201" aria-hidden="true" tabindex="-1"></a>Le due precedenti equazioni corrispondono alla *stima dei minimi quadrati* dei coefficienti di regressione della retta che minimizza la somma dei quadrati dei residui.</span>
<span id="cb36-202"><a href="#cb36-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-203"><a href="#cb36-203" aria-hidden="true" tabindex="-1"></a>Nel caso dell'esempio presente, tali coefficienti sono uguali a:</span>
<span id="cb36-204"><a href="#cb36-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-207"><a href="#cb36-207" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-208"><a href="#cb36-208" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">cov</span>(kidiq<span class="sc">$</span>kid_score, kidiq<span class="sc">$</span>mom_iq) <span class="sc">/</span> <span class="fu">var</span>(kidiq<span class="sc">$</span>mom_iq)</span>
<span id="cb36-209"><a href="#cb36-209" aria-hidden="true" tabindex="-1"></a>b</span>
<span id="cb36-210"><a href="#cb36-210" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-211"><a href="#cb36-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-214"><a href="#cb36-214" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-215"><a href="#cb36-215" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fu">mean</span>(kidiq<span class="sc">$</span>kid_score) <span class="sc">-</span> b <span class="sc">*</span> <span class="fu">mean</span>(kidiq<span class="sc">$</span>mom_iq)</span>
<span id="cb36-216"><a href="#cb36-216" aria-hidden="true" tabindex="-1"></a>a</span>
<span id="cb36-217"><a href="#cb36-217" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-218"><a href="#cb36-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-219"><a href="#cb36-219" aria-hidden="true" tabindex="-1"></a>In $\mathsf{R}$ li possiamo facilmente trovare con la seguente funzione:</span>
<span id="cb36-220"><a href="#cb36-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-223"><a href="#cb36-223" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-224"><a href="#cb36-224" aria-hidden="true" tabindex="-1"></a>fm <span class="ot">&lt;-</span> <span class="fu">lm</span>(kid_score <span class="sc">~</span> mom_iq, <span class="at">data =</span> kidiq)</span>
<span id="cb36-225"><a href="#cb36-225" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(fm)</span>
<span id="cb36-226"><a href="#cb36-226" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-227"><a href="#cb36-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-228"><a href="#cb36-228" aria-hidden="true" tabindex="-1"></a>In precedenza abbiamo soltanto accennato al problema di come si possono trovano i coefficienti dei minimi quadrati; ritorneremo su questo punto in seguito, con una simulazione. Per ora, chiediamoci cosa significano i due coefficienti che abbiamo appena trovato.</span>
<span id="cb36-229"><a href="#cb36-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-230"><a href="#cb36-230" aria-hidden="true" tabindex="-1"></a>Il coefficiente $a$ si chiama *intercetta*. L'intercetta, all'interno del diagramma a dispersione, specifica il punto in cui la retta di regressione interseca l'asse $y$ del sistema di assi cartesiani.</span>
<span id="cb36-231"><a href="#cb36-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-232"><a href="#cb36-232" aria-hidden="true" tabindex="-1"></a>Nel caso presente questo valore non è di alcun interesse, perché corrisponde al valore della retta di regressione quando $x = 0$, ovvero quando l'intelligenza della madre è uguale a 0. Vedremo in seguito come, trasformando i dati, è possibile assegnare al coefficiente $a$ un'interpretazione più utile. Per ora mi limito a fornire l'interpretazione del coefficiente.</span>
<span id="cb36-233"><a href="#cb36-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-234"><a href="#cb36-234" aria-hidden="true" tabindex="-1"></a>Passando a $b$, possiamo dire che questo secondo coefficiente va sotto il nome di *pendenza* della retta di regressione. Ovvero ci dice di quanto aumenta (se $b$ è positivo) o diminuisce (se $b$ è negativo) la retta di regressione in corrispondenza di un aumento di 1 punto della variabile $x$.</span>
<span id="cb36-235"><a href="#cb36-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-236"><a href="#cb36-236" aria-hidden="true" tabindex="-1"></a>Nel caso presente, il coefficiente $b$ ci dice che, se il QI delle madri aumenta di 1 punto, il QI dei bambini aumenta **in media** di 0.61 punti.</span>
<span id="cb36-237"><a href="#cb36-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-238"><a href="#cb36-238" aria-hidden="true" tabindex="-1"></a>È importante capire cosa significa che, in base ai risultati della regressione, $y$ aumenta *in media* di $b$ punti per ciascun aumento unitario di $x$.</span>
<span id="cb36-239"><a href="#cb36-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-240"><a href="#cb36-240" aria-hidden="true" tabindex="-1"></a>Il modello statistico di regressione *ipotizza* che, per ciascun valore osservato $x$ (per esempio, il valore del QI della prima madre del campione, ovvero $x = 121.11753$) ci sia una distribuzione di valori $y$ nella popolazione, di cui solo uno è stato osservato nel campione. Possiamo facilmente capire che, se consideriamo tutte le madri con QI di 121.12, il punteggio del QI dei loro figli non sia costante, ma assuma tanti valori possibili. Questa distribuzione di valori possibili si chiama distribuzione $y$ condizionata a $x$, ovvero $p(y \mid x_i)$.</span>
<span id="cb36-241"><a href="#cb36-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-242"><a href="#cb36-242" aria-hidden="true" tabindex="-1"></a>Il modello statistico della regressione lineare non può in alcun modo prevedere il valore assunto da ciascuna delle possibili osservazioni che fanno parte della distribuzione $p(y \mid x_i)$. Il modello della regressione lineare ha un obiettivo più limitato, ovvero si propone di prevedere *le medie* delle distribuzioni $p(y \mid x_i)$ conoscendo i valori $x$.</span>
<span id="cb36-243"><a href="#cb36-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-244"><a href="#cb36-244" aria-hidden="true" tabindex="-1"></a>Dunque, quando il coefficiente $b$ è uguale a 0.61, questo significa che il modello di regressione predice che *la medie* della distribuzione condizionata $p(y \mid x_i)$ aumenta di 0.61 punti se la variabile $x$ (QI delle madri) aumenta di un punto. Questo significa che il modello di regressione non fa una predizione sul punteggio di ciascun valore $y_i$ (in funzione di $x$), ma solo della media delle distribuzioni condizionate $p(y \mid x_i)$ di cui il valore osservato $y_i$ è una realizzazione casuale.</span>
<span id="cb36-245"><a href="#cb36-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-246"><a href="#cb36-246" aria-hidden="true" tabindex="-1"></a>Possiamo dire la stessa cosa con parole diverse dicendo che il modello di regressione fa delle predizioni sulla componente deterministica di ciascuna osservazione. È più semplice capire questo aspetto se rappresentiamo in maniera grafica la componente "deterministica" $\hat{y}_i = a + b x_i$ predetta dal modello di regressione.</span>
<span id="cb36-247"><a href="#cb36-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-250"><a href="#cb36-250" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-251"><a href="#cb36-251" aria-hidden="true" tabindex="-1"></a>kidiq<span class="sc">$</span>yhat <span class="ot">&lt;-</span> fm<span class="sc">$</span>fitted.values</span>
<span id="cb36-252"><a href="#cb36-252" aria-hidden="true" tabindex="-1"></a>kidiq <span class="sc">%&gt;%</span> </span>
<span id="cb36-253"><a href="#cb36-253" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> mom_iq, <span class="at">y =</span> yhat)) <span class="sc">+</span> </span>
<span id="cb36-254"><a href="#cb36-254" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb36-255"><a href="#cb36-255" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb36-256"><a href="#cb36-256" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(</span>
<span id="cb36-257"><a href="#cb36-257" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">x=</span><span class="fu">mean</span>(mom_iq), <span class="at">y=</span><span class="fu">mean</span>(kid_score)), </span>
<span id="cb36-258"><a href="#cb36-258" aria-hidden="true" tabindex="-1"></a>    <span class="at">colour=</span><span class="st">"red"</span>, <span class="at">size =</span> <span class="dv">4</span></span>
<span id="cb36-259"><a href="#cb36-259" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb36-260"><a href="#cb36-260" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-261"><a href="#cb36-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-262"><a href="#cb36-262" aria-hidden="true" tabindex="-1"></a>Il diagramma precedente presenta ciascun valore $\hat{y}_i = a + b x_i$ in funzione di $x_i$. Si vede che i valori predetti dal modello di regressione sono i punti che stanno sulla retta di regressione.</span>
<span id="cb36-263"><a href="#cb36-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-264"><a href="#cb36-264" aria-hidden="true" tabindex="-1"></a>In precedenza abbiamo detto che il residuo, ovvero la componente di ciascuna osservazione $y_i$ che non viene predetta dal modello di regressione, corrisponde alla *distanza verticale* tra il valore $y_i$ osservato e il valore $\hat{y}_i$ predetto dal modello di regressione:</span>
<span id="cb36-265"><a href="#cb36-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-266"><a href="#cb36-266" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-267"><a href="#cb36-267" aria-hidden="true" tabindex="-1"></a>e_i = y_i - (a + b x_i).</span>
<span id="cb36-268"><a href="#cb36-268" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-269"><a href="#cb36-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-270"><a href="#cb36-270" aria-hidden="true" tabindex="-1"></a>Nel caso nella prima osservazione, ad esempio abbiamo:</span>
<span id="cb36-271"><a href="#cb36-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-272"><a href="#cb36-272" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-273"><a href="#cb36-273" aria-hidden="true" tabindex="-1"></a>y_1 = (a + b x_1) + e_1</span>
<span id="cb36-274"><a href="#cb36-274" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-275"><a href="#cb36-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-276"><a href="#cb36-276" aria-hidden="true" tabindex="-1"></a>Abbiamo</span>
<span id="cb36-277"><a href="#cb36-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-280"><a href="#cb36-280" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-281"><a href="#cb36-281" aria-hidden="true" tabindex="-1"></a>kidiq<span class="sc">$</span>kid_score[<span class="dv">1</span>]</span>
<span id="cb36-282"><a href="#cb36-282" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-283"><a href="#cb36-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-284"><a href="#cb36-284" aria-hidden="true" tabindex="-1"></a>Dunque</span>
<span id="cb36-285"><a href="#cb36-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-286"><a href="#cb36-286" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-287"><a href="#cb36-287" aria-hidden="true" tabindex="-1"></a>e_1 = (a + b x_1) - y_1</span>
<span id="cb36-288"><a href="#cb36-288" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-289"><a href="#cb36-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-292"><a href="#cb36-292" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-293"><a href="#cb36-293" aria-hidden="true" tabindex="-1"></a>e_1 <span class="ot">&lt;-</span> kidiq<span class="sc">$</span>kid_score[<span class="dv">1</span>] <span class="sc">-</span> (a <span class="sc">+</span> b <span class="sc">*</span> kidiq<span class="sc">$</span>mom_iq[<span class="dv">1</span>])</span>
<span id="cb36-294"><a href="#cb36-294" aria-hidden="true" tabindex="-1"></a>e_1</span>
<span id="cb36-295"><a href="#cb36-295" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-296"><a href="#cb36-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-297"><a href="#cb36-297" aria-hidden="true" tabindex="-1"></a>Ciò significa che il valore osservato $y_1 = 65$ viene scomposto dal modello di regressione in due componenti. La componente deterministica $\hat{y}_1$, predicibile da $x_1$, è</span>
<span id="cb36-298"><a href="#cb36-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-301"><a href="#cb36-301" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-302"><a href="#cb36-302" aria-hidden="true" tabindex="-1"></a>yhat_1 <span class="ot">&lt;-</span> a <span class="sc">+</span> b <span class="sc">*</span> kidiq<span class="sc">$</span>mom_iq[<span class="dv">1</span>]</span>
<span id="cb36-303"><a href="#cb36-303" aria-hidden="true" tabindex="-1"></a>yhat_1</span>
<span id="cb36-304"><a href="#cb36-304" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-305"><a href="#cb36-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-306"><a href="#cb36-306" aria-hidden="true" tabindex="-1"></a>La somma della componente deterministica e della componente erratica, ovviamente, riproduce il valore osservato.</span>
<span id="cb36-307"><a href="#cb36-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-310"><a href="#cb36-310" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-311"><a href="#cb36-311" aria-hidden="true" tabindex="-1"></a>yhat_1 <span class="sc">+</span> e_1</span>
<span id="cb36-312"><a href="#cb36-312" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-313"><a href="#cb36-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-314"><a href="#cb36-314" aria-hidden="true" tabindex="-1"></a>Se sommiamo tutti i residui calcolati rispetto alla retta di regressione dei minimi quadrati otteniamo zero:</span>
<span id="cb36-315"><a href="#cb36-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-318"><a href="#cb36-318" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-319"><a href="#cb36-319" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(fm<span class="sc">$</span>res)</span>
<span id="cb36-320"><a href="#cb36-320" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-321"><a href="#cb36-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-322"><a href="#cb36-322" aria-hidden="true" tabindex="-1"></a><span class="fu">### Trasformazione dei dati</span></span>
<span id="cb36-323"><a href="#cb36-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-324"><a href="#cb36-324" aria-hidden="true" tabindex="-1"></a>In generale, per variabili a livello di scala ad intervalli, non è possibile assegnare un'interpretazione utile all'intercetta del modello di regressione lineare. L'intercetta ci dice infatti qual è il valore atteso della $y$ quando $x = 0$. Ma, se la variabile $x$ è misurata su scala ad intervalli, il valore $x = 0$ è arbitrario e non corrisponde "all'assenza di intensità" della variabile $x$. Un valore pari a 0 del QI della madre non vuol dire che l'intelligenza della madre sia nulla (un'affermazione, questa, che è difficile da capire), ma semplicemente che il punteggio del test usato per misurare il QI della madre assume valore 0 (qualcosa che, comunque, in pratica non succederà mai). Quindi è di poco interesse sapere qual è il valore medio del QI del bambino quando test usato per misurare il QI della madre ha valore 0. Per potere fornire all'intercetta del modello di regressione un'interpretazione più utile dobbiamo trasformare le osservazioni $x$.</span>
<span id="cb36-325"><a href="#cb36-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-326"><a href="#cb36-326" aria-hidden="true" tabindex="-1"></a>Esprimiamo $x$ come differenza dalla media. Chiamiamo questa nuova variabile $xd$:</span>
<span id="cb36-327"><a href="#cb36-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-330"><a href="#cb36-330" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-331"><a href="#cb36-331" aria-hidden="true" tabindex="-1"></a>kidiq<span class="sc">$</span>xd <span class="ot">&lt;-</span> kidiq<span class="sc">$</span>mom_iq <span class="sc">-</span> <span class="fu">mean</span>(kidiq<span class="sc">$</span>mom_iq)</span>
<span id="cb36-332"><a href="#cb36-332" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-333"><a href="#cb36-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-334"><a href="#cb36-334" aria-hidden="true" tabindex="-1"></a>Se ora usiamo le coppie di osservazioni $xd_i, y_i$, il diagramma a dispersione assume la forma seguente.</span>
<span id="cb36-335"><a href="#cb36-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-338"><a href="#cb36-338" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-339"><a href="#cb36-339" aria-hidden="true" tabindex="-1"></a>kidiq <span class="sc">%&gt;%</span> </span>
<span id="cb36-340"><a href="#cb36-340" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> xd, <span class="at">y =</span> kid_score)) <span class="sc">+</span> </span>
<span id="cb36-341"><a href="#cb36-341" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb36-342"><a href="#cb36-342" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb36-343"><a href="#cb36-343" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(</span>
<span id="cb36-344"><a href="#cb36-344" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">x=</span><span class="fu">mean</span>(xd), <span class="at">y=</span><span class="fu">mean</span>(kid_score)), </span>
<span id="cb36-345"><a href="#cb36-345" aria-hidden="true" tabindex="-1"></a>    <span class="at">colour=</span><span class="st">"red"</span>, <span class="at">size =</span> <span class="dv">4</span></span>
<span id="cb36-346"><a href="#cb36-346" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb36-347"><a href="#cb36-347" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-348"><a href="#cb36-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-349"><a href="#cb36-349" aria-hidden="true" tabindex="-1"></a>Quello che abbiamo fatto è stato di *traslare rigidamente* la nube di punti sul piano cartesiano di una quantità pari alla distanza tra $\bar{x}$ e l'origine. Dunque, le *relazioni spaziali* tra i punti del diagramma a dispersione restano immutate. Di conseguenza, la pendenza della retta di regressione calcolata sui dati trasformati è uguale a quella che si trova nel caso dei dati non trasformati. Ciò che cambia è il valore dell'intercetta.</span>
<span id="cb36-350"><a href="#cb36-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-353"><a href="#cb36-353" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-354"><a href="#cb36-354" aria-hidden="true" tabindex="-1"></a>fm1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(kid_score <span class="sc">~</span> xd, <span class="at">data =</span> kidiq)</span>
<span id="cb36-355"><a href="#cb36-355" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(fm1)</span>
<span id="cb36-356"><a href="#cb36-356" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-357"><a href="#cb36-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-358"><a href="#cb36-358" aria-hidden="true" tabindex="-1"></a>L'intercetta corrisponde al punto sull'asse $y$ dove la retta di regressione interseca l'ordinata. Ma, nel caso dei dati trasformati, dato che abbiamo traslato i punti di una quantità pari a $x - \bar{x}$, il valore $xd = 0$ corrisponde a $x = \bar{x}$ nel caso dei dati grezzi. Dunque, per i dati trasformati $xd_i, y_i$, l'intercetta corrisponderà al valore atteso della $y$ in corrispondenza del valore medio della variabile $x$ sulla scala dei dati non trasformati (ovvero $\bar{x}$). In altre parole, l'intercetta del modello di regressione lineare calcolata sui dati trasformati corrisponde al QI medio dei bambini in corrispondenza del QI medio delle madri.</span>
<span id="cb36-359"><a href="#cb36-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-360"><a href="#cb36-360" aria-hidden="true" tabindex="-1"></a><span class="fu">### Il metodo dei minimi quadrati</span></span>
<span id="cb36-361"><a href="#cb36-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-362"><a href="#cb36-362" aria-hidden="true" tabindex="-1"></a>Ora che abbiamo visto come interpretare il coefficienti di regressione, chiediamoci come vengono calcolati. La procedura generale è stata brevemente descritta in precedenza. Vediamo ora come si giunge alla conclusione descritta sopra usando una simulazione.</span>
<span id="cb36-363"><a href="#cb36-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-364"><a href="#cb36-364" aria-hidden="true" tabindex="-1"></a>Il problema è di trovare i valori $a$ e $b$ tali per cui la quantità $\sum_{i=1}^{n}{(y_i - (a + b x_i))^2}$ assume il valore minore possibile. Questo è un problema di minimizzazione rispetto a due parametri. Per dare un'idea di come si fa, semplifichiamo il problema e supponiamo che uno dei due parametri sia noto, ad esempio $a$, così ci resta una sola incognita.</span>
<span id="cb36-365"><a href="#cb36-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-366"><a href="#cb36-366" aria-hidden="true" tabindex="-1"></a>Credo una griglia di valori <span class="in">`b_grid`</span> possibili, ad esempio:</span>
<span id="cb36-367"><a href="#cb36-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-370"><a href="#cb36-370" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-371"><a href="#cb36-371" aria-hidden="true" tabindex="-1"></a>nrep <span class="ot">&lt;-</span> <span class="fl">1e5</span></span>
<span id="cb36-372"><a href="#cb36-372" aria-hidden="true" tabindex="-1"></a>b_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out =</span> nrep)</span>
<span id="cb36-373"><a href="#cb36-373" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-374"><a href="#cb36-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-375"><a href="#cb36-375" aria-hidden="true" tabindex="-1"></a>Definisco una funzione che calcola la quantità $\sum_{i=1}^{n}{(y_i - (a + b x_i))^2}$:</span>
<span id="cb36-376"><a href="#cb36-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-379"><a href="#cb36-379" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-380"><a href="#cb36-380" aria-hidden="true" tabindex="-1"></a>sse <span class="ot">&lt;-</span> <span class="cf">function</span>(a, b, x, y) {</span>
<span id="cb36-381"><a href="#cb36-381" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>((y <span class="sc">-</span> (a <span class="sc">+</span> b <span class="sc">*</span> x))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb36-382"><a href="#cb36-382" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb36-383"><a href="#cb36-383" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-384"><a href="#cb36-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-385"><a href="#cb36-385" aria-hidden="true" tabindex="-1"></a>Calcolo la somma degli errori quadratici per ciascun possibile valore <span class="in">`b_grid`</span>, fissando $a = 25.79978$.</span>
<span id="cb36-386"><a href="#cb36-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-389"><a href="#cb36-389" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-390"><a href="#cb36-390" aria-hidden="true" tabindex="-1"></a>sse_res <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, nrep)</span>
<span id="cb36-391"><a href="#cb36-391" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nrep) {</span>
<span id="cb36-392"><a href="#cb36-392" aria-hidden="true" tabindex="-1"></a>  sse_res[i] <span class="ot">&lt;-</span> <span class="fu">sse</span>(<span class="at">a =</span> <span class="fl">25.79978</span>, <span class="at">b =</span> b_grid[i], <span class="at">x =</span> kidiq<span class="sc">$</span>mom_iq, <span class="at">y =</span> kidiq<span class="sc">$</span>kid_score)</span>
<span id="cb36-393"><a href="#cb36-393" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb36-394"><a href="#cb36-394" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-395"><a href="#cb36-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-396"><a href="#cb36-396" aria-hidden="true" tabindex="-1"></a>Esaminiamo il risultato ottenuto.</span>
<span id="cb36-397"><a href="#cb36-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-400"><a href="#cb36-400" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-401"><a href="#cb36-401" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(</span>
<span id="cb36-402"><a href="#cb36-402" aria-hidden="true" tabindex="-1"></a>  b_grid, sse_res, <span class="at">type =</span> <span class="st">'l'</span></span>
<span id="cb36-403"><a href="#cb36-403" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb36-404"><a href="#cb36-404" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-405"><a href="#cb36-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-406"><a href="#cb36-406" aria-hidden="true" tabindex="-1"></a>Il risultato ottenuto con la simulazione</span>
<span id="cb36-407"><a href="#cb36-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-410"><a href="#cb36-410" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-411"><a href="#cb36-411" aria-hidden="true" tabindex="-1"></a>b_grid[<span class="fu">which.min</span>(sse_res)]</span>
<span id="cb36-412"><a href="#cb36-412" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-413"><a href="#cb36-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-414"><a href="#cb36-414" aria-hidden="true" tabindex="-1"></a>riproduce quello ottenuto per via analitica:</span>
<span id="cb36-415"><a href="#cb36-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-418"><a href="#cb36-418" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-419"><a href="#cb36-419" aria-hidden="true" tabindex="-1"></a>b</span>
<span id="cb36-420"><a href="#cb36-420" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-421"><a href="#cb36-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-422"><a href="#cb36-422" aria-hidden="true" tabindex="-1"></a>Una simulazione simile, ma computazionalmente più complessa, può essere usata per stimare simultaneamente entrambi i parametri. Ci siamo limitati qui ad una *proof of concept* del caso più semplice.</span>
<span id="cb36-423"><a href="#cb36-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-424"><a href="#cb36-424" aria-hidden="true" tabindex="-1"></a><span class="fu">### L'errore standard della regressione</span></span>
<span id="cb36-425"><a href="#cb36-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-426"><a href="#cb36-426" aria-hidden="true" tabindex="-1"></a>Il secondo obiettivo del modello statistico di regressione lineare è quello di stabilire *quanto sia grande la componente* $y$ predicibile da $x$, per ciascuna osservazione.</span>
<span id="cb36-427"><a href="#cb36-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-428"><a href="#cb36-428" aria-hidden="true" tabindex="-1"></a>Un indice assoluto della bontà di adattamento è fornito dalla deviazione standard dei residui, $s_e$, chiamata anche *errore standard della stima*. Uno stimatore non distorto della varianza dei residui nella popolazione è dato da</span>
<span id="cb36-429"><a href="#cb36-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-430"><a href="#cb36-430" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-431"><a href="#cb36-431" aria-hidden="true" tabindex="-1"></a>s^2_e = \frac{1}{n-2}\sum e_i^2</span>
<span id="cb36-432"><a href="#cb36-432" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-433"><a href="#cb36-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-434"><a href="#cb36-434" aria-hidden="true" tabindex="-1"></a>e quindi l'errore standard della stima sarà</span>
<span id="cb36-435"><a href="#cb36-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-436"><a href="#cb36-436" aria-hidden="true" tabindex="-1"></a><span class="in">```{=tex}</span></span>
<span id="cb36-437"><a href="#cb36-437" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{equation}</span></span>
<span id="cb36-438"><a href="#cb36-438" aria-hidden="true" tabindex="-1"></a><span class="in">s_e = \sqrt{\frac{1}{n-2}\sum e_i^2}.</span></span>
<span id="cb36-439"><a href="#cb36-439" aria-hidden="true" tabindex="-1"></a><span class="in">\end{equation}</span></span>
<span id="cb36-440"><a href="#cb36-440" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-441"><a href="#cb36-441" aria-hidden="true" tabindex="-1"></a>Si noti che questa è la stessa formula della varianza (dato che la media dei residui è zero), tranne per il fatto che al denominatore abbiamo $n-2$. Dato che, per calcolare $\hat{y}$ abbiamo usato due coefficienti ($a$ e $b$), si dice che "abbiamo perso due gradi di libertà".</span>
<span id="cb36-442"><a href="#cb36-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-443"><a href="#cb36-443" aria-hidden="true" tabindex="-1"></a>Dato che $s_e$ possiede la stessa unità di misura della variabile $y$, l'errore standard della stima può essere considerato come una sorta di "residuo medio." -- usando la stessa interpretazione che diamo alla deviazione standard in generale.</span>
<span id="cb36-444"><a href="#cb36-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-445"><a href="#cb36-445" aria-hidden="true" tabindex="-1"></a>Si noti che la formula precedente non fornisce la "deviazione standard dei residui nel campione" (quella formula avrebbe $n$ al denominatore). Invece, fornisce una *stima* della deviazione standard dei residui nella popolazione da cui il campione è stato estratto.</span>
<span id="cb36-446"><a href="#cb36-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-447"><a href="#cb36-447" aria-hidden="true" tabindex="-1"></a>Verifichiamo quanto detto con i dati a disposizione.</span>
<span id="cb36-448"><a href="#cb36-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-449"><a href="#cb36-449" aria-hidden="true" tabindex="-1"></a>I residui possono essere trovati nel modo seguente.</span>
<span id="cb36-450"><a href="#cb36-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-453"><a href="#cb36-453" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-454"><a href="#cb36-454" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> kidiq<span class="sc">$</span>kid_score <span class="sc">-</span> (a <span class="sc">+</span> b <span class="sc">*</span> kidiq<span class="sc">$</span>mom_iq)</span>
<span id="cb36-455"><a href="#cb36-455" aria-hidden="true" tabindex="-1"></a>e[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]</span>
<span id="cb36-456"><a href="#cb36-456" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-457"><a href="#cb36-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-458"><a href="#cb36-458" aria-hidden="true" tabindex="-1"></a>Oppure nel modo seguente.</span>
<span id="cb36-459"><a href="#cb36-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-462"><a href="#cb36-462" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-463"><a href="#cb36-463" aria-hidden="true" tabindex="-1"></a>fm<span class="sc">$</span>residuals[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]</span>
<span id="cb36-464"><a href="#cb36-464" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-465"><a href="#cb36-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-466"><a href="#cb36-466" aria-hidden="true" tabindex="-1"></a>Calcolo il residuo medio, prendendo il valore assoluto.</span>
<span id="cb36-467"><a href="#cb36-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-470"><a href="#cb36-470" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-471"><a href="#cb36-471" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">abs</span>(e))</span>
<span id="cb36-472"><a href="#cb36-472" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-473"><a href="#cb36-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-474"><a href="#cb36-474" aria-hidden="true" tabindex="-1"></a>L'errore standard della regressione è</span>
<span id="cb36-475"><a href="#cb36-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-478"><a href="#cb36-478" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-479"><a href="#cb36-479" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">sum</span>(e<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> (<span class="fu">length</span>(e) <span class="sc">-</span> <span class="dv">2</span>))</span>
<span id="cb36-480"><a href="#cb36-480" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-481"><a href="#cb36-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-482"><a href="#cb36-482" aria-hidden="true" tabindex="-1"></a>I due numeri non sono uguali, ma possiamo dire che hanno lo stesso ordine di grandezza.</span>
<span id="cb36-483"><a href="#cb36-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-484"><a href="#cb36-484" aria-hidden="true" tabindex="-1"></a>Se usiamo la funzione <span class="in">`lm()`</span> otteniamo lo stesso valore, chiamato <span class="in">`Residual standard error`</span>.</span>
<span id="cb36-485"><a href="#cb36-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-488"><a href="#cb36-488" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-489"><a href="#cb36-489" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fm)</span>
<span id="cb36-490"><a href="#cb36-490" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-491"><a href="#cb36-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-492"><a href="#cb36-492" aria-hidden="true" tabindex="-1"></a><span class="fu">## Indice di determinazione</span></span>
<span id="cb36-493"><a href="#cb36-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-494"><a href="#cb36-494" aria-hidden="true" tabindex="-1"></a>Un importante risultato dei minimi quadrati riguarda la cosiddetta *scomposizione della devianza* mediante la quale si definisce l'indice di determinazione, il quale fornisce una misura relativa della bontà di adattamento del modello di regressione ai dati del campione. Per una generica osservazione $x_i, y_i$, la variazione di $y_i$ rispetto alla media $\bar{y}$ può essere descritta come la somma di due componenti: il residuo $e_i=y_i- \hat{y}_i$ e lo scarto di $\hat{y}_i$ rispetto alla media $\bar{y}$:</span>
<span id="cb36-495"><a href="#cb36-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-496"><a href="#cb36-496" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-497"><a href="#cb36-497" aria-hidden="true" tabindex="-1"></a>y_i - \bar{y} = (y_i- \hat{y}_i) + (\hat{y}_i - \bar{y}) = e_i + (\hat{y}_i - \bar{y}).</span>
<span id="cb36-498"><a href="#cb36-498" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb36-499"><a href="#cb36-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-500"><a href="#cb36-500" aria-hidden="true" tabindex="-1"></a>Se consideriamo tutte le osservazioni, la devianza delle $y$ può essere scomposta nel seguente modo:</span>
<span id="cb36-501"><a href="#cb36-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-502"><a href="#cb36-502" aria-hidden="true" tabindex="-1"></a><span class="in">```{=tex}</span></span>
<span id="cb36-503"><a href="#cb36-503" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{align}</span></span>
<span id="cb36-504"><a href="#cb36-504" aria-hidden="true" tabindex="-1"></a><span class="in"> \sum (y_i - \bar{y})^2 &amp;= \sum \left[ e_i + (\hat{y}_i - \bar{y})</span></span>
<span id="cb36-505"><a href="#cb36-505" aria-hidden="true" tabindex="-1"></a><span class="in"> \right]^2 </span></span>
<span id="cb36-506"><a href="#cb36-506" aria-hidden="true" tabindex="-1"></a><span class="in"> = \sum e_i^2 + \sum (\hat{y}_i - \bar{y})^2 + 2 \sum e_i (\hat{y}_i -</span></span>
<span id="cb36-507"><a href="#cb36-507" aria-hidden="true" tabindex="-1"></a><span class="in"> \bar{y}) \notag</span></span>
<span id="cb36-508"><a href="#cb36-508" aria-hidden="true" tabindex="-1"></a><span class="in">\end{align}</span></span>
<span id="cb36-509"><a href="#cb36-509" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-510"><a href="#cb36-510" aria-hidden="true" tabindex="-1"></a>Per i vincoli imposti sul modello statistico di regressione, il doppio prodotto si annulla, infatti</span>
<span id="cb36-511"><a href="#cb36-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-512"><a href="#cb36-512" aria-hidden="true" tabindex="-1"></a><span class="in">```{=tex}</span></span>
<span id="cb36-513"><a href="#cb36-513" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{align}</span></span>
<span id="cb36-514"><a href="#cb36-514" aria-hidden="true" tabindex="-1"></a><span class="in">\sum e_i (\hat{y}_i - \bar{y}) &amp;= \sum e_i \hat{y}_i - \bar{y}\sum e_i = \sum e_i (a + b x_i) \notag \\</span></span>
<span id="cb36-515"><a href="#cb36-515" aria-hidden="true" tabindex="-1"></a><span class="in">&amp;= a \sum e_i + b \sum e_i x_i = 0 \notag</span></span>
<span id="cb36-516"><a href="#cb36-516" aria-hidden="true" tabindex="-1"></a><span class="in">\end{align}</span></span>
<span id="cb36-517"><a href="#cb36-517" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-518"><a href="#cb36-518" aria-hidden="true" tabindex="-1"></a>Il termine $b \sum e_i x_i$ è uguale a zero perché, come vedremo in seguito, i coefficienti di regressione vengono calcolati in modo tale da rendere nulla $\mbox{Cov}(e, x)$. Di conseguenza, il termine precedente deve essere nullo.</span>
<span id="cb36-519"><a href="#cb36-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-520"><a href="#cb36-520" aria-hidden="true" tabindex="-1"></a>Possiamo dunque concludere che la devianza totale ($\mbox{dev}_T$) si scompone nella somma di devianza d'errore (o devianza non spiegata) ($\mbox{dev}_E$) e devianza di regressione (o devianza spiegata) ($\mbox{dev}_T$):</span>
<span id="cb36-521"><a href="#cb36-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-522"><a href="#cb36-522" aria-hidden="true" tabindex="-1"></a><span class="in">```{=tex}</span></span>
<span id="cb36-523"><a href="#cb36-523" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{align}</span></span>
<span id="cb36-524"><a href="#cb36-524" aria-hidden="true" tabindex="-1"></a><span class="in">\underbrace{\sum_{i=1}^n (y_i - \bar{y})^2}_{\tiny{\text{Devianza</span></span>
<span id="cb36-525"><a href="#cb36-525" aria-hidden="true" tabindex="-1"></a><span class="in">totale}}} &amp;= \underbrace{\sum_{i=1}^n e_i^2}_{\tiny{\text{Devianza</span></span>
<span id="cb36-526"><a href="#cb36-526" aria-hidden="true" tabindex="-1"></a><span class="in">di dispersione}}} + \underbrace{\sum_{i=1}^n  (\hat{y}_i -</span></span>
<span id="cb36-527"><a href="#cb36-527" aria-hidden="true" tabindex="-1"></a><span class="in">\bar{y})^2}_{\tiny{\text{Devianza di regressione}}} \notag</span></span>
<span id="cb36-528"><a href="#cb36-528" aria-hidden="true" tabindex="-1"></a><span class="in">\end{align}</span></span>
<span id="cb36-529"><a href="#cb36-529" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-530"><a href="#cb36-530" aria-hidden="true" tabindex="-1"></a>La devianza di regressione, $\mbox{dev_R} \triangleq \mbox{dev_T} - \mbox{dev_E}$, indica dunque la riduzione degli errori al quadrato che è imputabile alla regressione lineare. Il rapporto $\mbox{dev_R}/\mbox{dev_T}$, detto *indice di determinazione*, esprime tale riduzione degli errori in termini proporzionali e definisce il coefficiente di correlazione al quadrato:</span>
<span id="cb36-531"><a href="#cb36-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-532"><a href="#cb36-532" aria-hidden="true" tabindex="-1"></a><span class="in">```{=tex}</span></span>
<span id="cb36-533"><a href="#cb36-533" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{equation}</span></span>
<span id="cb36-534"><a href="#cb36-534" aria-hidden="true" tabindex="-1"></a><span class="in">R^2 \triangleq \frac{\mbox{dev_R}}{\mbox{dev_T}} = 1 - \frac{\mbox{dev_E}}{\mbox{dev_T}}.</span></span>
<span id="cb36-535"><a href="#cb36-535" aria-hidden="true" tabindex="-1"></a><span class="in">\end{equation}</span></span>
<span id="cb36-536"><a href="#cb36-536" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-537"><a href="#cb36-537" aria-hidden="true" tabindex="-1"></a>Quando l'insieme di tutte le deviazioni della $y$ dalla media è spiegato dall'insieme di tutte le deviazioni della variabile teorica $\hat{y}$ dalla media, si ha che l'adattamento (o accostamento) del modello al campione di dati è perfetto, la devianza residua è nulla ed $r^2 = 1$; nel caso opposto, la variabilità totale coincide con quella residua, per cui $r^2 = 0$. Tra questi due estremi, $r$ indica l'intensità della relazione lineare tra le due variabili e $r^2$, con $0 \leq r^2 \leq 1$, esprime la porzione della devianza totale della $y$ che è spiegata dalla regressione lineare sulla $x$.</span>
<span id="cb36-538"><a href="#cb36-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-539"><a href="#cb36-539" aria-hidden="true" tabindex="-1"></a>Per l'esempio in discussione abbiamo quanto segue. La devianza totale è</span>
<span id="cb36-540"><a href="#cb36-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-543"><a href="#cb36-543" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-544"><a href="#cb36-544" aria-hidden="true" tabindex="-1"></a>dev_t <span class="ot">&lt;-</span> <span class="fu">sum</span>((kidiq<span class="sc">$</span>kid_score <span class="sc">-</span> <span class="fu">mean</span>(kidiq<span class="sc">$</span>kid_score))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb36-545"><a href="#cb36-545" aria-hidden="true" tabindex="-1"></a>dev_t</span>
<span id="cb36-546"><a href="#cb36-546" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-547"><a href="#cb36-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-548"><a href="#cb36-548" aria-hidden="true" tabindex="-1"></a>La devianza spiegata è</span>
<span id="cb36-549"><a href="#cb36-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-552"><a href="#cb36-552" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-553"><a href="#cb36-553" aria-hidden="true" tabindex="-1"></a>dev_r <span class="ot">&lt;-</span> <span class="fu">sum</span>((fm<span class="sc">$</span>fitted.values <span class="sc">-</span> <span class="fu">mean</span>(kidiq<span class="sc">$</span>kid_score))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb36-554"><a href="#cb36-554" aria-hidden="true" tabindex="-1"></a>dev_r</span>
<span id="cb36-555"><a href="#cb36-555" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-556"><a href="#cb36-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-557"><a href="#cb36-557" aria-hidden="true" tabindex="-1"></a>L'indice di determinazione è</span>
<span id="cb36-558"><a href="#cb36-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-561"><a href="#cb36-561" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-562"><a href="#cb36-562" aria-hidden="true" tabindex="-1"></a>R2 <span class="ot">&lt;-</span> dev_r <span class="sc">/</span> dev_t</span>
<span id="cb36-563"><a href="#cb36-563" aria-hidden="true" tabindex="-1"></a>R2</span>
<span id="cb36-564"><a href="#cb36-564" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-565"><a href="#cb36-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-566"><a href="#cb36-566" aria-hidden="true" tabindex="-1"></a>Nell'output di <span class="in">`lm()`</span> un tale valore è chiamato <span class="in">`Multiple R-squared`</span>.</span>
<span id="cb36-567"><a href="#cb36-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-570"><a href="#cb36-570" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-571"><a href="#cb36-571" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fm)</span>
<span id="cb36-572"><a href="#cb36-572" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-573"><a href="#cb36-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-574"><a href="#cb36-574" aria-hidden="true" tabindex="-1"></a>Il risultato ottenuto si può interpretare dicendo che circa il 20% della variabilità dei punteggi del QI dei bambini può essere predetto conoscendo il QI delle madri.</span>
<span id="cb36-575"><a href="#cb36-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-576"><a href="#cb36-576" aria-hidden="true" tabindex="-1"></a><span class="fu">### Inferenza sul modello di regressione</span></span>
<span id="cb36-577"><a href="#cb36-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-578"><a href="#cb36-578" aria-hidden="true" tabindex="-1"></a>La discussione precedente era tutta basata sulla trattazione "classica" del modello lineare, ovvero una trattazione basata sulle stime di massima verosimiglianza (se $y \sim \mathcal{N}(\alpha + \beta x, \sigma)$, allora le stime dei minimi quadrati coincidono con le stime di massima verosimiglianza). In altre parole, nella discussione precedente non abbiamo considerato in alcun modo le distribuzioni a priori dei parametri $\alpha$ e $\beta$. I risultati precedenti si confermano, in un contesto bayesiano, se e solo se imponiamo sui parametri delle distribuzioni a priori non informative (cioè, uniformi). In tali circostanze, le stime di massima verosimiglianza risultano identiche al massimo a posteriori bayesiano.</span>
<span id="cb36-579"><a href="#cb36-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-580"><a href="#cb36-580" aria-hidden="true" tabindex="-1"></a>Detto questo, il tema dell'inferenza viene trattato dall'approccio frequentista costruendo la "distribuzione campionaria" dei parametri (ovvero la distribuzione dei valori che i parametri otterrebbero in infiniti campioni casuali ($x, y$) di ampiezza $n$ estratti dalla medesima popolazione) e poi calcolando gli errori standard dei parametri e gli intervalli di fiducia dei parametri. Una domanda frequente è, per esempio, se la pendenza della retta di regressione sia maggiore di zero. Per rispondere a tale domanda l'approccio frequentista calcola l'intervallo di fiducia al 95% per il parametro $\beta$. Se tale intervallo non include lo zero, e se il limite inferiore di tale intervallo è maggiore di zero, allora si conclude, con un grado di confidenza del 95%, che il vero parametro $\beta$ nella popolazione è maggiore di zero. Ovvero, si conclude che vi sono evidenze di un'associazione lineare positiva tra $x$ e $y$.</span>
<span id="cb36-581"><a href="#cb36-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-582"><a href="#cb36-582" aria-hidden="true" tabindex="-1"></a>Alla stessa conclusione si può arrivare calcolando, in un ottica bayesiana, l'intervallo di credibilità al 95% per il parametro $\beta$. I due intervalli sono identici se usiamo una distribuzione a priori piatta. Sono invece diversi se usiamo una distribuzione a priori debolmente informativa, oppure informativa.</span>
<span id="cb36-583"><a href="#cb36-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-584"><a href="#cb36-584" aria-hidden="true" tabindex="-1"></a>Solitamente si usa una distribuzione a priori debolmente informativa centrata sullo zero. In tali circostanze, l'uso della distribuzione a priori ha solo un effetto di *regolarizzazione*, ovvero di riduzione del peso delle osservazioni estreme -- un tale risultato statistico è molto desiderabile, ma è difficile da ottenere in un contesto frequentista. Vedremo nel prossimo capitolo come può essere svolta l'inferenza sui coefficienti del modello di regressione lineare in un contesto bayesiano.</span>
<span id="cb36-585"><a href="#cb36-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-586"><a href="#cb36-586" aria-hidden="true" tabindex="-1"></a><span class="fu">## Commenti e considerazioni finali {.unnumbered}</span></span>
<span id="cb36-587"><a href="#cb36-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-588"><a href="#cb36-588" aria-hidden="true" tabindex="-1"></a>Il modello lineare bivariato viene usato per descrivere la relazione tra due variabili e per determinare il segno e l'intensità di tale relazione. Inoltre, il modello lineare ci consente di prevedere il valore della variabile dipendente in base al valore assunto dalla variabile indipendente.</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>