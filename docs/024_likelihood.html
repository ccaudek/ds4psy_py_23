<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Data Science per psicologi - 15&nbsp; La funzione di verosimiglianza</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./bayes_inference.html" rel="next">
<link href="./023_cont_rv_distr.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"><div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">La funzione di verosimiglianza</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Data Science per psicologi</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/ccaudek/data_science/" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Benvenuti</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">Prefazione</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./basics.html" class="sidebar-item-text sidebar-link">Parte 1: Nozioni di base</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./001_key_notions.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Concetti chiave</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./005_measurement.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">La misurazione in psicologia</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./007_freq_distr.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Analisi esplorativa dei dati</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./011_loc_scale.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Indici di posizione e di scala</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./012_correlation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Le relazioni tra variabili</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./013_penguins.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Manipolazione e visualizzazione dei dati in <span class="math inline">\(\mathsf{R}\)</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./prob.html" class="sidebar-item-text sidebar-link">Parte 2: Il calcolo delle probabilità</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./015_prob_intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">La logica dell’incerto</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./016_conditional_prob.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Probabilità condizionata: significato, teoremi, eventi indipendenti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./017_bayes_theorem.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Il teorema di Bayes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./018_expval_var.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Indici di posizione, di varianza e di associazione di variabili casuali</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./019_joint_prob.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Probabilità congiunta</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./020_density_func.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">La densità di probabilità</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./distr.html" class="sidebar-item-text sidebar-link">Parte 3: Distribuzioni di v.c. discrete e continue</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./022_discr_rv_distr.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. discrete</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./023_cont_rv_distr.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. continue</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./024_likelihood.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">La funzione di verosimiglianza</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./bayes_inference.html" class="sidebar-item-text sidebar-link">Parte 4: Inferenza bayesiana</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./025_intro_bayes.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Credibilità, modelli e parametri</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./026_subj_prop.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Pensare ad una proporzione in termini soggettivi</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./029_conjugate_families.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./030_balance_prior_post.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">L’influenza della distribuzione a priori</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./036_posterior_sim.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Approssimazione della distribuzione a posteriori</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./040_beta_binomial_mod.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Il modello beta-binomiale in linguaggio Stan</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./041_mcmc_diagnostics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Diagnostica delle catene markoviane</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./045_summarize_posterior.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Sintesi a posteriori</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./046_bayesian_prediction.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">La predizione bayesiana</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./050_normal_normal_mod.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Inferenza sul parametro <span class="math inline">\(\mu\)</span> (media di una v.c. Normale)</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./regression.html" class="sidebar-item-text sidebar-link">Parte 5: Regressione lineare</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./051_reglin1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./052_reglin2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Regressione lineare bivariata</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./053_reglin3.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Modello di regressione in linguaggio Stan</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./054_reglin4.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Inferenza sul modello lineare</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./055_reglin5.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Confronto tra due gruppi indipendenti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./056_pred_check.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Predictive checks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./060_anova.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Confronto tra le medie di tre o più gruppi</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./070_mod_hier.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Modello gerarchico</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./071_mod_hier_sim.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Modello gerarchico: simulazioni</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./entropy.html" class="sidebar-item-text sidebar-link">Parte 6: Entropia</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./090_entropy.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Entropia</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./091_kl.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">La divergenza di Kullback-Leibler</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./092_info_criterion.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Criterio di informazione e convalida incrociata</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./frequentist_inference.html" class="sidebar-item-text sidebar-link">Parte 7: Inferenza frequentista</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./220_intro_frequentist.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Legge dei grandi numeri</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./221_conf_interv.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Intervallo fiduciale</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./225_distr_camp_mean.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Distribuzione campionaria</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./226_test_ipotesi.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Significatività statistica</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./227_ttest.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Inferenza sulle medie</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./228_limiti_stat_frequentista.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Limiti dell’inferenza frequentista</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./999_refs.html" class="sidebar-item-text sidebar-link">Riferimenti bibliografici</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">Appendici</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a01_math_symbols.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Simbologia di base</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a02_number_sets.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Numeri binari, interi, razionali, irrazionali e reali</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a03_set_theory.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Insiemi</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a04_summation_notation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Simbolo di somma (sommatorie)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a05_calculus_notation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Per liberarvi dai terrori preliminari</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a10_markov_chains.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Le catene di Markov</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a15_stan_lang.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Programmare in Stan</span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><!-- margin-sidebar --><div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Sommario</h2>
   
  <ul>
<li>
<a href="#modello-binomiale" id="toc-modello-binomiale" class="nav-link active" data-scroll-target="#modello-binomiale"><span class="toc-section-number">15.1</span>  Modello binomiale</a>
  <ul class="collapse">
<li><a href="#interpretazione" id="toc-interpretazione" class="nav-link" data-scroll-target="#interpretazione"><span class="toc-section-number">15.1.1</span>  Interpretazione</a></li>
  <li><a href="#la-log-verosimiglianza" id="toc-la-log-verosimiglianza" class="nav-link" data-scroll-target="#la-log-verosimiglianza"><span class="toc-section-number">15.1.2</span>  La log-verosimiglianza</a></li>
  </ul>
</li>
  <li>
<a href="#modello-gaussiano" id="toc-modello-gaussiano" class="nav-link" data-scroll-target="#modello-gaussiano"><span class="toc-section-number">15.2</span>  Modello gaussiano</a>
  <ul class="collapse">
<li><a href="#una-singola-osservazione" id="toc-una-singola-osservazione" class="nav-link" data-scroll-target="#una-singola-osservazione"><span class="toc-section-number">15.2.1</span>  Una singola osservazione</a></li>
  <li><a href="#un-campione-di-osservazioni" id="toc-un-campione-di-osservazioni" class="nav-link" data-scroll-target="#un-campione-di-osservazioni"><span class="toc-section-number">15.2.2</span>  Un campione di osservazioni</a></li>
  <li><a href="#massima-verosimiglianza" id="toc-massima-verosimiglianza" class="nav-link" data-scroll-target="#massima-verosimiglianza"><span class="toc-section-number">15.2.3</span>  Massima verosimiglianza</a></li>
  </ul>
</li>
  <li><a href="#commenti-e-considerazioni-finali" id="toc-commenti-e-considerazioni-finali" class="nav-link" data-scroll-target="#commenti-e-considerazioni-finali">Commenti e considerazioni finali</a></li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-likelihood" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">La funzione di verosimiglianza</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Codice</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header><p>La verosimiglianza viene utilizzata sia nell’inferenza bayesiana che in quella frequentista. In entrambi i paradigmi di inferenza, il suo ruolo è quello di quantificare la forza con la quale i dati osservati supportano i possibili valori dei parametri sconosciuti di un modello statistico.</p>
<div id="def-likelihood" class="theorem definition">
<p><span class="theorem-title"><strong>Definizione 15.1 </strong></span>La <em>funzione di verosimiglianza</em> <span class="math inline">\(\mathcal{L}(\theta \mid y) = f(y \mid \theta), \theta \in \Theta,\)</span> è la funzione di massa o di densità di probabilità dei dati <span class="math inline">\(y\)</span> vista come una funzione del parametro sconosciuto (o dei parametri sconosciuti) <span class="math inline">\(\theta\)</span>.</p>
</div>
<p>Detto in altre parole, la funzione di verosimiglianza e la funzione di (massa o densità di) probabilità sono formalmente identiche, ma è completamente diversa la loro interpretazione:</p>
<ul>
<li>nel caso della funzione di massa o di densità di probabilità, la distribuzione del vettore casuale delle osservazioni campionarie <span class="math inline">\(y\)</span> dipende dai valori assunti dal parametro (o dai parametri) <span class="math inline">\(\theta\)</span> – per esempio, nel caso della distribuzione binomiale, fissata <span class="math inline">\(\theta\)</span> (probabilità di successo) a 0.5, la probabilità di osservare <span class="math inline">\(y = 0, \dots, 10\)</span> successi in <span class="math inline">\(n\)</span> prove è determinata in maniera univoca (se il valore del parametro <span class="math inline">\(\theta\)</span> è noto, quello che resta da stabilire è la probabilità da assegnare a ciascuno degli esiti <span class="math inline">\(y\)</span> possibili);</li>
<li>nel caso della la funzione di verosimiglianza la credibilità assegnata a ciascun possibile valore <span class="math inline">\(\theta\)</span> viene determinata avendo acquisita l’informazione campionaria <span class="math inline">\(y\)</span> che rappresenta l’elemento condizionante (in questo secondo caso, <span class="math inline">\(y\)</span> è noto, ma <span class="math inline">\(\theta\)</span> è ignoto; ci chiediamo quale sia la credibilità relativa di ciascuno dei possibili valori <span class="math inline">\(\theta\)</span>, avendo osservato un determinato <span class="math inline">\(y\)</span>).</li>
</ul>
<p>La funzione di verosimiglianza descrive dunque in termini relativi il sostegno empirico che <span class="math inline">\(\theta \in \Theta\)</span> riceve da <span class="math inline">\(y\)</span>. Infatti, la funzione di verosimiglianza assume forme diverse al variare di <span class="math inline">\(y\)</span>. Possiamo dunque pensare alla funzione di verosimiglianza come alla risposta alla seguente domanda: avendo osservato i dati <span class="math inline">\(y\)</span>, quanto risultano (relativamente) credibili i diversi valori del parametro <span class="math inline">\(\theta\)</span>? In termini più formali possiamo dire che, sulla base dei dati, <span class="math inline">\(\theta_1 \in \Theta\)</span> risulta più credibile di <span class="math inline">\(\theta_2 \in \Theta\)</span> quale indice del modello probabilistico generatore dei dati se <span class="math inline">\(\mathcal{L}(\theta_1) &gt; \mathcal{L}(\theta_1)\)</span>.</p>
<p>Si noti un punto importante: la funzione <span class="math inline">\(\mathcal{L}(\theta \mid y)\)</span> non è una funzione di densità. Infatti, essa non racchiude un’area unitaria.</p>
<section id="modello-binomiale" class="level2" data-number="15.1"><h2 data-number="15.1" class="anchored" data-anchor-id="modello-binomiale">
<span class="header-section-number">15.1</span> Modello binomiale</h2>
<p>Per chiarire il concetto di verosimiglianza consideriamo innanzitutto il caso più semplice, ovvero quello Binomiale.</p>
<p>Per <span class="math inline">\(n\)</span> prove Bernoulliane indipendenti, le quali producono <span class="math inline">\(y\)</span> successi e (<span class="math inline">\(n-y\)</span>) insuccessi, la funzione nucleo di verosimiglianza (ovvero, la funzione di verosimiglianza da cui sono state escluse tutte le costanti moltiplicative che non hanno alcun effetto su <span class="math inline">\(\hat{\theta}\)</span>) è</p>
<p><span id="eq-like-binomial-kernel"><span class="math display">\[
\mathcal{L}(p \mid y) = \theta^y (1-\theta)^{n - y}.\notag
\tag{15.1}\]</span></span></p>
<p>Per fare un esempio pratico, consideriamo la ricerca di <span class="citation" data-cites="zetschefuture2019">Zetsche et al. (<a href="999_refs.html#ref-zetschefuture2019" role="doc-biblioref">2019</a>)</span>. Questi ricercatori hanno trovato che, su 30 pazienti clinicamente depressi, 23 manifestavano delle aspettative distorsione negativamente relativamente al loro umore futuro. Se i dati di <span class="citation" data-cites="zetschefuture2019">Zetsche et al. (<a href="999_refs.html#ref-zetschefuture2019" role="doc-biblioref">2019</a>)</span> vengono riassunti mediante una proporzione (ovvero, 23/30), allora è sensato adottare un modello probabilistico binomiale quale meccanismo generatore dei dati:</p>
<p><span id="eq-binomialmodel"><span class="math display">\[
y  \sim \mbox{Bin}(n, \theta),
\tag{15.2}\]</span></span></p>
<p>laddove <span class="math inline">\(\theta\)</span> è la probabiltà che una prova Bernoulliana assuma il valore 1 e <span class="math inline">\(n\)</span> corrisponde al numero di prove Bernoulliane. Questo modello assume che le prove Bernoulliane <span class="math inline">\(y\)</span> che costituiscono il campione siano tra loro indipendenti e che ciascuna abbia la stessa probabilità <span class="math inline">\(\theta \in [0, 1]\)</span> di essere un “successo” (valore 1). In altre parole, il modello generatore dei dati avrà la seguente funzione di massa di probabilità</p>
<p><span class="math display">\[
p(y \mid \theta)
\ = \
\mbox{Bin}(y \mid n, \theta).
\]</span></p>
<p>Nei capitoli precedenti è stato mostrato come, sulla base del modello binomiale, sia possibile assegnare una probabilità a ciascun possibile valore <span class="math inline">\(y \in \{0, 1, \dots, n\}\)</span> <em>assumendo noto il valore del parametro</em> <span class="math inline">\(\theta\)</span>. Ma ora abbiamo il problema inverso, ovvero quello di fare inferenza su <span class="math inline">\(\theta\)</span> alla luce dei dati campionari <span class="math inline">\(y\)</span>. In altre parole, riteniamo di conoscere il modello probabilistico che ha generato i dati, ma di tale modello non conosciamo i parametri: vogliamo dunque ottenere informazioni su <span class="math inline">\(\theta\)</span> avendo osservato i dati <span class="math inline">\(y\)</span>. Per fare questo, in un ottica bayesiana, è innanzitutto necessario definire la funzione di verosimiglianza.</p>
<p>Per i dati di <span class="citation" data-cites="zetschefuture2019">Zetsche et al. (<a href="999_refs.html#ref-zetschefuture2019" role="doc-biblioref">2019</a>)</span>, la funzione di verosimiglianza corrisponde alla funzione binomiale di parametro <span class="math inline">\(\theta \in [0, 1]\)</span> sconosciuto. Abbiamo osservato <span class="math inline">\(y\)</span> = 23 successi, in <span class="math inline">\(n\)</span> = 30 prove.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">23</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La funzione di verosimiglianza dunque diventa</p>
<p><span id="eq-likebino23"><span class="math display">\[
\mathcal{L}(\theta \mid y) = \frac{(23 + 7)!}{23!7!} \theta^{23} + (1-\theta)^7.
\tag{15.3}\]</span></span></p>
<p>Per costruire la funzione di verosimiglianza dobbiamo applicare l’<a href="#eq-likebino23">Equazione&nbsp;<span>15.3</span></a> tante volte, cambiando ogni volta il valore <span class="math inline">\(\theta\)</span>, ma tenendo sempre costante il valore dei dati. Nella seguente simulazione considereremo 100 possibili valori <span class="math inline">\(\theta \in [0, 1]\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> binom</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.linspace(<span class="fl">0.0</span>, <span class="fl">1.0</span>, num<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(theta)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [0.         0.01010101 0.02020202 0.03030303 0.04040404 0.05050505</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  0.06060606 0.07070707 0.08080808 0.09090909 0.1010101  0.11111111</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  0.18181818 0.19191919 0.2020202  0.21212121 0.22222222 0.23232323</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  0.3030303  0.31313131 0.32323232 0.33333333 0.34343434 0.35353535</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  0.36363636 0.37373737 0.38383838 0.39393939 0.4040404  0.41414141</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.5959596</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  0.66666667 0.67676768 0.68686869 0.6969697  0.70707071 0.71717172</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  0.78787879 0.7979798  0.80808081 0.81818182 0.82828283 0.83838384</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.8989899</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  0.96969697 0.97979798 0.98989899 1.        ]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Per esempio, ponendo <span class="math inline">\(\theta = 0.1\)</span> otteniamo il seguente valore dell’ordinata della funzione di verosimiglianza:</p>
<p><span class="math display">\[
\mathcal{L}(\theta \mid y) = \frac{(23 + 7)!}{23!7!} 0.1^{23} + (1-0.1)^7.
\]</span></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>binom.pmf(<span class="dv">23</span>, <span class="dv">30</span>, <span class="fl">0.1</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 9.7371682902e-18</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Ponendo <span class="math inline">\(\theta = 0.2\)</span> otteniamo il seguente valore dell’ordinata della funzione di verosimiglianza:</p>
<p><span class="math display">\[
\mathcal{L}(\theta \mid y) = \frac{(23 + 7)!}{23!7!} 0.2^{23} + (1-0.2)^7.
\]</span></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>binom.pmf(<span class="dv">23</span>, <span class="dv">30</span>, <span class="fl">0.2</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 3.58141723492221e-11</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Se ripetiamo questo processo 100 volte, una volta per ciascuno dei valori <span class="math inline">\(\theta\)</span> considerati, otteniamo 100 coppie di punti <span class="math inline">\(\theta\)</span> e <span class="math inline">\(f(\theta)\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> like(n, r, theta):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> math.comb(n, r) <span class="op">*</span> theta<span class="op">**</span>r <span class="op">*</span> (<span class="dv">1</span><span class="op">-</span>theta)<span class="op">**</span>(n<span class="op">-</span>r)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La curva che interpola tali punti è la funzione di verosimiglianza. La <a href="#fig-likefutexpect">Figura&nbsp;<span>15.1</span></a> fornisce una rappresentazione grafica di tale funzione.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> <span class="dv">23</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>plt.plot(theta, like(n<span class="op">=</span>n, r<span class="op">=</span>r, theta<span class="op">=</span>theta), <span class="st">'r-'</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Funzione di verosimiglianza'</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Valore della variabile casuale theta [0, 1]'</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Verosimiglianza'</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-likefutexpect" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="024_likelihood_files/figure-html/fig-likefutexpect-1.png" class="img-fluid figure-img" width="576"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;15.1: Funzione di verosimiglianza nel caso di 23 successi in 30 prove.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<section id="interpretazione" class="level3" data-number="15.1.1"><h3 data-number="15.1.1" class="anchored" data-anchor-id="interpretazione">
<span class="header-section-number">15.1.1</span> Interpretazione</h3>
<p>Come possiamo interpretare la curva che abbiamo ottenuto? Per alcuni valori <span class="math inline">\(\theta\)</span> la funzione di verosimiglianza assume valori piccoli; per altri valori <span class="math inline">\(\theta\)</span> la funzione di verosimiglianza assume valori più grandi. Questi ultimi sono i valori <span class="math inline">\(\theta\)</span> più credibili e il valore 23/30 = 0.767 (la moda della funzione di verosimiglianza) è il valore più credibile di tutti.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>input_list <span class="op">=</span> like(n<span class="op">=</span>n, r<span class="op">=</span>r, theta<span class="op">=</span>theta)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="bu">max</span> <span class="op">=</span> input_list[<span class="dv">0</span>]</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>index <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="bu">len</span>(input_list)):</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> input_list[i] <span class="op">&gt;</span> <span class="bu">max</span>:</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">max</span> <span class="op">=</span> input_list[i]</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        index <span class="op">=</span> i</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Index of the maximum value is : </span><span class="sc">{</span>index<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Index of the maximum value is : 76</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>theta[<span class="dv">76</span>]</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 0.7676767676767677</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Si noti che, anziché usare la funzione <code>like()</code> che (per chiarezza) abbiamo definito sopra, in una maniera del tutto equivalente è possibile usare la funzione <code>binom.pmf()</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> <span class="dv">23</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>plt.plot(theta,  binom.pmf(r, n, theta), <span class="st">'r-'</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Funzione di verosimiglianza'</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Valore della variabile casuale theta [0, 1]'</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Verosimiglianza'</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="024_likelihood_files/figure-html/unnamed-chunk-9-3.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</section><section id="la-log-verosimiglianza" class="level3" data-number="15.1.2"><h3 data-number="15.1.2" class="anchored" data-anchor-id="la-log-verosimiglianza">
<span class="header-section-number">15.1.2</span> La log-verosimiglianza</h3>
<p>Dal punto di vista pratico risulta più conveniente utilizzare, al posto della funzione di verosimiglianza, il suo logaritmo naturale, ovvero la funzione di log-verosimiglianza:</p>
<p><span id="eq-loglike-definition"><span class="math display">\[
\ell(\theta) = \log \mathcal{L}(\theta).
\tag{15.4}\]</span></span></p>
<p>Poiché il logaritmo è una funzione strettamente crescente (usualmente si considera il logaritmo naturale), allora <span class="math inline">\(\mathcal{L}(\theta)\)</span> e <span class="math inline">\(\ell(\theta)\)</span> assumono il massimo (o i punti di massimo) in corrispondenza degli stessi valori di <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
\hat{\theta} = \mbox{argmax}_{\theta \in \Theta} \ell(\theta) = \mbox{argmax}_{\theta \in \Theta} \mathcal{L}(\theta).
\]</span></p>
<p>Per le proprietà del logaritmo, la funzione nucleo di log-verosimiglianza della binomiale è</p>
<p><span class="math display">\[
\begin{aligned}
\ell(\theta \mid y) &amp;= \log \mathcal{L}(\theta \mid y) \notag\\
          &amp;= \log \left(\theta^y (1-\theta)^{n - y} \right) \notag\\
          &amp;= \log \theta^y + \log \left( (1-\theta)^{n - y} \right) \notag\\
          &amp;= y \log \theta + (n - y) \log (1-\theta).\notag
\end{aligned}
\]</span></p>
<p>Si noti che non è necessario lavorare con i logaritmi, ma è fortemente consigliato. Il motivo è che i valori della verosimiglianza, in cui si moltiplicano valori di probabilità molto piccoli, possono diventare estremamente piccoli – qualcosa come <span class="math inline">\(10^{-34}\)</span>. In tali circostanze, non è sorprendente che i programmi dei computer mostrino problemi di arrotondamento numerico. Le trasformazioni logaritmiche risolvono questo problema.</p>
<p>Svolgiamo nuovamente il problema precedente usando la log-verosimiglianza per trovare il massimo della funzione di log-verosimiglianza. Ora utilizziamo la funzione <code>binom.logpmf()</code>.</p>
<p>La funzione di log-verosimiglianza è rappresentata nella <a href="#fig-loglike-futureexp">Figura&nbsp;<span>15.2</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> <span class="dv">23</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>plt.plot(theta, binom.logpmf(r, n, theta), <span class="st">'r-'</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Funzione di log-verosimiglianza'</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Valore della variabile casuale theta [0, 1]'</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Log-verosimiglianza'</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-loglike-futureexp" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="024_likelihood_files/figure-html/fig-loglike-futureexp-5.png" class="img-fluid figure-img" width="576"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;15.2: Funzione di log-verosimiglianza nel caso di 23 successi in 30 prove.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Il risultato replica quello trovato in precedenza con la funzione di verosimiglianza.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>input_list <span class="op">=</span> binom.logpmf(r, n, theta)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="bu">max</span> <span class="op">=</span> input_list[<span class="dv">0</span>]</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>index <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="bu">len</span>(input_list)):</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> input_list[i] <span class="op">&gt;</span> <span class="bu">max</span>:</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">max</span> <span class="op">=</span> input_list[i]</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>        index <span class="op">=</span> i</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Index of the maximum value is : </span><span class="sc">{</span>index<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Index of the maximum value is : 76</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>theta[index]</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 0.7676767676767677</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section></section><section id="modello-gaussiano" class="level2" data-number="15.2"><h2 data-number="15.2" class="anchored" data-anchor-id="modello-gaussiano">
<span class="header-section-number">15.2</span> Modello gaussiano</h2>
<p>Ora che abbiamo capito come costruire la funzione verosimiglianza di una binomiale è relativamente semplice fare un passo ulteriore e considerare la verosimiglianza del caso di una funzione di densità, ovvero nel caso di una variabile casuale continua. Consideriamo qui il caso della Normale. La densità di una distribuzione Normale di parametri <span class="math inline">\(\mu\)</span> e <span class="math inline">\(\sigma\)</span> è</p>
<p><span id="eq-gaussian-sim-like"><span class="math display">\[
f(y \mid \mu, \sigma) = \frac{1}{\sigma \sqrt{2\pi}} \exp\left\{-\frac{1}{2\sigma^2}(y-\mu)^2\right\}.
\tag{15.5}\]</span></span></p>
<p>Costruiamo dunque la funzione di verosimiglianza nel caso dell’<a href="#eq-gaussian-sim-like">Equazione&nbsp;<span>15.5</span></a>.</p>
<section id="una-singola-osservazione" class="level3" data-number="15.2.1"><h3 data-number="15.2.1" class="anchored" data-anchor-id="una-singola-osservazione">
<span class="header-section-number">15.2.1</span> Una singola osservazione</h3>
<p>Esaminiamo prima il caso in cui i dati corrispondono ad una singola osservazione <span class="math inline">\(y\)</span>. Poniamo</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">114</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>L’<a href="#eq-gaussian-sim-like">Equazione&nbsp;<span>15.5</span></a> dipende dai parametri <span class="math inline">\(\mu\)</span> e <span class="math inline">\(\sigma\)</span> e dai dati <span class="math inline">\(y\)</span>. Per semplicità, ipotizziamo <span class="math inline">\(\sigma\)</span> noto e uguale a 15. Nell’esercizio considereremo 1000 valori <span class="math inline">\(\mu\)</span> compresi tra 70 e 160.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> np.linspace(<span class="fl">70.0</span>, <span class="fl">160.0</span>, num<span class="op">=</span><span class="dv">1000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Dato che consideriamo 1000 possibili valori <span class="math inline">\(\mu\)</span>, per costruire la funzione di verosimiglianza applicheremo 1000 volte l’<a href="#eq-gaussian-sim-like">Equazione&nbsp;<span>15.5</span></a>. In ciascun passo dell’esercizio inseriremo nell’<a href="#eq-gaussian-sim-like">Equazione&nbsp;<span>15.5</span></a></p>
<ul>
<li>il singolo valore <span class="math inline">\(y\)</span> considerato (che viene mantenuto costante),</li>
<li>il valore <span class="math inline">\(\sigma\)</span> assunto noto (anch’esso costante),</li>
<li>uno alla volta ciascuno dei valori <span class="math inline">\(\mu\)</span> che abbiamo definito sopra (quindi, nelle 1000 applicazioni dell’<a href="#eq-gaussian-sim-like">Equazione&nbsp;<span>15.5</span></a>, il valore <span class="math inline">\(\mu\)</span> è l’unico che varia: <span class="math inline">\(y\)</span> e <span class="math inline">\(\sigma\)</span> sono mantenuti costanti).</li>
</ul>
<p>La distribuzione Gaussiana è implementata in Python mediante <code>norm.pdf()</code>. La funzione <code>norm.pdf()</code> richiede tre argomenti: il valore <span class="math inline">\(y\)</span> (o il vettore <span class="math inline">\(y\)</span>), la media, ovvero il parametro <span class="math inline">\(\mu\)</span>, e la deviazione standard, ovvero il parametro <span class="math inline">\(\sigma\)</span>.</p>
<p>Applicando la funzione <code>norm.pdf()</code> 1000 volte, una volta per ciascuno dei valori <span class="math inline">\(\mu\)</span> che abbiamo definito (e tenendo fissi <span class="math inline">\(y = 114\)</span> e <span class="math inline">\(\sigma = 15\)</span>), otteniamo 1000 valori <span class="math inline">\(f(\mu)\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>f_mu <span class="op">=</span> norm.pdf(y, loc<span class="op">=</span>mu, scale<span class="op">=</span><span class="dv">15</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La funzione di verosimiglianza è la curva che interpola i punti <span class="math inline">\(\big(\mu, f(\mu)\big)\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>plt.plot(mu, f_mu, <span class="st">'r-'</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Funzione di verosimiglianza'</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Valore della variabile casuale mu [70, 160]'</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Verosimiglianza'</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>plt.xlim([<span class="dv">70</span>, <span class="dv">160</span>])</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (70.0, 160.0)</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="024_likelihood_files/figure-html/unnamed-chunk-16-7.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Si noti che la funzione di verosimiglianza così trovata ha la forma della distribuzione Gaussiana. Nel caso di una singola osservazione, <em>ma solo in questo caso</em>, ha anche un’area unitaria.</p>
<p>Per l’esempio presente, la moda della funzione di verosimiglianza è 114.</p>
</section><section id="un-campione-di-osservazioni" class="level3" data-number="15.2.2"><h3 data-number="15.2.2" class="anchored" data-anchor-id="un-campione-di-osservazioni">
<span class="header-section-number">15.2.2</span> Un campione di osservazioni</h3>
<p>Consideriamo ora il caso più generale, ovvero quello di un campione di <span class="math inline">\(n\)</span> osservazioni. Possiamo immaginare un campione casuale <span class="math inline">\(y_1, y_2, \dots, y_n\)</span> estratto da una popolazione <span class="math inline">\(\mathcal{N}(\mu, \sigma)\)</span> come una sequenza di realizzazioni indipendenti ed identicamente distribuite (di seguito, i.i.d.) della medesima variabile casuale <span class="math inline">\(Y \sim \mathcal{N}(\mu, \sigma)\)</span>. I parametri sconosciuti sono <span class="math inline">\(\theta = \{\mu, \sigma\}\)</span>.</p>
<p>Se le variabili casuali <span class="math inline">\(y_1, y_2, \dots, y_n\)</span> sono i.i.d., la loro densità congiunta è data da: <span class="math display">\[\begin{align}
f(y \mid \theta) &amp;= f(y_1 \mid \theta) \cdot f(y_2 \mid \theta) \cdot \; \dots \; \cdot f(y_n \mid \theta)\notag\\
                 &amp;= \prod_{i=1}^n f(y_i \mid \theta),
\end{align}\]</span></p>
<p>laddove <span class="math inline">\(f(\cdot)\)</span> è la densità Gaussiana di parametri <span class="math inline">\(\mu, \sigma\)</span>. Tenendo costanti i dati <span class="math inline">\(y\)</span>, la funzione di verosimiglianza diventa:</p>
<span class="math display">\[\begin{equation}
\mathcal{L}(\theta \mid y) = \prod_{i=1}^n f(y_i \mid \theta).
\end{equation}\]</span>
<p>Per chiarire la formula precedente, consideriamo un esempio che utilizza come dati i valori BDI-II dei trenta soggetti del campione clinico di Zetsche et al.&nbsp;(2020).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> [<span class="dv">26</span>, <span class="dv">35</span>, <span class="dv">30</span>, <span class="dv">25</span>, <span class="dv">44</span>, <span class="dv">30</span>, <span class="dv">33</span>, <span class="dv">43</span>, <span class="dv">22</span>, <span class="dv">43</span>, <span class="dv">24</span>, <span class="dv">19</span>, <span class="dv">39</span>, <span class="dv">31</span>, <span class="dv">25</span>, <span class="dv">28</span>, <span class="dv">35</span>, <span class="dv">30</span>, <span class="dv">26</span>, </span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="dv">31</span>, <span class="dv">41</span>, <span class="dv">36</span>, <span class="dv">26</span>, <span class="dv">35</span>, <span class="dv">33</span>, <span class="dv">28</span>, <span class="dv">27</span>, <span class="dv">34</span>, <span class="dv">27</span>, <span class="dv">22</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Ci poniamo l’obiettivo di creare la funzione di verosimiglianza per questi dati, supponendo di sapere (in base ai risultati di ricerche precedenti) che i punteggi BDI-II si distribuiscono secondo la legge Normale e supponendo <span class="math inline">\(\sigma\)</span> noto e uguale alla deviazione standard del campione.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>true_sigma <span class="op">=</span> np.std(y)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>true_sigma </span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 6.495810615739622</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Abbiamo visto in precedenza che, per una singola osservazione, la funzione di verosimiglianza è la densità Gaussiana espressa in funzione dei parametri (in questo caso, solo <span class="math inline">\(\mu\)</span>). Per un campione di osservazioni i.i.d., ovvero <span class="math inline">\(y = (y_1, y_2, \dots, y_n)\)</span>, la verosimiglianza è la funzione di densità congiunta <span class="math inline">\(f(y \mid \mu, \sigma)\)</span> espressa in funzione dei parametri. Dato che le osservazioni sono i.i.d., la densità congiunta è data dal prodotto delle densità delle singole osservazioni.</p>
<p>Poniamoci il problema di trovare l’ordinata della funzione di log-verosimiglianza per le 30 osservazioni del campione in corrispondenza di <span class="math inline">\(\mu = \mu_0\)</span></p>
<p>Per la prima osservazione del campione (<span class="math inline">\(y_1 = 26\)</span>) abbiamo</p>
<p><span class="math display">\[
f(26 \mid \mu_0, \sigma=6.50) = \frac{1}{{6.50 \sqrt {2\pi}}}\exp\left\{{-\frac{(26 - \mu_0)^2}{2\cdot 6.50^2}}\right\}.
\]</span></p>
<p>Se consideriamo tutte le osservazioni, la densità congiunta è data dal prodotto delle densità delle singole osservazioni: <span class="math inline">\(f(y \mid \mu, \sigma = 6.50) = \, \prod_{i=1}^n f(y_i \mid \mu, \sigma = 6.50)\)</span>. Utilizzando i dati del campione, e assumendo <span class="math inline">\(\sigma = 6.50\)</span>, l’ordinata della funzione di verosimiglianza in corrispondenza di <span class="math inline">\(\mu_0\)</span> è uguale a</p>
<p><span class="math display">\[
\begin{aligned}
\mathcal{L}(\mu_0, \sigma=6.50 \mid y) =&amp; \, \prod_{i=1}^{30} f(y_i \mid \mu_0, \sigma = 6.50) = \notag\\
&amp; \frac{1}{{6.50 \sqrt {2\pi}}}\exp\left\{{-\frac{(26 - \mu_0)^2}{2\cdot 6.50^2}}\right\} \times \notag\\
&amp; \frac{1}{{6.61 \sqrt {2\pi}}}\exp\left\{{-\frac{(35 - \mu_0)^2}{2\cdot 6.50^2}}\right\} \times  \notag\\
&amp; \vdots \notag\\
&amp; \frac{1}{{6.61 \sqrt {2\pi}}}\exp\left\{{-\frac{(22 - \mu_0)^2}{2\cdot 6.50^2}}\right\}.
\end{aligned}
\]</span></p>
<p>È più conveniente svolgere i calcoli usando il logaritmo della verosimiglianza. In Python definiamo la funzione di log-verosimiglianza, <code>log_likelihood()</code>, che prende come argomenti <code>y</code>, <code>mu</code> e <code>sigma = 6.50</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_likelihood(y, mu, sigma <span class="op">=</span> true_sigma):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">sum</span>(norm.logpdf(y, loc<span class="op">=</span>mu, scale<span class="op">=</span><span class="dv">15</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Consideriamo il valore <span class="math inline">\(\mu_0 = \bar{y}\)</span>, ovvero</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>bar_y <span class="op">=</span> np.mean(y)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(bar_y)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 30.933333333333334</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>L’ordinata della funzione di log-verosimiglianza in corrispondenza di <span class="math inline">\(\mu = 30.93\)</span> è</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(log_likelihood(y, <span class="fl">30.93</span>, sigma <span class="op">=</span> true_sigma))</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -111.62269980698424</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Troviamo ora i valori della log-verosimiglianza per ciascuno dei 1000 valori <span class="math inline">\(\mu\)</span> nell’intervallo <span class="math inline">\([\bar{y} - 2 \sigma, \bar{y} + 2 \sigma]\)</span>. Iniziamo a definire <code>mu</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> np.linspace(np.mean(y) <span class="op">-</span> <span class="dv">2</span><span class="op">*</span>np.std(y), np.mean(y) <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>np.std(y), num<span class="op">=</span><span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Troviamo il valore dell’ordinata della funzione di log-verosimiglianza in corrispondenza di ciascuno dei 1000 valori <code>mu</code> che abbiamo definito.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>ll <span class="op">=</span> [log_likelihood(y, mu_val, true_sigma) <span class="cf">for</span> mu_val <span class="kw">in</span> mu]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Nel caso di un solo parametro sconosciuto (nel caso presente, <span class="math inline">\(\mu\)</span>) è possibile rappresentare la log-verosimiglianza con una curva che interpola i punti (<code>mu</code>, <code>ll</code>). Tale funzione descrive la <em>credibilità relativa</em> che può essere attribuita ai valori del parametro <span class="math inline">\(\mu\)</span> alla luce dei dati osservati.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>plt.plot(mu, ll, <span class="st">'r-'</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Funzione di log-verosimiglianza'</span>)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Valore della variabile casuale mu'</span>)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Log-verosimiglianza'</span>)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span>np.mean(y), color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, ls<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-loglike-bdi2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="024_likelihood_files/figure-html/fig-loglike-bdi2-9.png" class="img-fluid figure-img" width="576"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;15.3: Log-verosimiglianza del parametro <span class="math inline">\(\mu\)</span> per i dati di <span class="citation" data-cites="zetschefuture2019">Zetsche et al. (<a href="999_refs.html#ref-zetschefuture2019" role="doc-biblioref">2019</a>)</span>.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section><section id="massima-verosimiglianza" class="level3" data-number="15.2.3"><h3 data-number="15.2.3" class="anchored" data-anchor-id="massima-verosimiglianza">
<span class="header-section-number">15.2.3</span> Massima verosimiglianza</h3>
<p>Il valore <span class="math inline">\(\mu\)</span> più credibile corrisponde al massimo della funzione di log-verosimiglinza e viene detto <em>stima di massima verosimiglianza</em>.</p>
<p>Il massimo della funzione di log-verosimiglianza, ovvero 30.93 nel caso dell’esempio presente, è identico alla media dei dati campionari. Tale risultato, ottenuto per via numerica, può essere dimostrato formalmente (ma non lo faremo qui). Usando la notazione matematica possiamo dire che cerchiamo l’argmax dell’equazione precedente rispetto a <span class="math inline">\(\theta\)</span>, ovvero</p>
<p><span class="math display">\[
\hat{\theta} = \text{argmax}_{\theta} \prod_{i=1}^n f(y_i \mid \theta).
\]</span></p>
<p>Questo problema si risolve calcolando le derivate della funzione rispetto a <span class="math inline">\(\theta\)</span>, ponendo le derivate uguali a zero e risolvendo. Saltando tutti i passaggi algebrici di questo procedimento, per <span class="math inline">\(\mu\)</span> si trova</p>
<span class="math display">\[\begin{equation}
\hat{\mu} = \frac{1}{n} \sum_{i=1}^n y_i
\end{equation}\]</span>
<p>e</p>
<span class="math display">\[\begin{equation}
\hat{\sigma} = \sqrt{\sum_{i=1}^n\frac{1}{n}(y_i- \mu)^2}.
\end{equation}\]</span>
<p>In altri termini, la s.m.v. del parametro <span class="math inline">\(\mu\)</span> è la media del campione e la s.m.v. del parametro <span class="math inline">\(\sigma\)</span> è la deviazione standard del campione.</p>
<div id="exr-loglike-bdi2-1" class="theorem exercise">
<p><span class="theorem-title"><strong>Esercizio 15.1 </strong></span>Dalla <a href="#fig-loglike-bdi2">Figura&nbsp;<span>15.3</span></a> notiamo che il massimo della funzione di log-verosimiglianza calcolata per via numerica, ovvero 30.93, è identico alla media dei dati campionari e corrisponde al risultato teorico atteso.</p>
</div>
</section></section><section id="commenti-e-considerazioni-finali" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="commenti-e-considerazioni-finali">Commenti e considerazioni finali</h2>
<p>Nella funzione di verosimiglianza i dati (osservati) vengono trattati come fissi, mentre i valori del parametro (o dei parametri) <span class="math inline">\(\theta\)</span> vengono variati: la verosimiglianza è una funzione di <span class="math inline">\(\theta\)</span> per il dato fisso <span class="math inline">\(y\)</span>. Pertanto, la funzione di verosimiglianza riassume i seguenti elementi: un modello statistico che genera stocasticamente i dati (in questo capitolo abbiamo esaminato due modelli statistici: quello binomiale e quello Normale), un intervallo di valori possibili per <span class="math inline">\(\theta\)</span> e i dati osservati <span class="math inline">\(y\)</span>.</p>
<p>Nella statistica frequentista l’inferenza si basa solo sui dati a disposizione e qualunque informazione fornita dalle conoscenze precedenti non viene presa in considerazione. Nello specifico, nella statistica frequentista l’inferenza viene condotta massimizzando la funzione di (log) verosimiglianza, condizionatamente ai valori assunti dalle variabili casuali campionarie. Le basi dell’inferenza frequentista, dunque, sono state riassunte in questo Capitolo. Nella statistica bayesiana, invece, l’inferenza statistica viene condotta combinando la funzione di verosimiglianza con le distribuzioni a priori dei parametri incogniti <span class="math inline">\(\theta\)</span>. Ciò verrà discusso nei Capitoli successivi.</p>
<p>La differenza fondamentale tra inferenza bayesiana e frequentista è dunque che i frequentisti non ritengono utile descrivere i parametri in termini probabilistici: i parametri dei modelli statistici vengono concepiti come fissi ma sconosciuti. Nell’inferenza bayesiana, invece, i parametri sconosciuti sono intesi come delle variabili casuali e ciò consente di quantificare in termini probabilistici il nostro grado di intertezza relativamente al loro valore.</p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-line-spacing="2" role="doc-bibliography" style="display: none">
<div id="ref-zetschefuture2019" class="csl-entry" role="doc-biblioentry">
Zetsche, U., Bürkner, P.-C., &amp; Renneberg, B. (2019). Future expectations in clinical depression: <span>Biased</span> or realistic? <em>Journal of Abnormal Psychology</em>, <em>128</em>(7), 678–688.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./023_cont_rv_distr.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. continue</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./bayes_inference.html" class="pagination-link">
        <span class="nav-page-text">Parte 4: Inferenza bayesiana</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb26" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># La funzione di verosimiglianza {#sec-likelihood}</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, include = FALSE}</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(<span class="st">"_common.R"</span>)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"scales"</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>La verosimiglianza viene utilizzata sia nell'inferenza bayesiana che in quella frequentista. In entrambi i paradigmi di inferenza, il suo ruolo è quello di quantificare la forza con la quale i dati osservati supportano i possibili valori dei parametri sconosciuti di un modello statistico.</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>::: {#def-likelihood}</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>La *funzione di verosimiglianza* $\mathcal{L}(\theta \mid y) = f(y \mid \theta), \theta \in \Theta,$ è la funzione di massa o di densità di probabilità dei dati $y$ vista come una funzione del parametro sconosciuto (o dei parametri sconosciuti) $\theta$.</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>Detto in altre parole, la funzione di verosimiglianza e la funzione di (massa o densità di) probabilità sono formalmente identiche, ma è completamente diversa la loro interpretazione:</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>nel caso della funzione di massa o di densità di probabilità, la distribuzione del vettore casuale delle osservazioni campionarie $y$ dipende dai valori assunti dal parametro (o dai parametri) $\theta$ -- per esempio, nel caso della distribuzione binomiale, fissata $\theta$ (probabilità di successo) a 0.5, la probabilità di osservare $y = 0, \dots, 10$ successi in $n$ prove è determinata in maniera univoca (se il valore del parametro $\theta$ è noto, quello che resta da stabilire è la probabilità da assegnare a ciascuno degli esiti $y$ possibili);</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>nel caso della la funzione di verosimiglianza la credibilità assegnata a ciascun possibile valore $\theta$ viene determinata avendo acquisita l'informazione campionaria $y$ che rappresenta l'elemento condizionante (in questo secondo caso, $y$ è noto, ma $\theta$ è ignoto; ci chiediamo quale sia la credibilità relativa di ciascuno dei possibili valori $\theta$, avendo osservato un determinato $y$).</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>La funzione di verosimiglianza descrive dunque in termini relativi il sostegno empirico che $\theta \in \Theta$ riceve da $y$. Infatti, la funzione di verosimiglianza assume forme diverse al variare di $y$. Possiamo dunque pensare alla funzione di verosimiglianza come alla risposta alla seguente domanda: avendo osservato i dati $y$, quanto risultano (relativamente) credibili i diversi valori del parametro $\theta$? In termini più formali possiamo dire che, sulla base dei dati, $\theta_1 \in \Theta$ risulta più credibile di $\theta_2 \in \Theta$ quale indice del modello probabilistico generatore dei dati se $\mathcal{L}(\theta_1) &gt; \mathcal{L}(\theta_1)$.</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>Si noti un punto importante: la funzione $\mathcal{L}(\theta \mid y)$ non è una funzione di densità. Infatti, essa non racchiude un'area unitaria.</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a><span class="fu">## Modello binomiale</span></span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>Per chiarire il concetto di verosimiglianza consideriamo innanzitutto il caso più semplice, ovvero quello Binomiale.</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a>Per $n$ prove Bernoulliane indipendenti, le quali producono $y$ successi e ($n-y$) insuccessi, la funzione nucleo di verosimiglianza (ovvero, la funzione di verosimiglianza da cui sono state escluse tutte le costanti moltiplicative che non hanno alcun effetto su $\hat{\theta}$) è</span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a>\mathcal{L}(p \mid y) = \theta^y (1-\theta)^{n - y}.\notag</span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a>$$ {#eq-like-binomial-kernel}</span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-33"><a href="#cb26-33" aria-hidden="true" tabindex="-1"></a>Per fare un esempio pratico, consideriamo la ricerca di @zetschefuture2019. Questi ricercatori hanno trovato che, su 30 pazienti clinicamente depressi, 23 manifestavano delle aspettative distorsione negativamente relativamente al loro umore futuro. Se i dati di @zetschefuture2019 vengono riassunti mediante una proporzione (ovvero, 23/30), allora è sensato adottare un modello probabilistico binomiale quale meccanismo generatore dei dati:</span>
<span id="cb26-34"><a href="#cb26-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-35"><a href="#cb26-35" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb26-36"><a href="#cb26-36" aria-hidden="true" tabindex="-1"></a>y  \sim \mbox{Bin}(n, \theta),</span>
<span id="cb26-37"><a href="#cb26-37" aria-hidden="true" tabindex="-1"></a>$$ {#eq-binomialmodel}</span>
<span id="cb26-38"><a href="#cb26-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-39"><a href="#cb26-39" aria-hidden="true" tabindex="-1"></a>laddove $\theta$ è la probabiltà che una prova Bernoulliana assuma il valore 1 e $n$ corrisponde al numero di prove Bernoulliane. Questo modello assume che le prove Bernoulliane $y$ che costituiscono il campione siano tra loro indipendenti e che ciascuna abbia la stessa probabilità $\theta \in <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$ di essere un "successo" (valore 1). In altre parole, il modello generatore dei dati avrà la seguente funzione di massa di probabilità</span>
<span id="cb26-40"><a href="#cb26-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-41"><a href="#cb26-41" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb26-42"><a href="#cb26-42" aria-hidden="true" tabindex="-1"></a>p(y \mid \theta)</span>
<span id="cb26-43"><a href="#cb26-43" aria-hidden="true" tabindex="-1"></a>\ = \</span>
<span id="cb26-44"><a href="#cb26-44" aria-hidden="true" tabindex="-1"></a>\mbox{Bin}(y \mid n, \theta).</span>
<span id="cb26-45"><a href="#cb26-45" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb26-46"><a href="#cb26-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-47"><a href="#cb26-47" aria-hidden="true" tabindex="-1"></a>Nei capitoli precedenti è stato mostrato come, sulla base del modello binomiale, sia possibile assegnare una probabilità a ciascun possibile valore $y \in <span class="sc">\{</span>0, 1, \dots, n<span class="sc">\}</span>$ *assumendo noto il valore del parametro* $\theta$. Ma ora abbiamo il problema inverso, ovvero quello di fare inferenza su $\theta$ alla luce dei dati campionari $y$. In altre parole, riteniamo di conoscere il modello probabilistico che ha generato i dati, ma di tale modello non conosciamo i parametri: vogliamo dunque ottenere informazioni su $\theta$ avendo osservato i dati $y$. Per fare questo, in un ottica bayesiana, è innanzitutto necessario definire la funzione di verosimiglianza.</span>
<span id="cb26-48"><a href="#cb26-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-49"><a href="#cb26-49" aria-hidden="true" tabindex="-1"></a>Per i dati di @zetschefuture2019, la funzione di verosimiglianza corrisponde alla funzione binomiale di parametro $\theta \in <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$ sconosciuto. Abbiamo osservato $y$ = 23 successi, in $n$ = 30 prove.</span>
<span id="cb26-50"><a href="#cb26-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-53"><a href="#cb26-53" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb26-54"><a href="#cb26-54" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb26-55"><a href="#cb26-55" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">23</span></span>
<span id="cb26-56"><a href="#cb26-56" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-57"><a href="#cb26-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-58"><a href="#cb26-58" aria-hidden="true" tabindex="-1"></a>La funzione di verosimiglianza dunque diventa</span>
<span id="cb26-59"><a href="#cb26-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-60"><a href="#cb26-60" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb26-61"><a href="#cb26-61" aria-hidden="true" tabindex="-1"></a>\mathcal{L}(\theta \mid y) = \frac{(23 + 7)!}{23!7!} \theta^{23} + (1-\theta)^7.</span>
<span id="cb26-62"><a href="#cb26-62" aria-hidden="true" tabindex="-1"></a>$$ {#eq-likebino23}</span>
<span id="cb26-63"><a href="#cb26-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-64"><a href="#cb26-64" aria-hidden="true" tabindex="-1"></a>Per costruire la funzione di verosimiglianza dobbiamo applicare l'@eq-likebino23 tante volte, cambiando ogni volta il valore $\theta$, ma tenendo sempre costante il valore dei dati. Nella seguente simulazione considereremo 100 possibili valori $\theta \in <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$.</span>
<span id="cb26-65"><a href="#cb26-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-68"><a href="#cb26-68" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb26-69"><a href="#cb26-69" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb26-70"><a href="#cb26-70" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb26-71"><a href="#cb26-71" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> binom</span>
<span id="cb26-72"><a href="#cb26-72" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb26-73"><a href="#cb26-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-74"><a href="#cb26-74" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.linspace(<span class="fl">0.0</span>, <span class="fl">1.0</span>, num<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb26-75"><a href="#cb26-75" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(theta)</span>
<span id="cb26-76"><a href="#cb26-76" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-77"><a href="#cb26-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-78"><a href="#cb26-78" aria-hidden="true" tabindex="-1"></a>Per esempio, ponendo $\theta = 0.1$ otteniamo il seguente valore dell'ordinata della funzione di verosimiglianza:</span>
<span id="cb26-79"><a href="#cb26-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-80"><a href="#cb26-80" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb26-81"><a href="#cb26-81" aria-hidden="true" tabindex="-1"></a>\mathcal{L}(\theta \mid y) = \frac{(23 + 7)!}{23!7!} 0.1^{23} + (1-0.1)^7.</span>
<span id="cb26-82"><a href="#cb26-82" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb26-83"><a href="#cb26-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-86"><a href="#cb26-86" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb26-87"><a href="#cb26-87" aria-hidden="true" tabindex="-1"></a>binom.pmf(<span class="dv">23</span>, <span class="dv">30</span>, <span class="fl">0.1</span>)</span>
<span id="cb26-88"><a href="#cb26-88" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-89"><a href="#cb26-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-90"><a href="#cb26-90" aria-hidden="true" tabindex="-1"></a>Ponendo $\theta = 0.2$ otteniamo il seguente valore dell'ordinata della funzione di verosimiglianza:</span>
<span id="cb26-91"><a href="#cb26-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-92"><a href="#cb26-92" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb26-93"><a href="#cb26-93" aria-hidden="true" tabindex="-1"></a>\mathcal{L}(\theta \mid y) = \frac{(23 + 7)!}{23!7!} 0.2^{23} + (1-0.2)^7.</span>
<span id="cb26-94"><a href="#cb26-94" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb26-95"><a href="#cb26-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-98"><a href="#cb26-98" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb26-99"><a href="#cb26-99" aria-hidden="true" tabindex="-1"></a>binom.pmf(<span class="dv">23</span>, <span class="dv">30</span>, <span class="fl">0.2</span>)</span>
<span id="cb26-100"><a href="#cb26-100" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-101"><a href="#cb26-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-102"><a href="#cb26-102" aria-hidden="true" tabindex="-1"></a>Se ripetiamo questo processo 100 volte, una volta per ciascuno dei valori $\theta$ considerati, otteniamo 100 coppie di punti $\theta$ e $f(\theta)$.</span>
<span id="cb26-103"><a href="#cb26-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-106"><a href="#cb26-106" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb26-107"><a href="#cb26-107" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> like(n, r, theta):</span>
<span id="cb26-108"><a href="#cb26-108" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> math.comb(n, r) <span class="op">*</span> theta<span class="op">**</span>r <span class="op">*</span> (<span class="dv">1</span><span class="op">-</span>theta)<span class="op">**</span>(n<span class="op">-</span>r)</span>
<span id="cb26-109"><a href="#cb26-109" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-110"><a href="#cb26-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-111"><a href="#cb26-111" aria-hidden="true" tabindex="-1"></a>La curva che interpola tali punti è la funzione di verosimiglianza. La @fig-likefutexpect fornisce una rappresentazione grafica di tale funzione.</span>
<span id="cb26-112"><a href="#cb26-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-113"><a href="#cb26-113" aria-hidden="true" tabindex="-1"></a><span class="in">```{python, fig-likefutexpect, fig.cap="Funzione di verosimiglianza nel caso di 23 successi in 30 prove."}</span></span>
<span id="cb26-114"><a href="#cb26-114" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb26-115"><a href="#cb26-115" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> <span class="dv">23</span></span>
<span id="cb26-116"><a href="#cb26-116" aria-hidden="true" tabindex="-1"></a>plt.plot(theta, like(n<span class="op">=</span>n, r<span class="op">=</span>r, theta<span class="op">=</span>theta), <span class="st">'r-'</span>)</span>
<span id="cb26-117"><a href="#cb26-117" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Funzione di verosimiglianza'</span>)</span>
<span id="cb26-118"><a href="#cb26-118" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Valore della variabile casuale theta [0, 1]'</span>)</span>
<span id="cb26-119"><a href="#cb26-119" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Verosimiglianza'</span>)</span>
<span id="cb26-120"><a href="#cb26-120" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb26-121"><a href="#cb26-121" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-122"><a href="#cb26-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-123"><a href="#cb26-123" aria-hidden="true" tabindex="-1"></a><span class="fu">### Interpretazione</span></span>
<span id="cb26-124"><a href="#cb26-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-125"><a href="#cb26-125" aria-hidden="true" tabindex="-1"></a>Come possiamo interpretare la curva che abbiamo ottenuto? Per alcuni valori $\theta$ la funzione di verosimiglianza assume valori piccoli; per altri valori $\theta$ la funzione di verosimiglianza assume valori più grandi. Questi ultimi sono i valori $\theta$ più credibili e il valore 23/30 = 0.767 (la moda della funzione di verosimiglianza) è il valore più credibile di tutti.</span>
<span id="cb26-126"><a href="#cb26-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-129"><a href="#cb26-129" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb26-130"><a href="#cb26-130" aria-hidden="true" tabindex="-1"></a>input_list <span class="op">=</span> like(n<span class="op">=</span>n, r<span class="op">=</span>r, theta<span class="op">=</span>theta)</span>
<span id="cb26-131"><a href="#cb26-131" aria-hidden="true" tabindex="-1"></a><span class="bu">max</span> <span class="op">=</span> input_list[<span class="dv">0</span>]</span>
<span id="cb26-132"><a href="#cb26-132" aria-hidden="true" tabindex="-1"></a>index <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb26-133"><a href="#cb26-133" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="bu">len</span>(input_list)):</span>
<span id="cb26-134"><a href="#cb26-134" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> input_list[i] <span class="op">&gt;</span> <span class="bu">max</span>:</span>
<span id="cb26-135"><a href="#cb26-135" aria-hidden="true" tabindex="-1"></a>        <span class="bu">max</span> <span class="op">=</span> input_list[i]</span>
<span id="cb26-136"><a href="#cb26-136" aria-hidden="true" tabindex="-1"></a>        index <span class="op">=</span> i</span>
<span id="cb26-137"><a href="#cb26-137" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Index of the maximum value is : </span><span class="sc">{</span>index<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb26-138"><a href="#cb26-138" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-139"><a href="#cb26-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-142"><a href="#cb26-142" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb26-143"><a href="#cb26-143" aria-hidden="true" tabindex="-1"></a>theta[<span class="dv">76</span>]</span>
<span id="cb26-144"><a href="#cb26-144" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-145"><a href="#cb26-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-146"><a href="#cb26-146" aria-hidden="true" tabindex="-1"></a>Si noti che, anziché usare la funzione <span class="in">`like()`</span> che (per chiarezza) abbiamo definito sopra, in una maniera del tutto equivalente è possibile usare la funzione <span class="in">`binom.pmf()`</span>.</span>
<span id="cb26-147"><a href="#cb26-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-150"><a href="#cb26-150" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb26-151"><a href="#cb26-151" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb26-152"><a href="#cb26-152" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> <span class="dv">23</span></span>
<span id="cb26-153"><a href="#cb26-153" aria-hidden="true" tabindex="-1"></a>plt.plot(theta,  binom.pmf(r, n, theta), <span class="st">'r-'</span>)</span>
<span id="cb26-154"><a href="#cb26-154" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Funzione di verosimiglianza'</span>)</span>
<span id="cb26-155"><a href="#cb26-155" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Valore della variabile casuale theta [0, 1]'</span>)</span>
<span id="cb26-156"><a href="#cb26-156" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Verosimiglianza'</span>)</span>
<span id="cb26-157"><a href="#cb26-157" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb26-158"><a href="#cb26-158" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-159"><a href="#cb26-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-160"><a href="#cb26-160" aria-hidden="true" tabindex="-1"></a><span class="fu">### La log-verosimiglianza</span></span>
<span id="cb26-161"><a href="#cb26-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-162"><a href="#cb26-162" aria-hidden="true" tabindex="-1"></a>Dal punto di vista pratico risulta più conveniente utilizzare, al posto della funzione di verosimiglianza, il suo logaritmo naturale, ovvero la funzione di log-verosimiglianza:</span>
<span id="cb26-163"><a href="#cb26-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-164"><a href="#cb26-164" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb26-165"><a href="#cb26-165" aria-hidden="true" tabindex="-1"></a>\ell(\theta) = \log \mathcal{L}(\theta).</span>
<span id="cb26-166"><a href="#cb26-166" aria-hidden="true" tabindex="-1"></a>$$ {#eq-loglike-definition}</span>
<span id="cb26-167"><a href="#cb26-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-168"><a href="#cb26-168" aria-hidden="true" tabindex="-1"></a>Poiché il logaritmo è una funzione strettamente crescente (usualmente si considera il logaritmo naturale), allora $\mathcal{L}(\theta)$ e $\ell(\theta)$ assumono il massimo (o i punti di massimo) in corrispondenza degli stessi valori di $\theta$:</span>
<span id="cb26-169"><a href="#cb26-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-170"><a href="#cb26-170" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb26-171"><a href="#cb26-171" aria-hidden="true" tabindex="-1"></a>\hat{\theta} = \mbox{argmax}_{\theta \in \Theta} \ell(\theta) = \mbox{argmax}_{\theta \in \Theta} \mathcal{L}(\theta).</span>
<span id="cb26-172"><a href="#cb26-172" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb26-173"><a href="#cb26-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-174"><a href="#cb26-174" aria-hidden="true" tabindex="-1"></a>Per le proprietà del logaritmo, la funzione nucleo di log-verosimiglianza della binomiale è</span>
<span id="cb26-175"><a href="#cb26-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-176"><a href="#cb26-176" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb26-177"><a href="#cb26-177" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb26-178"><a href="#cb26-178" aria-hidden="true" tabindex="-1"></a>\ell(\theta \mid y) &amp;= \log \mathcal{L}(\theta \mid y) \notag<span class="sc">\\</span></span>
<span id="cb26-179"><a href="#cb26-179" aria-hidden="true" tabindex="-1"></a>          &amp;= \log \left(\theta^y (1-\theta)^{n - y} \right) \notag<span class="sc">\\</span></span>
<span id="cb26-180"><a href="#cb26-180" aria-hidden="true" tabindex="-1"></a>          &amp;= \log \theta^y + \log \left( (1-\theta)^{n - y} \right) \notag<span class="sc">\\</span></span>
<span id="cb26-181"><a href="#cb26-181" aria-hidden="true" tabindex="-1"></a>          &amp;= y \log \theta + (n - y) \log (1-\theta).\notag</span>
<span id="cb26-182"><a href="#cb26-182" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb26-183"><a href="#cb26-183" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb26-184"><a href="#cb26-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-185"><a href="#cb26-185" aria-hidden="true" tabindex="-1"></a>Si noti che non è necessario lavorare con i logaritmi, ma è fortemente consigliato. Il motivo è che i valori della verosimiglianza, in cui si moltiplicano valori di probabilità molto piccoli, possono diventare estremamente piccoli -- qualcosa come $10^{-34}$. In tali circostanze, non è sorprendente che i programmi dei computer mostrino problemi di arrotondamento numerico. Le trasformazioni logaritmiche risolvono questo problema.</span>
<span id="cb26-186"><a href="#cb26-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-187"><a href="#cb26-187" aria-hidden="true" tabindex="-1"></a>Svolgiamo nuovamente il problema precedente usando la log-verosimiglianza per trovare il massimo della funzione di log-verosimiglianza. Ora utilizziamo la funzione <span class="in">`binom.logpmf()`</span>.</span>
<span id="cb26-188"><a href="#cb26-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-189"><a href="#cb26-189" aria-hidden="true" tabindex="-1"></a>La funzione di log-verosimiglianza è rappresentata nella @fig-loglike-futureexp.</span>
<span id="cb26-190"><a href="#cb26-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-191"><a href="#cb26-191" aria-hidden="true" tabindex="-1"></a><span class="in">```{python, fig-loglike-futureexp, fig.cap="Funzione di log-verosimiglianza nel caso di 23 successi in 30 prove."}</span></span>
<span id="cb26-192"><a href="#cb26-192" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb26-193"><a href="#cb26-193" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> <span class="dv">23</span></span>
<span id="cb26-194"><a href="#cb26-194" aria-hidden="true" tabindex="-1"></a>plt.plot(theta, binom.logpmf(r, n, theta), <span class="st">'r-'</span>)</span>
<span id="cb26-195"><a href="#cb26-195" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Funzione di log-verosimiglianza'</span>)</span>
<span id="cb26-196"><a href="#cb26-196" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Valore della variabile casuale theta [0, 1]'</span>)</span>
<span id="cb26-197"><a href="#cb26-197" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Log-verosimiglianza'</span>)</span>
<span id="cb26-198"><a href="#cb26-198" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb26-199"><a href="#cb26-199" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-200"><a href="#cb26-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-201"><a href="#cb26-201" aria-hidden="true" tabindex="-1"></a>Il risultato replica quello trovato in precedenza con la funzione di verosimiglianza.</span>
<span id="cb26-202"><a href="#cb26-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-205"><a href="#cb26-205" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb26-206"><a href="#cb26-206" aria-hidden="true" tabindex="-1"></a>input_list <span class="op">=</span> binom.logpmf(r, n, theta)</span>
<span id="cb26-207"><a href="#cb26-207" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-208"><a href="#cb26-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-211"><a href="#cb26-211" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb26-212"><a href="#cb26-212" aria-hidden="true" tabindex="-1"></a><span class="bu">max</span> <span class="op">=</span> input_list[<span class="dv">0</span>]</span>
<span id="cb26-213"><a href="#cb26-213" aria-hidden="true" tabindex="-1"></a>index <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb26-214"><a href="#cb26-214" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="bu">len</span>(input_list)):</span>
<span id="cb26-215"><a href="#cb26-215" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> input_list[i] <span class="op">&gt;</span> <span class="bu">max</span>:</span>
<span id="cb26-216"><a href="#cb26-216" aria-hidden="true" tabindex="-1"></a>        <span class="bu">max</span> <span class="op">=</span> input_list[i]</span>
<span id="cb26-217"><a href="#cb26-217" aria-hidden="true" tabindex="-1"></a>        index <span class="op">=</span> i</span>
<span id="cb26-218"><a href="#cb26-218" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Index of the maximum value is : </span><span class="sc">{</span>index<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb26-219"><a href="#cb26-219" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-220"><a href="#cb26-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-223"><a href="#cb26-223" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb26-224"><a href="#cb26-224" aria-hidden="true" tabindex="-1"></a>theta[index]</span>
<span id="cb26-225"><a href="#cb26-225" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-226"><a href="#cb26-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-227"><a href="#cb26-227" aria-hidden="true" tabindex="-1"></a><span class="fu">## Modello gaussiano</span></span>
<span id="cb26-228"><a href="#cb26-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-229"><a href="#cb26-229" aria-hidden="true" tabindex="-1"></a>Ora che abbiamo capito come costruire la funzione verosimiglianza di una binomiale è relativamente semplice fare un passo ulteriore e considerare la verosimiglianza del caso di una funzione di densità, ovvero nel caso di una variabile casuale continua. Consideriamo qui il caso della Normale. La densità di una distribuzione Normale di parametri $\mu$ e $\sigma$ è</span>
<span id="cb26-230"><a href="#cb26-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-231"><a href="#cb26-231" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb26-232"><a href="#cb26-232" aria-hidden="true" tabindex="-1"></a>f(y \mid \mu, \sigma) = \frac{1}{\sigma \sqrt{2\pi}} \exp\left<span class="sc">\{</span>-\frac{1}{2\sigma^2}(y-\mu)^2\right<span class="sc">\}</span>.</span>
<span id="cb26-233"><a href="#cb26-233" aria-hidden="true" tabindex="-1"></a>$$ {#eq-gaussian-sim-like}</span>
<span id="cb26-234"><a href="#cb26-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-235"><a href="#cb26-235" aria-hidden="true" tabindex="-1"></a>Costruiamo dunque la funzione di verosimiglianza nel caso dell'@eq-gaussian-sim-like.</span>
<span id="cb26-236"><a href="#cb26-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-237"><a href="#cb26-237" aria-hidden="true" tabindex="-1"></a><span class="fu">### Una singola osservazione</span></span>
<span id="cb26-238"><a href="#cb26-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-239"><a href="#cb26-239" aria-hidden="true" tabindex="-1"></a>Esaminiamo prima il caso in cui i dati corrispondono ad una singola osservazione $y$. Poniamo</span>
<span id="cb26-240"><a href="#cb26-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-243"><a href="#cb26-243" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb26-244"><a href="#cb26-244" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">114</span></span>
<span id="cb26-245"><a href="#cb26-245" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-246"><a href="#cb26-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-247"><a href="#cb26-247" aria-hidden="true" tabindex="-1"></a>L'@eq-gaussian-sim-like dipende dai parametri $\mu$ e $\sigma$ e dai dati $y$. Per semplicità, ipotizziamo $\sigma$ noto e uguale a 15. Nell'esercizio considereremo 1000 valori $\mu$ compresi tra 70 e 160.</span>
<span id="cb26-248"><a href="#cb26-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-251"><a href="#cb26-251" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb26-252"><a href="#cb26-252" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> np.linspace(<span class="fl">70.0</span>, <span class="fl">160.0</span>, num<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb26-253"><a href="#cb26-253" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-254"><a href="#cb26-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-255"><a href="#cb26-255" aria-hidden="true" tabindex="-1"></a>Dato che consideriamo 1000 possibili valori $\mu$, per costruire la funzione di verosimiglianza applicheremo 1000 volte l'@eq-gaussian-sim-like. In ciascun passo dell'esercizio inseriremo nell'@eq-gaussian-sim-like</span>
<span id="cb26-256"><a href="#cb26-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-257"><a href="#cb26-257" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>il singolo valore $y$ considerato (che viene mantenuto costante),</span>
<span id="cb26-258"><a href="#cb26-258" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>il valore $\sigma$ assunto noto (anch'esso costante),</span>
<span id="cb26-259"><a href="#cb26-259" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>uno alla volta ciascuno dei valori $\mu$ che abbiamo definito sopra (quindi, nelle 1000 applicazioni dell'@eq-gaussian-sim-like, il valore $\mu$ è l'unico che varia: $y$ e $\sigma$ sono mantenuti costanti).</span>
<span id="cb26-260"><a href="#cb26-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-261"><a href="#cb26-261" aria-hidden="true" tabindex="-1"></a>La distribuzione Gaussiana è implementata in Python mediante <span class="in">`norm.pdf()`</span>. La funzione <span class="in">`norm.pdf()`</span> richiede tre argomenti: il valore $y$ (o il vettore $y$), la media, ovvero il parametro $\mu$, e la deviazione standard, ovvero il parametro $\sigma$.</span>
<span id="cb26-262"><a href="#cb26-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-263"><a href="#cb26-263" aria-hidden="true" tabindex="-1"></a>Applicando la funzione <span class="in">`norm.pdf()`</span> 1000 volte, una volta per ciascuno dei valori $\mu$ che abbiamo definito (e tenendo fissi $y = 114$ e $\sigma = 15$), otteniamo 1000 valori $f(\mu)$.</span>
<span id="cb26-264"><a href="#cb26-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-267"><a href="#cb26-267" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb26-268"><a href="#cb26-268" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb26-269"><a href="#cb26-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-270"><a href="#cb26-270" aria-hidden="true" tabindex="-1"></a>f_mu <span class="op">=</span> norm.pdf(y, loc<span class="op">=</span>mu, scale<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb26-271"><a href="#cb26-271" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-272"><a href="#cb26-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-273"><a href="#cb26-273" aria-hidden="true" tabindex="-1"></a>La funzione di verosimiglianza è la curva che interpola i punti $\big(\mu, f(\mu)\big)$.</span>
<span id="cb26-274"><a href="#cb26-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-277"><a href="#cb26-277" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb26-278"><a href="#cb26-278" aria-hidden="true" tabindex="-1"></a>plt.plot(mu, f_mu, <span class="st">'r-'</span>)</span>
<span id="cb26-279"><a href="#cb26-279" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Funzione di verosimiglianza'</span>)</span>
<span id="cb26-280"><a href="#cb26-280" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Valore della variabile casuale mu [70, 160]'</span>)</span>
<span id="cb26-281"><a href="#cb26-281" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Verosimiglianza'</span>)</span>
<span id="cb26-282"><a href="#cb26-282" aria-hidden="true" tabindex="-1"></a>plt.xlim([<span class="dv">70</span>, <span class="dv">160</span>])</span>
<span id="cb26-283"><a href="#cb26-283" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb26-284"><a href="#cb26-284" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-285"><a href="#cb26-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-286"><a href="#cb26-286" aria-hidden="true" tabindex="-1"></a>Si noti che la funzione di verosimiglianza così trovata ha la forma della distribuzione Gaussiana. Nel caso di una singola osservazione, *ma solo in questo caso*, ha anche un'area unitaria.</span>
<span id="cb26-287"><a href="#cb26-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-288"><a href="#cb26-288" aria-hidden="true" tabindex="-1"></a>Per l'esempio presente, la moda della funzione di verosimiglianza è 114.</span>
<span id="cb26-289"><a href="#cb26-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-290"><a href="#cb26-290" aria-hidden="true" tabindex="-1"></a><span class="fu">### Un campione di osservazioni</span></span>
<span id="cb26-291"><a href="#cb26-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-292"><a href="#cb26-292" aria-hidden="true" tabindex="-1"></a>Consideriamo ora il caso più generale, ovvero quello di un campione di $n$ osservazioni. Possiamo immaginare un campione casuale $y_1, y_2, \dots, y_n$ estratto da una popolazione $\mathcal{N}(\mu, \sigma)$ come una sequenza di realizzazioni indipendenti ed identicamente distribuite (di seguito, i.i.d.) della medesima variabile casuale $Y \sim \mathcal{N}(\mu, \sigma)$. I parametri sconosciuti sono $\theta = <span class="sc">\{</span>\mu, \sigma<span class="sc">\}</span>$.</span>
<span id="cb26-293"><a href="#cb26-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-294"><a href="#cb26-294" aria-hidden="true" tabindex="-1"></a>Se le variabili casuali $y_1, y_2, \dots, y_n$ sono i.i.d., la loro densità congiunta è data da: \begin{align}</span>
<span id="cb26-295"><a href="#cb26-295" aria-hidden="true" tabindex="-1"></a>f(y \mid \theta) &amp;= f(y_1 \mid \theta) \cdot f(y_2 \mid \theta) \cdot \; \dots \; \cdot f(y_n \mid \theta)\notag<span class="sc">\\</span></span>
<span id="cb26-296"><a href="#cb26-296" aria-hidden="true" tabindex="-1"></a>                 &amp;= \prod_{i=1}^n f(y_i \mid \theta),</span>
<span id="cb26-297"><a href="#cb26-297" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb26-298"><a href="#cb26-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-299"><a href="#cb26-299" aria-hidden="true" tabindex="-1"></a>laddove $f(\cdot)$ è la densità Gaussiana di parametri $\mu, \sigma$. Tenendo costanti i dati $y$, la funzione di verosimiglianza diventa:</span>
<span id="cb26-300"><a href="#cb26-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-301"><a href="#cb26-301" aria-hidden="true" tabindex="-1"></a><span class="in">```{=tex}</span></span>
<span id="cb26-302"><a href="#cb26-302" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{equation}</span></span>
<span id="cb26-303"><a href="#cb26-303" aria-hidden="true" tabindex="-1"></a><span class="in">\mathcal{L}(\theta \mid y) = \prod_{i=1}^n f(y_i \mid \theta).</span></span>
<span id="cb26-304"><a href="#cb26-304" aria-hidden="true" tabindex="-1"></a><span class="in">\end{equation}</span></span>
<span id="cb26-305"><a href="#cb26-305" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-306"><a href="#cb26-306" aria-hidden="true" tabindex="-1"></a>Per chiarire la formula precedente, consideriamo un esempio che utilizza come dati i valori BDI-II dei trenta soggetti del campione clinico di Zetsche et al. (2020).</span>
<span id="cb26-307"><a href="#cb26-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-310"><a href="#cb26-310" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb26-311"><a href="#cb26-311" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> [<span class="dv">26</span>, <span class="dv">35</span>, <span class="dv">30</span>, <span class="dv">25</span>, <span class="dv">44</span>, <span class="dv">30</span>, <span class="dv">33</span>, <span class="dv">43</span>, <span class="dv">22</span>, <span class="dv">43</span>, <span class="dv">24</span>, <span class="dv">19</span>, <span class="dv">39</span>, <span class="dv">31</span>, <span class="dv">25</span>, <span class="dv">28</span>, <span class="dv">35</span>, <span class="dv">30</span>, <span class="dv">26</span>, </span>
<span id="cb26-312"><a href="#cb26-312" aria-hidden="true" tabindex="-1"></a>    <span class="dv">31</span>, <span class="dv">41</span>, <span class="dv">36</span>, <span class="dv">26</span>, <span class="dv">35</span>, <span class="dv">33</span>, <span class="dv">28</span>, <span class="dv">27</span>, <span class="dv">34</span>, <span class="dv">27</span>, <span class="dv">22</span>]</span>
<span id="cb26-313"><a href="#cb26-313" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-314"><a href="#cb26-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-315"><a href="#cb26-315" aria-hidden="true" tabindex="-1"></a>Ci poniamo l'obiettivo di creare la funzione di verosimiglianza per questi dati, supponendo di sapere (in base ai risultati di ricerche precedenti) che i punteggi BDI-II si distribuiscono secondo la legge Normale e supponendo $\sigma$ noto e uguale alla deviazione standard del campione.</span>
<span id="cb26-316"><a href="#cb26-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-319"><a href="#cb26-319" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb26-320"><a href="#cb26-320" aria-hidden="true" tabindex="-1"></a>true_sigma <span class="op">=</span> np.std(y)</span>
<span id="cb26-321"><a href="#cb26-321" aria-hidden="true" tabindex="-1"></a>true_sigma </span>
<span id="cb26-322"><a href="#cb26-322" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-323"><a href="#cb26-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-324"><a href="#cb26-324" aria-hidden="true" tabindex="-1"></a>Abbiamo visto in precedenza che, per una singola osservazione, la funzione di verosimiglianza è la densità Gaussiana espressa in funzione dei parametri (in questo caso, solo $\mu$). Per un campione di osservazioni i.i.d., ovvero $y = (y_1, y_2, \dots, y_n)$, la verosimiglianza è la funzione di densità congiunta $f(y \mid \mu, \sigma)$ espressa in funzione dei parametri. Dato che le osservazioni sono i.i.d., la densità congiunta è data dal prodotto delle densità delle singole osservazioni.</span>
<span id="cb26-325"><a href="#cb26-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-326"><a href="#cb26-326" aria-hidden="true" tabindex="-1"></a>Poniamoci il problema di trovare l'ordinata della funzione di log-verosimiglianza per le 30 osservazioni del campione in corrispondenza di $\mu = \mu_0$</span>
<span id="cb26-327"><a href="#cb26-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-328"><a href="#cb26-328" aria-hidden="true" tabindex="-1"></a>Per la prima osservazione del campione ($y_1 = 26$) abbiamo</span>
<span id="cb26-329"><a href="#cb26-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-330"><a href="#cb26-330" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb26-331"><a href="#cb26-331" aria-hidden="true" tabindex="-1"></a>f(26 \mid \mu_0, \sigma=6.50) = \frac{1}{{6.50 \sqrt {2\pi}}}\exp\left<span class="sc">\{</span>{-\frac{(26 - \mu_0)^2}{2\cdot 6.50^2}}\right<span class="sc">\}</span>.</span>
<span id="cb26-332"><a href="#cb26-332" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb26-333"><a href="#cb26-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-334"><a href="#cb26-334" aria-hidden="true" tabindex="-1"></a>Se consideriamo tutte le osservazioni, la densità congiunta è data dal prodotto delle densità delle singole osservazioni: $f(y \mid \mu, \sigma = 6.50) = \, \prod_{i=1}^n f(y_i \mid \mu, \sigma = 6.50)$. Utilizzando i dati del campione, e assumendo $\sigma = 6.50$, l'ordinata della funzione di verosimiglianza in corrispondenza di $\mu_0$ è uguale a</span>
<span id="cb26-335"><a href="#cb26-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-336"><a href="#cb26-336" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb26-337"><a href="#cb26-337" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb26-338"><a href="#cb26-338" aria-hidden="true" tabindex="-1"></a>\mathcal{L}(\mu_0, \sigma=6.50 \mid y) =&amp; \, \prod_{i=1}^{30} f(y_i \mid \mu_0, \sigma = 6.50) = \notag<span class="sc">\\</span></span>
<span id="cb26-339"><a href="#cb26-339" aria-hidden="true" tabindex="-1"></a>&amp; \frac{1}{{6.50 \sqrt {2\pi}}}\exp\left<span class="sc">\{</span>{-\frac{(26 - \mu_0)^2}{2\cdot 6.50^2}}\right<span class="sc">\}</span> \times \notag<span class="sc">\\</span></span>
<span id="cb26-340"><a href="#cb26-340" aria-hidden="true" tabindex="-1"></a> &amp; \frac{1}{{6.61 \sqrt {2\pi}}}\exp\left<span class="sc">\{</span>{-\frac{(35 - \mu_0)^2}{2\cdot 6.50^2}}\right<span class="sc">\}</span> \times  \notag<span class="sc">\\</span></span>
<span id="cb26-341"><a href="#cb26-341" aria-hidden="true" tabindex="-1"></a>&amp; \vdots \notag<span class="sc">\\</span></span>
<span id="cb26-342"><a href="#cb26-342" aria-hidden="true" tabindex="-1"></a> &amp; \frac{1}{{6.61 \sqrt {2\pi}}}\exp\left<span class="sc">\{</span>{-\frac{(22 - \mu_0)^2}{2\cdot 6.50^2}}\right<span class="sc">\}</span>.</span>
<span id="cb26-343"><a href="#cb26-343" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb26-344"><a href="#cb26-344" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb26-345"><a href="#cb26-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-346"><a href="#cb26-346" aria-hidden="true" tabindex="-1"></a>È più conveniente svolgere i calcoli usando il logaritmo della verosimiglianza. In Python definiamo la funzione di log-verosimiglianza, <span class="in">`log_likelihood()`</span>, che prende come argomenti <span class="in">`y`</span>, <span class="in">`mu`</span> e <span class="in">`sigma = 6.50`</span>.</span>
<span id="cb26-347"><a href="#cb26-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-350"><a href="#cb26-350" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb26-351"><a href="#cb26-351" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_likelihood(y, mu, sigma <span class="op">=</span> true_sigma):</span>
<span id="cb26-352"><a href="#cb26-352" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">sum</span>(norm.logpdf(y, loc<span class="op">=</span>mu, scale<span class="op">=</span><span class="dv">15</span>))</span>
<span id="cb26-353"><a href="#cb26-353" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-354"><a href="#cb26-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-355"><a href="#cb26-355" aria-hidden="true" tabindex="-1"></a>Consideriamo il valore $\mu_0 = \bar{y}$, ovvero</span>
<span id="cb26-356"><a href="#cb26-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-359"><a href="#cb26-359" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb26-360"><a href="#cb26-360" aria-hidden="true" tabindex="-1"></a>bar_y <span class="op">=</span> np.mean(y)</span>
<span id="cb26-361"><a href="#cb26-361" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(bar_y)</span>
<span id="cb26-362"><a href="#cb26-362" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-363"><a href="#cb26-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-364"><a href="#cb26-364" aria-hidden="true" tabindex="-1"></a>L'ordinata della funzione di log-verosimiglianza in corrispondenza di $\mu = 30.93$ è</span>
<span id="cb26-365"><a href="#cb26-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-368"><a href="#cb26-368" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb26-369"><a href="#cb26-369" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(log_likelihood(y, <span class="fl">30.93</span>, sigma <span class="op">=</span> true_sigma))</span>
<span id="cb26-370"><a href="#cb26-370" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-371"><a href="#cb26-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-372"><a href="#cb26-372" aria-hidden="true" tabindex="-1"></a>Troviamo ora i valori della log-verosimiglianza per ciascuno dei 1000 valori $\mu$ nell'intervallo $<span class="co">[</span><span class="ot">\bar{y} - 2 \sigma, \bar{y} + 2 \sigma</span><span class="co">]</span>$. Iniziamo a definire <span class="in">`mu`</span>.</span>
<span id="cb26-373"><a href="#cb26-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-376"><a href="#cb26-376" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb26-377"><a href="#cb26-377" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> np.linspace(np.mean(y) <span class="op">-</span> <span class="dv">2</span><span class="op">*</span>np.std(y), np.mean(y) <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>np.std(y), num<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb26-378"><a href="#cb26-378" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-379"><a href="#cb26-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-380"><a href="#cb26-380" aria-hidden="true" tabindex="-1"></a>Troviamo il valore dell'ordinata della funzione di log-verosimiglianza in corrispondenza di ciascuno dei 1000 valori <span class="in">`mu`</span> che abbiamo definito.</span>
<span id="cb26-381"><a href="#cb26-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-384"><a href="#cb26-384" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb26-385"><a href="#cb26-385" aria-hidden="true" tabindex="-1"></a>ll <span class="op">=</span> [log_likelihood(y, mu_val, true_sigma) <span class="cf">for</span> mu_val <span class="kw">in</span> mu]</span>
<span id="cb26-386"><a href="#cb26-386" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-387"><a href="#cb26-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-388"><a href="#cb26-388" aria-hidden="true" tabindex="-1"></a>Nel caso di un solo parametro sconosciuto (nel caso presente, $\mu$) è possibile rappresentare la log-verosimiglianza con una curva che interpola i punti (<span class="in">`mu`</span>, <span class="in">`ll`</span>). Tale funzione descrive la *credibilità relativa* che può essere attribuita ai valori del parametro $\mu$ alla luce dei dati osservati.</span>
<span id="cb26-389"><a href="#cb26-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-390"><a href="#cb26-390" aria-hidden="true" tabindex="-1"></a><span class="in">```{python, fig-loglike-bdi2, fig.cap="Log-verosimiglianza del parametro $\\mu$ per i dati di @zetschefuture2019."}</span></span>
<span id="cb26-391"><a href="#cb26-391" aria-hidden="true" tabindex="-1"></a>plt.plot(mu, ll, <span class="st">'r-'</span>)</span>
<span id="cb26-392"><a href="#cb26-392" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Funzione di log-verosimiglianza'</span>)</span>
<span id="cb26-393"><a href="#cb26-393" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Valore della variabile casuale mu'</span>)</span>
<span id="cb26-394"><a href="#cb26-394" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Log-verosimiglianza'</span>)</span>
<span id="cb26-395"><a href="#cb26-395" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span>np.mean(y), color<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, ls<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb26-396"><a href="#cb26-396" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb26-397"><a href="#cb26-397" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-398"><a href="#cb26-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-399"><a href="#cb26-399" aria-hidden="true" tabindex="-1"></a><span class="fu">### Massima verosimiglianza</span></span>
<span id="cb26-400"><a href="#cb26-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-401"><a href="#cb26-401" aria-hidden="true" tabindex="-1"></a>Il valore $\mu$ più credibile corrisponde al massimo della funzione di log-verosimiglinza e viene detto *stima di massima verosimiglianza*.</span>
<span id="cb26-402"><a href="#cb26-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-403"><a href="#cb26-403" aria-hidden="true" tabindex="-1"></a>Il massimo della funzione di log-verosimiglianza, ovvero 30.93 nel caso dell'esempio presente, è identico alla media dei dati campionari. Tale risultato, ottenuto per via numerica, può essere dimostrato formalmente (ma non lo faremo qui). Usando la notazione matematica possiamo dire che cerchiamo l'argmax dell'equazione precedente rispetto a $\theta$, ovvero</span>
<span id="cb26-404"><a href="#cb26-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-405"><a href="#cb26-405" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb26-406"><a href="#cb26-406" aria-hidden="true" tabindex="-1"></a>\hat{\theta} = \text{argmax}_{\theta} \prod_{i=1}^n f(y_i \mid \theta).</span>
<span id="cb26-407"><a href="#cb26-407" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb26-408"><a href="#cb26-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-409"><a href="#cb26-409" aria-hidden="true" tabindex="-1"></a>Questo problema si risolve calcolando le derivate della funzione rispetto a $\theta$, ponendo le derivate uguali a zero e risolvendo. Saltando tutti i passaggi algebrici di questo procedimento, per $\mu$ si trova</span>
<span id="cb26-410"><a href="#cb26-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-411"><a href="#cb26-411" aria-hidden="true" tabindex="-1"></a><span class="in">```{=tex}</span></span>
<span id="cb26-412"><a href="#cb26-412" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{equation}</span></span>
<span id="cb26-413"><a href="#cb26-413" aria-hidden="true" tabindex="-1"></a><span class="in">\hat{\mu} = \frac{1}{n} \sum_{i=1}^n y_i</span></span>
<span id="cb26-414"><a href="#cb26-414" aria-hidden="true" tabindex="-1"></a><span class="in">\end{equation}</span></span>
<span id="cb26-415"><a href="#cb26-415" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-416"><a href="#cb26-416" aria-hidden="true" tabindex="-1"></a>e</span>
<span id="cb26-417"><a href="#cb26-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-418"><a href="#cb26-418" aria-hidden="true" tabindex="-1"></a><span class="in">```{=tex}</span></span>
<span id="cb26-419"><a href="#cb26-419" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{equation}</span></span>
<span id="cb26-420"><a href="#cb26-420" aria-hidden="true" tabindex="-1"></a><span class="in">\hat{\sigma} = \sqrt{\sum_{i=1}^n\frac{1}{n}(y_i- \mu)^2}.</span></span>
<span id="cb26-421"><a href="#cb26-421" aria-hidden="true" tabindex="-1"></a><span class="in">\end{equation}</span></span>
<span id="cb26-422"><a href="#cb26-422" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb26-423"><a href="#cb26-423" aria-hidden="true" tabindex="-1"></a>In altri termini, la s.m.v. del parametro $\mu$ è la media del campione e la s.m.v. del parametro $\sigma$ è la deviazione standard del campione.</span>
<span id="cb26-424"><a href="#cb26-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-425"><a href="#cb26-425" aria-hidden="true" tabindex="-1"></a>::: {#exr-loglike-bdi2-1}</span>
<span id="cb26-426"><a href="#cb26-426" aria-hidden="true" tabindex="-1"></a>Dalla @fig-loglike-bdi2 notiamo che il massimo della funzione di log-verosimiglianza calcolata per via numerica, ovvero 30.93, è identico alla media dei dati campionari e corrisponde al risultato teorico atteso.</span>
<span id="cb26-427"><a href="#cb26-427" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb26-428"><a href="#cb26-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-429"><a href="#cb26-429" aria-hidden="true" tabindex="-1"></a><span class="fu">## Commenti e considerazioni finali {.unnumbered}</span></span>
<span id="cb26-430"><a href="#cb26-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-431"><a href="#cb26-431" aria-hidden="true" tabindex="-1"></a>Nella funzione di verosimiglianza i dati (osservati) vengono trattati come fissi, mentre i valori del parametro (o dei parametri) $\theta$ vengono variati: la verosimiglianza è una funzione di $\theta$ per il dato fisso $y$. Pertanto, la funzione di verosimiglianza riassume i seguenti elementi: un modello statistico che genera stocasticamente i dati (in questo capitolo abbiamo esaminato due modelli statistici: quello binomiale e quello Normale), un intervallo di valori possibili per $\theta$ e i dati osservati $y$.</span>
<span id="cb26-432"><a href="#cb26-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-433"><a href="#cb26-433" aria-hidden="true" tabindex="-1"></a>Nella statistica frequentista l'inferenza si basa solo sui dati a disposizione e qualunque informazione fornita dalle conoscenze precedenti non viene presa in considerazione. Nello specifico, nella statistica frequentista l'inferenza viene condotta massimizzando la funzione di (log) verosimiglianza, condizionatamente ai valori assunti dalle variabili casuali campionarie. Le basi dell'inferenza frequentista, dunque, sono state riassunte in questo Capitolo. Nella statistica bayesiana, invece, l'inferenza statistica viene condotta combinando la funzione di verosimiglianza con le distribuzioni a priori dei parametri incogniti $\theta$. Ciò verrà discusso nei Capitoli successivi.</span>
<span id="cb26-434"><a href="#cb26-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-435"><a href="#cb26-435" aria-hidden="true" tabindex="-1"></a>La differenza fondamentale tra inferenza bayesiana e frequentista è dunque che i frequentisti non ritengono utile descrivere i parametri in termini probabilistici: i parametri dei modelli statistici vengono concepiti come fissi ma sconosciuti. Nell'inferenza bayesiana, invece, i parametri sconosciuti sono intesi come delle variabili casuali e ciò consente di quantificare in termini probabilistici il nostro grado di intertezza relativamente al loro valore.</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>