<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Data Science per psicologi - 7&nbsp; La logica dell’incerto</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./016_conditional_prob.html" rel="next">
<link href="./prob.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script><script src="site_libs/kePrint-0.0.1/kePrint.js"></script><link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"><div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">La logica dell’incerto</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Data Science per psicologi</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/ccaudek/data_science/" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Benvenuti</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">Prefazione</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./basics.html" class="sidebar-item-text sidebar-link">Parte 1: Nozioni di base</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./001_key_notions.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Concetti chiave</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./005_measurement.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">La misurazione in psicologia</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./007_freq_distr.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Analisi esplorativa dei dati</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./011_loc_scale.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Indici di posizione e di scala</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./012_correlation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Le relazioni tra variabili</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./013_penguins.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Manipolazione e visualizzazione dei dati in <span class="math inline">\(\mathsf{R}\)</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./prob.html" class="sidebar-item-text sidebar-link">Parte 2: Il calcolo delle probabilità</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./015_prob_intro.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">La logica dell’incerto</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./016_conditional_prob.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Probabilità condizionata: significato, teoremi, eventi indipendenti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./017_bayes_theorem.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Il teorema di Bayes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./018_expval_var.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Indici di posizione, di varianza e di associazione di variabili casuali</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./019_joint_prob.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Probabilità congiunta</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./020_density_func.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">La densità di probabilità</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./distr.html" class="sidebar-item-text sidebar-link">Parte 3: Distribuzioni di v.c. discrete e continue</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./022_discr_rv_distr.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. discrete</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./023_cont_rv_distr.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Distribuzioni di v.c. continue</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./024_likelihood.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">La funzione di verosimiglianza</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./bayes_inference.html" class="sidebar-item-text sidebar-link">Parte 4: Inferenza bayesiana</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./025_intro_bayes.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Credibilità, modelli e parametri</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./026_subj_prop.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Pensare ad una proporzione in termini soggettivi</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./029_conjugate_families.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./030_balance_prior_post.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">L’influenza della distribuzione a priori</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./036_posterior_sim.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Approssimazione della distribuzione a posteriori</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./040_beta_binomial_mod.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Il modello beta-binomiale in linguaggio Stan</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./041_mcmc_diagnostics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Diagnostica delle catene markoviane</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./045_summarize_posterior.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Sintesi a posteriori</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./046_bayesian_prediction.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">La predizione bayesiana</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./050_normal_normal_mod.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Inferenza sul parametro <span class="math inline">\(\mu\)</span> (media di una v.c. Normale)</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./regression.html" class="sidebar-item-text sidebar-link">Parte 5: Regressione lineare</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./051_reglin1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./052_reglin2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Regressione lineare bivariata</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./053_reglin3.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Modello di regressione in linguaggio Stan</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./054_reglin4.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Inferenza sul modello lineare</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./055_reglin5.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Confronto tra due gruppi indipendenti</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./056_pred_check.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Predictive checks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./060_anova.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Confronto tra le medie di tre o più gruppi</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./070_mod_hier.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Modello gerarchico</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./071_mod_hier_sim.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Modello gerarchico: simulazioni</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./entropy.html" class="sidebar-item-text sidebar-link">Parte 6: Entropia</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./090_entropy.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Entropia</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./091_kl.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">La divergenza di Kullback-Leibler</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./092_info_criterion.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Criterio di informazione e convalida incrociata</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./frequentist_inference.html" class="sidebar-item-text sidebar-link">Parte 7: Inferenza frequentista</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./220_intro_frequentist.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Legge dei grandi numeri</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./221_conf_interv.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Intervallo fiduciale</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./225_distr_camp_mean.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Distribuzione campionaria</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./226_test_ipotesi.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Significatività statistica</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./227_ttest.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Inferenza sulle medie</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./228_limiti_stat_frequentista.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Limiti dell’inferenza frequentista</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./999_refs.html" class="sidebar-item-text sidebar-link">Riferimenti bibliografici</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">Appendici</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a01_math_symbols.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Simbologia di base</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a02_number_sets.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Numeri binari, interi, razionali, irrazionali e reali</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a03_set_theory.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Insiemi</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a04_summation_notation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Simbolo di somma (sommatorie)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a05_calculus_notation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Per liberarvi dai terrori preliminari</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a10_markov_chains.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Le catene di Markov</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a15_stan_lang.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Programmare in Stan</span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><!-- margin-sidebar --><div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Sommario</h2>
   
  <ul>
<li>
<a href="#che-cos%C3%A8-la-probabilit%C3%A0" id="toc-che-cosè-la-probabilità" class="nav-link active" data-scroll-target="#che-cos%C3%A8-la-probabilit%C3%A0"><span class="toc-section-number">7.1</span>  Che cos’è la probabilità?</a>
  <ul class="collapse">
<li><a href="#formalizzazione-dellincertezza" id="toc-formalizzazione-dellincertezza" class="nav-link" data-scroll-target="#formalizzazione-dellincertezza"><span class="toc-section-number">7.1.1</span>  Formalizzazione dell’incertezza</a></li>
  </ul>
</li>
  <li>
<a href="#variabili-casuali-e-probabilit%C3%A0-di-un-evento" id="toc-variabili-casuali-e-probabilità-di-un-evento" class="nav-link" data-scroll-target="#variabili-casuali-e-probabilit%C3%A0-di-un-evento"><span class="toc-section-number">7.2</span>  Variabili casuali e probabilità di un evento</a>
  <ul class="collapse">
<li><a href="#eventi-e-probabilit%C3%A0" id="toc-eventi-e-probabilità" class="nav-link" data-scroll-target="#eventi-e-probabilit%C3%A0"><span class="toc-section-number">7.2.1</span>  Eventi e probabilità</a></li>
  <li><a href="#spazio-campione-e-risultati-possibili" id="toc-spazio-campione-e-risultati-possibili" class="nav-link" data-scroll-target="#spazio-campione-e-risultati-possibili"><span class="toc-section-number">7.2.2</span>  Spazio campione e risultati possibili</a></li>
  </ul>
</li>
  <li><a href="#distribuzione-di-probabilit%C3%A0" id="toc-distribuzione-di-probabilità" class="nav-link" data-scroll-target="#distribuzione-di-probabilit%C3%A0"><span class="toc-section-number">7.3</span>  Distribuzione di probabilità</a></li>
  <li><a href="#usare-la-simulazione-per-stimare-le-probabilit%C3%A0" id="toc-usare-la-simulazione-per-stimare-le-probabilità" class="nav-link" data-scroll-target="#usare-la-simulazione-per-stimare-le-probabilit%C3%A0"><span class="toc-section-number">7.4</span>  Usare la simulazione per stimare le probabilità</a></li>
  <li><a href="#la-legge-dei-grandi-numeri" id="toc-la-legge-dei-grandi-numeri" class="nav-link" data-scroll-target="#la-legge-dei-grandi-numeri"><span class="toc-section-number">7.5</span>  La legge dei grandi numeri</a></li>
  <li><a href="#variabili-casuali-multiple" id="toc-variabili-casuali-multiple" class="nav-link" data-scroll-target="#variabili-casuali-multiple"><span class="toc-section-number">7.6</span>  Variabili casuali multiple</a></li>
  <li>
<a href="#sec-fun-mass-prob" id="toc-sec-fun-mass-prob" class="nav-link" data-scroll-target="#sec-fun-mass-prob"><span class="toc-section-number">7.7</span>  Funzione di massa di probabilità</a>
  <ul class="collapse">
<li><a href="#funzione-di-ripartizione" id="toc-funzione-di-ripartizione" class="nav-link" data-scroll-target="#funzione-di-ripartizione"><span class="toc-section-number">7.7.1</span>  Funzione di ripartizione</a></li>
  </ul>
</li>
  <li><a href="#commenti-e-considerazioni-finali" id="toc-commenti-e-considerazioni-finali" class="nav-link" data-scroll-target="#commenti-e-considerazioni-finali">Commenti e considerazioni finali</a></li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-intro-prob-1" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">La logica dell’incerto</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Codice</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header><div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pandas for managing datasets</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Matplotlib for additional customization</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Seaborn for plotting and styling</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> random</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> bernoulli, binom</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Set theme</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># sns.set_theme(style="ticks")</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>sns.set_palette(<span class="st">"colorblind"</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># cwd = os.getcwd()</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># print(cwd)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In questa parte della dispensa verrà introdotta la teoria delle probabilità. Prima di entrare nei dettagli, cerchiamo di capire perché la probabilità sia così importante per la ricerca scientifica.</p>
<!-- Ingenuamente, potremmo pensare che il modo migliore di procedere nella ricerca scientifica sia quello di usare la logica deduttiva (aristotelica) -- con un tale metodo, infatti, siamo sicuri di non commettere errori. Un esempio è il sillogismo, come  -->
<!-- - tutti gli uomini sono mortali,  -->
<!-- - Socrate è un uomo,  -->
<!-- - quindi, Socrate è mortale. -->
<!-- La logica deduttiva, però, non può essere utilizzata in psicologia, né in alcun'altra scienza empirica. Nel sillogismo, la correttezza del ragionamento dipende dalla sua struttura e non dal significato delle parole (come uomo, mortale, ecc.). Nelle scienze empiriche, però, il "significato delle parole" è cruciale. Le "parole" usate nel sillogismo corrispondono ai "concetti teorici" (detti, in psicologia, costrutti) delle teorie scientifiche. Il problema è che la _corrispondenza_ tra relazioni tra costrutti teorici, da una parte, e relazioni tra i fenomeni empirici, dall'altra, dipende dalla validità delle teorie. In fisica, ad esempio, i concetti teorici di massa ($m$), peso ($P$) e forza di gravità ($g$) consentono di descrivere accuratamente ciò che si osserva nel mondo empirico: $P = m \cdot g$. Non è così, invece, nelle scienze sociali, dove le relazioni tra costrutti sono in grado di descrivere _solo in parte_ le relazioni tra i corrispondenti fenomeni empirici.  -->
<p>La teoria delle probabilità è cruciale per la scienza perché la ricerca procede mediante l’inferenza induttiva. Non siamo mai completamente sicuri della verità di una proposizione (ipotesi, teoria): al valore di verità di una proposizione possiamo solo assegnare un grado di certezza probabilistico. L’approccio bayesiano è una scuola di pensiero che usa la probabilità per quantificare il grado di fiducia che può essere attribuito ad una proposizione. L’inferenza statistica bayesiana è un tipo di inferenza induttiva che ha lo scopo di quantificare la fiducia che si ha nell’ipotesi <span class="math inline">\(H\)</span> dopo avere osservato il dato di evidenza <span class="math inline">\(E\)</span>. Per quantificare un tale grado di fiducia l’inferenza statistica bayesiana utilizza, appunto, la teoria delle probabilità. Una comprensione dell’inferenza statistica bayesiana richiede dunque, preliminarmente, la conoscenza (di almeno gli elementi di base) della teoria delle probabilità.</p>
<!-- ## Proposizioni e modelli statistici -->
<!-- Nell'inferenza bayesiana, le proposizioni di una teoria scientifica sono espresse nella forma di un modello statistico, ovvero mediante una legge generale che descrive il modo in cui un fenomeno si manifesta. Tale legge generale viene anche detta _processo generativo dei dati_.^[Per fare un esempio, consideriamo il quoziente d'intelligenza. Sappiamo che il punteggio totale della _Wechsler Adult Intelligence Scale_ ha, nella popolazione, media 100 e deviazione standard 15 (dato che il test WAIS è stato costruito in modo da avere una tale proprietà). Quindi, se prendiamo un campione abbastanza grande di persone, i valori del QI di tali persone avranno, circa, media uguale a 100 e deviazione standard uguale a 15. Se con tali dati costruiamo un istogramma, sappiamo anche che il profilo di tale istogramma sarà ben descritto da una funzione matematica che va sotto il nome di _legge gaussiana_. La rappresentazione grafica della funzione gaussiana è la classica curva a campana che sicuramente avrete già vista. La funzione gaussiana dipende da due parametri: la media (solitamente indicata con $\mu$) e la deviazione standard ($\sigma$). Se cambiamo questi parametri, ma usiamo sempre la stessa formula, otteniamo una curva diversa.  Per esempio, se consideriamo solo la sotto-popolazione dei bambini plus-dotati, la distribuzione dei punteggi QI sarà una gaussiana centrata su 130, con una qualche deviazione standard. In questo esempio, la gaussiana è il modello generatore dei dati e i parametri sono $\mu$ e $\sigma$. Per altri fenomeni, come ad esempio i tempi di reazione nel compito Stroop, o la gravità della sintomatologia ansiosa negli adulti misurata attraverso il test _Beck Anxiety Inventory_, il modello gaussiano non è più appropriato ed è necessario _ipotizzare_ un diverso processo generativo dei dati.] In generale, l'inferenza induttiva bayesiana procede _ipotizzando_ un modello generativo dei dati per poi, sulla base dei dati osservati in un campione e sulla base delle nostre credenze a priori, _inferire_ i valori plausibili dei parametri del modello. In questo processo inferenziale, possiamo individuare cinque fonti di incertezza: -->
<!-- 1. incertezza sui parametri dei modelli; -->
<!-- 2. incertezza su quale sia il modello migliore; -->
<!-- 3. incertezza su cosa fare con l'output dei (migliori) modelli; -->
<!-- 4. incertezza sul funzionamento del software che produce i risultati; -->
<!-- 5. incertezza sul fatto che il/i modello/i (migliore/i) siano coerenti con altri campioni di dati. -->
<!-- * L'approccio bayesiano usa la teoria delle probabilità per descrivere l'incertezza relativa ai punti (1) e (2); -->
<!-- * l'approccio bayesiano si collega alla teoria delle decisioni, che prescrive come affrontare il problema descritto nel punto (3); -->
<!-- * il software utilizzato (nel nostro caso, Stan) fa tutto ciò che è possibile per mitigare la preoccupazione (4); -->
<!-- * l'approccio bayesiano consente di quantificare l'incertezza descritta al punto (5). -->
<!-- ## Oggettività e soggettività -->
<!-- Facendo delle assunzioni non controverse, l'inferenza bayesiana consente di aggiornare le credenze a priori sui valori (sconosciuti) dei parametri $\theta$ di un modello statistico alla luce di nuovi dati $y_1, y_2, \dots, y_N$ che vengono osservati. L'approccio bayesiano è etichettato come "soggettivo" perché non ci dice quale valore dovrebbe essere assegnato ai parametri prima di avere osservato i dati $y_1, y_2, \dots, y_N$. In realtà, l'aggiornamento bayesiano è il modo più razionale di procedere: se, prima di avere osservato i dati, il ricercatore ha una credenza assurda relativamente al valore di $\theta$, dopo avere osservato $y_1, y_2, \dots, y_N$ le sue credenze _a posteriori_ su $\theta$, aggiornate secondo i principi bayesiani, saranno meno assurde. Il problema di questo modo di procedere non è che, a priori, i ricercatori (o chiunque altro) possono avere delle credenze sbagliate, ma bensì il fatto che, avendo osservato $y_1, y_2, \dots, y_N$, le credenze su $\theta$ non vengono aggiornate secondo i principi bayesiani. Infatti, in alcune situazioni, l'osservazione di dati che contraddicono le credenze pregresse non fa altro che rafforzare tali convinzioni errate -- il problema, dunque, non nasce dalla "soggettività" dell'approccio bayesiano, ma quanto dal fatto di non seguire un tale modo di procedere! -->
<!-- Lo scopo delle prossime sezioni della dispensa è quello di introdurre quei concetti base della teoria delle probabilità che risultano necessari per una presentazione delle procedure dell'inferenza induttiva bayesiana. -->
<section id="che-cosè-la-probabilità" class="level2" data-number="7.1"><h2 data-number="7.1" class="anchored" data-anchor-id="che-cosè-la-probabilità">
<span class="header-section-number">7.1</span> Che cos’è la probabilità?</h2>
<p>La definizione della probabilità è un problema estremamente dibattuto ed aperto. Sono state fornite due possibili soluzioni al problema di definire il concetto di probabilità.</p>
<ol type="a">
<li><p>La natura della probabilità è “ontologica” (ovvero, basata sulla metafisica): la probabilità è una proprietà della della realtà, del mondo, di come sono le cose, indipendentemente dalla nostra esperienza. È una visione che qualcuno chiama “oggettiva”.</p></li>
<li><p>La natura della probabilità è “epistemica” (ovvero, basata sulla conoscenza): la probabilità si riferisce alla conoscenza che abbiamo del mondo, non al mondo in sé. Di conseguenza è detta, in contrapposizione alla precedente definizione, “soggettiva”.</p></li>
</ol>
<p>In termini epistemici, la probabilità fornisce una misura della nostra incertezza sul verificarsi di un evento, alla luce delle informazioni disponibili. Potremmo dire che c’è una “scala” naturale che ha per estremi il vero (1: evento certo), da una parte, ed il falso (0: evento impossibile), dall’altra. La probabilità è la quantificazione di questa scala: descrive lo stato della nostra incertezza rispetto al contenuto di verità di una proposizione.</p>
<p>L’incertezza nelle nostre previsioni può sorgere per due ragioni fondamentalmente diverse. La prima è dovuta alla nostra ignoranza relativamente alle cause nascoste sottostanti o dei meccanismi che generano i dati. Questa è, appunto, un’incertezza <em>epistemica</em>. Il secondo tipo di incertezza deriva invece dalla variabilità intrinseca dei fenomeni, che non può essere ridotta anche se raccogliamo più dati. Questa seconda forma di incertezza è talvolta chiamata <em>aleatoria</em>. Come esempio concreto, consideriamo il lancio di una moneta equilibrata. Sappiamo con certezza che la probabilità di testa è <span class="math inline">\(P = 0.5\)</span>, quindi non c’è incertezza epistemica, ma non questo non è sufficiente per prevedere con certezza il risultato – in altre parole, l’incertezza aleatoria persiste anche in assenza di incertezza epistemica.</p>
<p>L’interpretazione bayesiana di probabilità si contrappone all’interpretazione frequentista. Nell’interpretazione frequentista, la probabilità <span class="math inline">\(P(E)\)</span> rappresenta la frequenza relativa a lungo termine di un grande numero di ripetizioni di un esperimento casuale sotto le medesime condizioni. Viene stressata qui l’idea che ciò di cui parliamo è qualcosa che emerge nel momento in cui è possibile ripetere l’esperimento casuale tante volte sotto le medesime condizioni – sono invece esclusi gli eventi unici e irripetibili.</p>
<!-- - Nell'interpretazione bayesiana della probabilità, $P(A)$ rappresenta il grado di fiducia che si ha relativamente al verificarsi dell'evento $A$. -->
<!-- Possiamo citare De Finetti, ad esempio, il quale ha formulato la seguente definizione "soggettiva" di probabilità la quale risulta applicabile anche ad esperimenti casuali i cui eventi elementari non siano ritenuti ugualmente possibili e che non siano necessariamente ripetibili più volte sotto le stesse condizioni.  -->
<p>L’interpretazione bayesiana della probabilità fa invece ricorso ad una concezione più ampia, non legata al solo evento in sé, ma che include anche il soggetto assegnante la funzione di probabilità. In pratica l’assegnazione di probabilità bayesiana viene effettuata dal decisore, in base alle proprie conoscenze a priori integrate con tutto il generico bagaglio culturale personale. In questo modo, la probabilità non sarà obbligatoriamente la stessa per tutti i soggetti, ma variarierà a seconda delle informazioni a disposizione, dell’esperienza personale e soprattutto del punto di vista proprio di ogni decisore ed è dunque assimilabile al “grado di fiducia” – in inglese <em>degree of belief</em> – di un dato soggetto, in un dato istante e con un dato insieme d’informazioni, circa l’accadere dell’evento <span class="math inline">\(E\)</span>.</p>
<blockquote class="blockquote">
<p>[N]essuna scienza ci permetterà di dire: il tale fatto accadrà, andrà così e così, perché ciò è conseguenza di tale legge, e tale legge è una verità assoluta, ma tanto meno ci condurrà a concludere scetticamente: la verità assoluta non esiste, e quindi tale fatto può accadere e può non accadere, può andare così e può andare in tutt’altro modo, nulla io ne so. Quel che si potrà dire è questo: io prevedo che il tale fatto avverrà, e avverrà nel tal modo, perché l’esperienza del passato e l’elaborazione scientifica cui il pensiero dell’uomo l’ha sottoposta mi fanno sembrare ragionevole questa previsione” <span class="citation" data-cites="definetti1931prob">(<a href="999_refs.html#ref-definetti1931prob" role="doc-biblioref">Finetti, 1931</a>)</span>.</p>
</blockquote>
<div class="callout-note callout callout-style-simple">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>La caratterizzazione ‘epistemica’ della nozione di probabilità può essere chiarita facendo riferimento all’esempio prototipico con il quale la probabilità viene solitamente descritta, ovvero come una frequenza relativa. A questo proposito, McElreath ci chiede di riflettere con maggiore attenzione sul fenomeno costituito da una sequenza di lanci di una moneta, ovvero l’esempio tradizionale con il quale si descrive un evento “aleatorio”. Ingenuamente potremmo pensare che un tale fenomeno sia “casuale”, nel senso che, all’interno della sequenza, non vi è alcuna informazione negli eventi (lanci) passati che sia utile per prevedere gli eventi futuri. Ma non è così. Il lancio di una moneta è un fenomeno deterministico, regolato dalle leggi fisiche. Infatti, sono state create delle macchine che, applicando la stessa forza ogni volta, sono in grado di ripetere lo stesso esito (testa o croce) in ogni prova. Questo significa che la “casualità” della sequenza di lanci non è una proprietà del fenomeno fisico che vorremmo descrivere (i fenomeni fisici sono sempre deterministici in quanto sono regolati dalle leggi della fisica) ma bensì è epistemica, ovvero riguarda lo stato dell’informazione disponibile all’osservatore.</p>
</div>
</div>
</div>
<section id="formalizzazione-dellincertezza" class="level3" data-number="7.1.1"><h3 data-number="7.1.1" class="anchored" data-anchor-id="formalizzazione-dellincertezza">
<span class="header-section-number">7.1.1</span> Formalizzazione dell’incertezza</h3>
<p>La caratterizzazione della probabilità quale rappresentazione dell’incertezza epistemica è stata formalizzata in ambito bayesiano da Ramsey e de Finetti. De Finetti riconduce l’assegnazione di probabilità allo scommettere sul verificarsi di un evento: la probabilità di un evento <span class="math inline">\(E\)</span> è la quota <span class="math inline">\(p(E)\)</span> che un individuo reputa di dover pagare ad un banco per ricevere “1” ovvero “0” verificandosi o non verificandosi <span class="math inline">\(E\)</span>.</p>
<p>In termini formali, secondo De Finetti, le valutazioni di probabilità degli eventi devono rispondere ai principi di equità e coerenza.</p>
<ul>
<li>Una scommessa risponde al principio di <em>equità</em> se il ruolo di banco e giocatore sono scambiabili in ogni momento del gioco e sempre alle stesse condizioni.</li>
<li>Una scommessa risponde al principio di <em>coerenza</em> se non vi sono combinazioni di scommesse che consentano (sia al banco che al giocatore) di realizzare perdite o vincite certe.</li>
</ul>
<p>L’approccio definettiano dell’impostazione della scommessa si basa dunque sulle assunzioni di razionalità e coerenza del decisore, al quale è fatto esplicito divieto di effettuare scommesse a perdita o guadagno certo. Il decisore, proponendo la scommessa, deve essere disposto a scambiare il posto dello scommettitore con quello del banco.</p>
<p>Il metodo della scommessa, oltre che una definizione, fornisce un mezzo operativo di assegnazione della probabilità. Sulla base di questa definizione operativa, che si può ritenere ragionevolmente soddisfatta dal comportamento di un qualunque individuo che agisca in modo razionale in condizioni di incertezza, possono essere agevolmente dimostrate tutte le proprietà classiche della probabilità: essa non può assumere valori negativi, né può essere superiore all’unità; se <span class="math inline">\(E\)</span> è un evento certo, la sua probabilità è 1; se invece <span class="math inline">\(E\)</span> è un evento impossibile, la sua probabilità è 0.</p>
<p>I problemi posti dall’approccio definettiano riguardano l’arbitrarietà dell’assegnazione soggettività di probabilità la quale sembra negare la validità dell’intero costrutto teorico. In risposta a tale critica, i bayesiani sostengono che gli approcci oggettivisti alla probabilità nascondono scelte arbitrarie preliminari e sono basate su assunzioni implausibili. È molto più onesto esplicitare subito tutte le scelte arbitrarie effettuate nel corso dell’analisi in modo da controllarne coerenza e razionalità.</p>
<div class="callout-note callout callout-style-simple">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>Per chi desidera approfondire, un’introduzione molto leggibile alle tematiche della definizione della probabilità nella storia della scienza è fornita nel primo capitolo del testo <em>Bernoulli’s fallacy</em> <span class="citation" data-cites="clayton2021bernoulli">(<a href="999_refs.html#ref-clayton2021bernoulli" role="doc-biblioref">Clayton, 2021</a>)</span>.</p>
</div>
</div>
</div>
<!-- In altri termini, de Finetti ritiene che la probabilità debba essere concepita non come una proprietà "oggettiva" dei fenomeni, ma bensì come il "grado di fiducia" -- in inglese _degree of belief_ -- di un dato soggetto, in un dato istante e con un dato insieme d'informazioni, riguardo al verificarsi di un evento. Per denotare sia la probabilità (soggettiva) di un evento sia il concetto di _valore atteso_ (che descriveremo in seguito), @definetti1970teoria utilizza il termine "previsione" (e lo stesso simbolo $P$): *"la previsione [$\dots$] consiste nel considerare ponderatamente tutte le alternative possibili per ripartire fra di esse nel modo che parrà più appropriato le proprie aspettative, le proprie sensazioni di probabilità."* -->
</section></section><section id="variabili-casuali-e-probabilità-di-un-evento" class="level2" data-number="7.2"><h2 data-number="7.2" class="anchored" data-anchor-id="variabili-casuali-e-probabilità-di-un-evento">
<span class="header-section-number">7.2</span> Variabili casuali e probabilità di un evento</h2>
<p>Esaminiamo qui di seguito alcuni concetti di base della teoria delle probabilità, la quale può essere intesa come un’estensione della logica.</p>
<section id="eventi-e-probabilità" class="level3" data-number="7.2.1"><h3 data-number="7.2.1" class="anchored" data-anchor-id="eventi-e-probabilità">
<span class="header-section-number">7.2.1</span> Eventi e probabilità</h3>
<p>Nella teoria delle probabilità il risultato “testa” nel lancio di una moneta è chiamato <em>evento</em>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Un evento, denotato da una variabile binaria, corrisponde ad uno stato del mondo che si verifica oppure no. Ad esempio, <span class="math inline">\(Y\)</span> = 1 può denotare l’evento per cui il lancio di una moneta produce il risultato testa. Il funzionale <span class="math inline">\(P(Y)\)</span> denota la probabilità con cui si ritiene che l’evento <span class="math inline">\(Y\)</span> sia vero (o la proporzione di volte che si verifica tale evento osservando a lungo termine delle ripetizioni indipendenti di un esperimento casuale). Ad esempio, per il lancio di una moneta equilibrata, la probabilità dell’evento “il risultato del lancio della moneta è testa” è scritta come <span class="math inline">\(P(Y = 1) = 0.5.\)</span></p>
<p>Se la moneta è equilibrata dobbiamo anche avere <span class="math inline">\(P(Y = 0) = 0.5\)</span>. I due eventi <span class="math inline">\(Y\)</span> = 1 e <span class="math inline">\(Y\)</span> = 0 sono <em>mutuamente esclusivi</em> nel senso che non possono entrambi verificarsi contemporaneamente: <span class="math inline">\(P(Y = 1\; \land \; Y = 0) = 0.\)</span> Gli eventi <span class="math inline">\(Y\)</span> = 1 e <span class="math inline">\(Y\)</span> = 0 di dicono <em>esaustivi</em>, nel senso che almeno uno di essi deve verificarsi e nessun altro tipo di evento è possibile. Nella notazione probabilistica, <span class="math inline">\(P(Y = 1\; \lor \; Y = 0) = 1.\)</span> Il connettivo logico “o” (<span class="math inline">\(\lor\)</span>) specifica eventi <em>disgiunti</em>, ovvero eventi che non possono verificarsi contemporaneamente (eventi <em>incompatibili</em>) e per i quali, perciò, la probabilità della loro congiunzione è <span class="math inline">\(P(A \; \land \; B) = 0\)</span>. Il connettivo logico “e” (<span class="math inline">\(\land\)</span>), invece, specifica eventi <em>congiunti</em>, ovvero eventi che possono verificarsi contemporaneamente (eventi <em>compatibili</em>) e per i quali, perciò, la probabilità della loro congiunzione è <span class="math inline">\(P(A \; \land \; B) &gt; 0\)</span>. La probabilità del verificarsi di due eventi congiunti <span class="math inline">\(A\)</span> e <span class="math inline">\(B\)</span> si può denotare, in maniera equivalente, con la notazione precedente, oppure con <span class="math inline">\(P(A \cap B)\)</span>, oppure con <span class="math inline">\(P(A, B)\)</span>.</p>
<p>Si richiede che <span class="math inline">\(0 \leq P(A) \leq 1\)</span>, dove <span class="math inline">\(P(A) = 0\)</span> denota l’evento impossibile e <span class="math inline">\(P(A) = 1\)</span> denota l’evento certo. Scriviamo <span class="math inline">\(P(\lnot A)\)</span> o <span class="math inline">\(P(\bar{A})\)</span> per denotare la probabilità che l’evento <span class="math inline">\(A\)</span> non avvenga; questa probabilità è definita come <span class="math inline">\(P(\bar{A}) = 1 − P(A)\)</span>.</p>
</section><section id="spazio-campione-e-risultati-possibili" class="level3" data-number="7.2.2"><h3 data-number="7.2.2" class="anchored" data-anchor-id="spazio-campione-e-risultati-possibili">
<span class="header-section-number">7.2.2</span> Spazio campione e risultati possibili</h3>
<p>Anche se il lancio di una moneta produce sempre uno specifico risultato nel mondo reale, possiamo anche immaginare i possibili risultati alternativi che si sarebbero potuti osservare. Quindi, anche se in uno specifico lancio la moneta dà testa (<span class="math inline">\(Y\)</span> = 1), possiamo immaginare la possibilità che il lancio possa avere prodotto croce (<span class="math inline">\(Y\)</span> = 0). Tale ragionamento controfattuale è la chiave per comprendere la teoria delle probabilità e l’inferenza statistica.</p>
<p>I risultati possibili che si possono osservare come conseguenza del lancio di una moneta determinano i valori possibili che la variabile casuale può assumere. L’insieme <span class="math inline">\(\Omega\)</span> di tutti i risultati possibili è chiamato <em>spazio campione</em> (<em>sample space</em>). Lo spazio campione può essere concettualizzato come un’urna contenente una pallina per ogni possibile risultato del lancio della moneta. Su ogni pallina è scritto il valore della variabile casuale. Uno specifico lancio di una moneta – ovvero, l’osservazione di uno specifico valore di una variabile casuale – è chiamato <em>esperimento casuale</em>.</p>
<p>Il lancio di un dado ci fornisce l’esempio di un altro esperimento casuale. Supponiamo di essere interessati all’evento “il lancio del dado produce un numero dispari”. Un <em>evento</em> seleziona un sottoinsieme dello spazio campione: in questo caso, l’insieme dei risultati <span class="math inline">\(\{1, 3, 5\}\)</span>. Se esce 3, per esempio, diciamo che si è verificato l’evento “dispari” (ma l’evento “dispari” si sarebbe anche verificato anche se fosse uscito 1 o 5).</p>
</section></section><section id="distribuzione-di-probabilità" class="level2" data-number="7.3"><h2 data-number="7.3" class="anchored" data-anchor-id="distribuzione-di-probabilità">
<span class="header-section-number">7.3</span> Distribuzione di probabilità</h2>
<p>Sia <span class="math inline">\(Y\)</span> il risultato del lancio di moneta equilibrata; non di un generico lancio di una moneta, ma un’istanza specifica del lancio di una specifica moneta in un dato momento. Definita in questo modo, <span class="math inline">\(Y\)</span> è una <em>variabile casuale</em>, ovvero una variabile i cui valori non possono essere previsti con esattezza. Se la moneta è equilibrata, c’è una probabilità del 50% che il lancio della moneta dia come risultato “testa” e una probabilità del 50% che dia come risultato “croce”. Per facilitare la trattazione, le variabili casuali assumono solo valori numerici. Per lo specifico lancio della moneta in questione, diciamo, ad esempio, che la variabile casuale <span class="math inline">\(Y\)</span> assume il valore 1 se esce testa e il valore 0 se esce croce.</p>
<p>Una variabile casuale può essere <em>discreta</em> o <em>continua</em>. Una variabile casuale discreta può assumere un numero finito di valori <span class="math inline">\(x_1, \dots ,x_n\)</span>, in corrispondenza degli eventi <span class="math inline">\(E_i, \dots, E_n\)</span> che si verificano con le rispettive probabilità <span class="math inline">\(p_1, \dots, p_n\)</span>. Un esempio è il punteggio totale di un test psicometrico costituito da item su scala Likert. Invece un esempio di una variabile casuale continua è la distanza tra due punti, che può assumere infiniti valori all’interno di un certo intervallo. L’insieme <span class="math inline">\(S\)</span> dei valori che la variabile casuale può assumere è detto <em>spazio dei valori</em> o <em>spazio degli stati</em>.</p>
<p>La caratteristica fondamentale di una variabile casuale è data dall’insieme delle probabilità dei suoi valori, detta <em>distribuzione di probabilità</em>. Nel seguito useremo la notazione <span class="math inline">\(P(\cdot)\)</span> per fare riferimento alle distribuzioni di probabilità delle variabili casuali discrete e <span class="math inline">\(p(\cdot)\)</span> per fare riferimento alla densità di probabilità delle variabili casuali continue. In questo contesto, l’insieme dei valori che la variabile casuale può assumere è detto <em>supporto</em> della sua distribuzione di probabilità. Il supporto di una variabile casuale può essere finito (come nel caso di una variabile casuale uniforme di supporto <span class="math inline">\([a, b]\)</span>) o infinito (nel caso di una variabile causale gaussiana il cui supporto coincide con la retta reale).</p>
</section><section id="usare-la-simulazione-per-stimare-le-probabilità" class="level2" data-number="7.4"><h2 data-number="7.4" class="anchored" data-anchor-id="usare-la-simulazione-per-stimare-le-probabilità">
<span class="header-section-number">7.4</span> Usare la simulazione per stimare le probabilità</h2>
<!-- I metodi basati sulla simulazione consentono di stimare le probabilità degli eventi in un modo diretto, se siamo in grado di generare molteplici e casuali realizzazioni delle variabili casuali coinvolte nelle definizioni degli eventi. -->
<p>In questa dispensa verrà adottata l’interpretazione bayesiana delle probabilità. Tuttavia, le regole di base della teoria delle probabilità sono le stesse, indipendentemente dall’interpretazione adottata. Pertanto, negli esempi seguenti, possiamo utilizzare la simulazione per stimare le probabilità degli eventi in un modo diretto, ovvero mediante la generazione di molteplici osservazioni delle variabili casuali derivate dagli eventi di interesse.</p>
<p>Simuliamo il lancio di una moneta equilibrata.</p>
<p>Definiamo una variabile casuale bernoulliana <span class="math inline">\(X\)</span> con <span class="math inline">\(\pi\)</span> (probabilità di successo) uguale a 0.5.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> bernoulli(p)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Esaminiamo 20 realizzazioni di <span class="math inline">\(X\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> X.rvs(<span class="dv">20</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 0 0]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>La stima della probabilità dell’evento <span class="math inline">\(P(Y = 1)\)</span> è data dalla frequenza relativa del numero di volte in cui abbiamo osservato l’evento di interesse (<span class="math inline">\(Y = 1\)</span>).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>np.mean(x)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Esaminiamo ora una simulazione nella quale facciamo variare l’ampiezza del campione usato per la stima empirica della probabilità. Nella simulazione vengono generati 10,000 campioni di ampiezza <span class="math inline">\(n\)</span>. Di ciascuno di questi campioni casuali viene calcolata la media. Le 10,000 stime empiriche di <span class="math inline">\(\pi\)</span> vengono poi visualizzate con un istogramma.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cointoss(p<span class="op">=</span><span class="fl">0.5</span>, size<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    niter <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> bernoulli(p)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    random_samples <span class="op">=</span> []</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(niter):</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> X.rvs(size)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        random_samples.append(x)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    rs <span class="op">=</span> np.array(random_samples)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    p_estimates <span class="op">=</span> np.mean(rs, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    sns.histplot(p_estimates, stat <span class="op">=</span> <span class="st">"density"</span>)<span class="op">;</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    plt.xlim([<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Iniziamo con campioni di ampiezza <span class="math inline">\(n\)</span> = 10.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>cointoss(size<span class="op">=</span><span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="015_prob_intro_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Dato che la moneta è equilibrata, le stime empiriche della probabilità dell’evento <span class="math inline">\(P(Y = 1)\)</span> sono uguali, <em>in media</em>, al valore che ci aspettiamo, ovvero <span class="math inline">\(P(Y = 1) = 0.5\)</span>, ma il risultato ottenuto varia di molto da campione a campione. Proviamo ora ad aumentare il numero di lanci in ciascun campione: <span class="math inline">\(n\)</span> = 100.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>cointoss(size<span class="op">=</span><span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="015_prob_intro_files/figure-html/unnamed-chunk-8-3.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>In questo secondo caso, gli errori tendono ad essere più piccoli che nel caso precedente. Usiamo ora 1000 lanci in ciascun campione.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>cointoss(size<span class="op">=</span><span class="dv">1000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="015_prob_intro_files/figure-html/unnamed-chunk-9-5.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Ora le stime empiriche che otteniamo in ciascun campione sono tutte molto vicine alla probabilità vera (cioè 0.5, perché la moneta è equilibrata).</p>
<p>I risultati delle simulazioni precedenti pongono dunque il problema di determinare quale sia il numero di lanci di cui abbiamo bisogno per assicurarci che le stime empiriche di <span class="math inline">\(\pi\)</span> siano accurate (ovvero, vicine al valore corretto della probabilità teorica).</p>
</section><section id="la-legge-dei-grandi-numeri" class="level2" data-number="7.5"><h2 data-number="7.5" class="anchored" data-anchor-id="la-legge-dei-grandi-numeri">
<span class="header-section-number">7.5</span> La legge dei grandi numeri</h2>
<p>La visualizzazione mediante grafici contribuisce alla comprensione dei concetti della statistica e della teoria delle probabilità. Un modo per descrivere ciò che accade all’aumentare del numero <span class="math inline">\(M\)</span> di ripetizioni del lancio della moneta consiste nel registrare la stima della probabilità dell’evento <span class="math inline">\(P(Y = 1)\)</span> in funzione del numero di ripetizioni dell’esperimento casuale per ogni <span class="math inline">\(m \in 1:M\)</span>. Possiamo ottenere un grafico dell’andamento della stima di <span class="math inline">\(P(Y = 1)\)</span> in funzione di <span class="math inline">\(m\)</span> nel modo seguente:</p>
<div class="cell" data-layout-align="center" data-hash="015_prob_intro_cache/html/fig-legge-grandi-n-1_c21119a15e257492028978997091cb84">
<div class="cell-output-display">
<div id="fig-legge-grandi-n-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="015_prob_intro_files/figure-html/fig-legge-grandi-n-1-7.png" class="img-fluid figure-img" width="576"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;7.1: Stima della probabilità di successo in funzione del numero dei lanci di una moneta.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>La <a href="#fig-legge-grandi-n-1">Figura&nbsp;<span>7.1</span></a>, quando è espressa su una scala lineare, non rivela chiaramente l’andamento della simulazione. Imponiamo dunque una scala logaritmica sull’asse delle ascisse (<span class="math inline">\(x\)</span>). Su scala logaritmica, i valori tra 1 e 10 vengono tracciati all’incirca con la stessa ampiezza che si osserva tra i valori 50 e 700, eccetera.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-legge-grandi-n-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="015_prob_intro_files/figure-html/fig-legge-grandi-n-2-7.png" class="img-fluid figure-img" width="576"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;7.2: Stima della probabilità di successo in funzione del numero dei lanci di una moneta.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>La <a href="#fig-legge-grandi-n-2">Figura&nbsp;<span>7.2</span></a> fornisce una rappresentazione grafica della legge dei grandi numeri. La <em>legge dei grandi numeri</em> dice che, all’aumentare del numero di ripetizioni dell’esperimento casuale, la media dei risultati ottenuti tende al valore atteso, man mano che vengono eseguite più prove. Nella figura <a href="#fig-legge-grandi-n-2">Figura&nbsp;<span>7.2</span></a> vediamo infatti che, all’aumentare del numero <em>M</em> di lanci della moneta, la stima di <span class="math inline">\(P(Y = 1)\)</span> converge al valore 0.5.</p>
</section><section id="variabili-casuali-multiple" class="level2" data-number="7.6"><h2 data-number="7.6" class="anchored" data-anchor-id="variabili-casuali-multiple">
<span class="header-section-number">7.6</span> Variabili casuali multiple</h2>
<p>Le variabili casuali non esistono isolatamente. Abbiamo iniziato con una sola variabile casuale <span class="math inline">\(Y\)</span> che rappresenta il risultato di un singolo, specifico lancio di una moneta equlibrata. Ma supponiamo ora di lanciare la moneta tre volte. I risultati di ciascuno dei tre lanci possono essere rappresentati da una diversa variabile casuale, ad esempio, <span class="math inline">\(Y_1 , Y_2 , Y_3\)</span>. Possiamo assumere che ogni lancio sia indipendente, ovvero che non dipenda dal risultato degli altri lanci. Per ciascuna di queste variabili <span class="math inline">\(Y_n\)</span>, con <span class="math inline">\(n \in 1:3\)</span>, abbiamo che <span class="math inline">\(P(Y_n =1)=0.5\)</span> e <span class="math inline">\(P(Y_n =0)=0.5\)</span>.</p>
<p>È possibile combinare più variabili casuali usando le operazioni aritmetiche. Se <span class="math inline">\(Y_1 , Y_2, Y_3\)</span> sono variabili casuali che rappresentano tre lanci di una moneta equilibrata (o, in maniera equivalente, un lancio di tre monete equilibrate), possiamo definire la somma di tali variabili casuali come</p>
<p><span class="math display">\[
Z = Y_1 + Y_2 + Y_3.
\]</span></p>
<p>Possiamo simulare i valori assunti dalla variabile casuale <span class="math inline">\(Z\)</span> simulando i valori di <span class="math inline">\(Y_1, Y_2, Y_3\)</span> per poi sommarli.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>size <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>np.<span class="bu">sum</span>(X.rvs(size))</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Ripetiamo questa simulazione <span class="math inline">\(M\)</span> = 10,000 volte.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>size <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>niter <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>random_samples <span class="op">=</span> [X.rvs(size) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(niter)]</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>rs <span class="op">=</span> np.array(random_samples)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>p_estimates <span class="op">=</span> np.mean(rs, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(p_estimates)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 10000</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>e calcoliamo poi una stima della probabilità che la variabile casuale <span class="math inline">\(Z\)</span> assuma ciascuno dei possibili valori 0, 1, 2, 3. Nel caso di 4 monete equilibrate, abbiamo:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(p_estimates, columns<span class="op">=</span>[<span class="st">'p_est'</span>])</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;       p_est</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 0      0.50</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1      0.50</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 2      0.25</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 3      0.50</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 4      0.50</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ...     ...</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 9995   1.00</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 9996   0.75</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 9997   0.25</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 9998   0.50</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 9999   0.25</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [10000 rows x 1 columns]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'p_est'</span>].value_counts() <span class="op">/</span> niter</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 0.50    0.3752</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 0.25    0.2552</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 0.75    0.2445</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1.00    0.0660</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 0.00    0.0591</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Name: p_est, dtype: float64</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Una variabile casuale le cui modalità possono essere costituite solo da numeri interi è detta <em>variabile casuale discreta</em>:</p>
<p><span class="math display">\[
\mathbb{Z} = \dots, -2, -1, 0, 1, 2, \dots
\]</span></p>
</section><section id="sec-fun-mass-prob" class="level2" data-number="7.7"><h2 data-number="7.7" class="anchored" data-anchor-id="sec-fun-mass-prob">
<span class="header-section-number">7.7</span> Funzione di massa di probabilità</h2>
<p>È conveniente avere una funzione che associa una probabilità a ciascun possibile valore di una variabile casuale. In generale, ciò è possibile se e solo se la variabile casuale è discreta, così com’è stata definita nel paragrafo precedente. Ad esempio, se consideriamo <span class="math inline">\(Z = Y_1 + \dots + Y_4\)</span> come, ad esempio, il numero di risultati “testa” in 4 lanci della moneta, allora possiamo definire la seguente funzione:</p>
<p><span class="math display">\[
\begin{array}{rclll}
p_Z(0) &amp; = &amp; 1/16 &amp; &amp; \mathrm{TTTT}
\\
p_Z(1) &amp; = &amp; 4/16 &amp; &amp; \mathrm{HTTT, THTT, TTHT, TTTH}
\\
p_Z(2) &amp; = &amp; 6/16 &amp; &amp; \mathrm{HHTT, HTHT, HTTH, THHT, THTH, TTTH}
\\
p_Z(3) &amp; = &amp; 4/16 &amp; &amp; \mathrm{HHHT, HHTH, HTHH, THHH}
\\
p_Z(4) &amp; = &amp; 1/16 &amp; &amp; \mathrm{HHHH}
\end{array}
\]</span></p>
<p>Il lancio di quattro monete può produrre 16 risultati possibili. Dato che i lanci sono indipendenti, se le monete sono equilibrate ogni possibile risultato è ugualmente probabile. Nella tabella precedente, le sequenze dei risultati possibili del lancio delle 4 monete sono riportate nella colonna più a destra. Le probabilità si ottengono dividendo il numero di sequenze che producono lo stesso numero di eventi testa per il numero dei risultati possibili.</p>
<p>Le sequenze come <span class="math inline">\(\mathrm{TTTT}\)</span>, <span class="math inline">\(\mathrm{HTTT}\)</span>, ecc. sono chiamate “eventi elementari” (ovvero, corrispondono ad un possibile esito dell’esperimento casuale). L’evento <span class="math inline">\(Z = u\)</span>, con <span class="math inline">\(u \in 0 \dots, 4\)</span> è un “evento composto”, il quale può essere costituito da più eventi elementari.</p>
<p>La funzione <span class="math inline">\(p_Z\)</span> è stata costruita per associare a ciascun valore <span class="math inline">\(u\)</span> della variabile casuale <span class="math inline">\(Z\)</span> la probabilità dell’evento <span class="math inline">\(Z = u\)</span>. Convenzionalmente, queste probabilità sono scritte come</p>
<p><span class="math display">\[
P_Z(z) = P(Z = z).
\]</span></p>
<p>La parte a destra dell’uguale si può leggere come: “la probabilità che la variabile casuale <span class="math inline">\(Z\)</span> assuma il valore <span class="math inline">\(z\)</span>”. Una funzione definita come sopra è detta <em>funzione di massa di probabilità</em> della variabile casuale <span class="math inline">\(Z\)</span>. Ad ogni variabile casuale discreta è associata un’unica funzione di massa di probabilità.</p>
<p>Una rappresentazione grafica della stima della funzione di massa di probabilità per l’esperimento casuale del lancio di quattro monete equilibrate è fornita nella <a href="#fig-barplot-mdf-4coins">Figura&nbsp;<span>7.3</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-barplot-mdf-4coins" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="015_prob_intro_files/figure-html/fig-barplot-mdf-4coins-1.png" class="img-fluid figure-img" width="576"></p>
<p></p><figcaption class="figure-caption">Figura&nbsp;7.3: Grafico di <span class="math inline">\(M = 100,000\)</span> simulazioni della funzione di massa di probabilità di una variabile casuale definita come il numero di teste in quattro lanci di una moneta equilibrata.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Se <span class="math inline">\(A\)</span> è un sottoinsieme della variabile casuale <span class="math inline">\(Z\)</span>, allora denotiamo con <span class="math inline">\(P_{z}(A)\)</span> la probabilità assegnata ad <span class="math inline">\(A\)</span> dalla distribuzione <span class="math inline">\(P_{z}\)</span>. Mediante una distribuzione di probabilità <span class="math inline">\(P_{z}\)</span> è possibile determinare la probabilità di ciascun sottoinsieme <span class="math inline">\(A \subset Z\)</span> come</p>
<span class="math display">\[\begin{equation}
P_{z}(A) = \sum_{z \in A} P_{z}(Z = z).
\end{equation}\]</span>
<p>Una funzione di massa di probabilità soddisfa le proprietà</p>
<ul>
<li>
<span class="math inline">\(0 \leq P(X=x) \leq 1\)</span>,</li>
<li>
<span class="math inline">\(\sum_{x \in X} P(x) = 1\)</span>.</li>
</ul>
<div id="exr-prob-4-coins-1" class="theorem exercise">
<p><span class="theorem-title"><strong>Esercizio 7.1 </strong></span>Per l’esempio discusso nella <a href="#sec-fun-mass-prob"><span>Sezione&nbsp;7.7</span></a>, la probabilità che la variabile casuale <span class="math inline">\(Z\)</span> sia un numero dispari è</p>
<p><span class="math display">\[
P(\text{Z è un numero dispari}) = P_{z}(Z = 1) + P_{z}(Z = 3) = \frac{4}{16} + \frac{4}{16} = \frac{1}{2}.
\]</span></p>
</div>
<section id="funzione-di-ripartizione" class="level3" data-number="7.7.1"><h3 data-number="7.7.1" class="anchored" data-anchor-id="funzione-di-ripartizione">
<span class="header-section-number">7.7.1</span> Funzione di ripartizione</h3>
<p>Data una variabile casuale discreta <span class="math inline">\(X\)</span> possiamo calcolare la probabilità che <span class="math inline">\(X\)</span> non superi un certo valore <span class="math inline">\(x\)</span>, ossia la sua <em>funzione di ripartizione</em>. Poichè <span class="math inline">\(X\)</span> assume valori discreti possiamo cumulare le probabilità mediante una somma:</p>
<p><span class="math display">\[
F(x_k) = P(X \leq x_k) = \sum_{x \leq x_k} P(x).
\]</span></p>
<div id="exr-prob-4-coins-2" class="theorem exercise">
<p><span class="theorem-title"><strong>Esercizio 7.2 </strong></span>Per l’esempio discusso nella <a href="#sec-fun-mass-prob"><span>Sezione&nbsp;7.7</span></a>, la funzione di ripartizione della variabile casuale <span class="math inline">\(Z\)</span> è fornita nella tabella seguente.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">

<table class=" lightable-paper lightable-hover table table-striped table-hover table-condensed" style="font-family: &quot;Arial Narrow&quot;, arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto; margin-left: auto; margin-right: auto;">
<thead><tr>
<th style="text-align:right;"> z </th>
   <th style="text-align:right;"> pz </th>
   <th style="text-align:right;"> cum_prob </th>
  </tr></thead>
<tbody>
<tr>
<td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0.0625 </td>
   <td style="text-align:right;"> 0.0625 </td>
  </tr>
<tr>
<td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0.2500 </td>
   <td style="text-align:right;"> 0.3125 </td>
  </tr>
<tr>
<td style="text-align:right;"> 2 </td>
   <td style="text-align:right;"> 0.3750 </td>
   <td style="text-align:right;"> 0.6875 </td>
  </tr>
<tr>
<td style="text-align:right;"> 3 </td>
   <td style="text-align:right;"> 0.2500 </td>
   <td style="text-align:right;"> 0.9375 </td>
  </tr>
<tr>
<td style="text-align:right;"> 4 </td>
   <td style="text-align:right;"> 0.0625 </td>
   <td style="text-align:right;"> 1.0000 </td>
  </tr>
</tbody>
</table>
</div>
</div>
</div>
</section></section><section id="commenti-e-considerazioni-finali" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="commenti-e-considerazioni-finali">Commenti e considerazioni finali</h2>
<p>In questo capitolo abbiamo visto come si costruisce lo spazio campione di un esperimento casuale, quali sono le proprietà di base della probabilità e come si assegnano le probabilità agli eventi definiti sopra uno spazio campione discreto. Abbiamo anche introdotto le nozioni di variabile casuale, ovvero di una variabile che assume i suoi valori in maniera casuale. Abbiamo descritto il modo di specificare la probabilità con cui sono una variabile casuale assume i suoi differenti valori, ovvero la funzione di ripartizione <span class="math inline">\(F(X) = P(X &lt; x)\)</span> e la funzione di massa di probabilità.</p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-line-spacing="2" role="doc-bibliography" style="display: none">
<div id="ref-clayton2021bernoulli" class="csl-entry" role="doc-biblioentry">
Clayton, A. (2021). <em>Bernoulli’s fallacy: Statistical illogic and the crisis of modern science</em>. Columbia University Press.
</div>
<div id="ref-definetti1931prob" class="csl-entry" role="doc-biblioentry">
Finetti, B. de. (1931). Probabilismo. <em>Logos</em>, 163–219.
</div>
</div>
</section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr>
<ol>
<li id="fn1"><p>Per un ripasso delle nozioni di base della teoria degli insiemi, si veda <span class="quarto-unresolved-ref">?sec-appendix-sets</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./prob.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Parte 2: Il calcolo delle probabilità</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./016_conditional_prob.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Probabilità condizionata: significato, teoremi, eventi indipendenti</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb13" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># La logica dell'incerto {#sec-intro-prob-1}</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, include = FALSE}</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(<span class="st">"_common.R"</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"kableExtra"</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"tidyverse"</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Pandas for managing datasets</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Matplotlib for additional customization</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Seaborn for plotting and styling</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> random</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> bernoulli, binom</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Set theme</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a><span class="co"># sns.set_theme(style="ticks")</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>sns.set_palette(<span class="st">"colorblind"</span>)</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a><span class="co"># cwd = os.getcwd()</span></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a><span class="co"># print(cwd)</span></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>In questa parte della dispensa verrà introdotta la teoria delle probabilità. Prima di entrare nei dettagli, cerchiamo di capire perché la probabilità sia così importante per la ricerca scientifica.</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Ingenuamente, potremmo pensare che il modo migliore di procedere nella ricerca scientifica sia quello di usare la logica deduttiva (aristotelica) -- con un tale metodo, infatti, siamo sicuri di non commettere errori. Un esempio è il sillogismo, come  --&gt;</span></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - tutti gli uomini sono mortali,  --&gt;</span></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - Socrate è un uomo,  --&gt;</span></span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - quindi, Socrate è mortale. --&gt;</span></span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- La logica deduttiva, però, non può essere utilizzata in psicologia, né in alcun'altra scienza empirica. Nel sillogismo, la correttezza del ragionamento dipende dalla sua struttura e non dal significato delle parole (come uomo, mortale, ecc.). Nelle scienze empiriche, però, il "significato delle parole" è cruciale. Le "parole" usate nel sillogismo corrispondono ai "concetti teorici" (detti, in psicologia, costrutti) delle teorie scientifiche. Il problema è che la _corrispondenza_ tra relazioni tra costrutti teorici, da una parte, e relazioni tra i fenomeni empirici, dall'altra, dipende dalla validità delle teorie. In fisica, ad esempio, i concetti teorici di massa ($m$), peso ($P$) e forza di gravità ($g$) consentono di descrivere accuratamente ciò che si osserva nel mondo empirico: $P = m \cdot g$. Non è così, invece, nelle scienze sociali, dove le relazioni tra costrutti sono in grado di descrivere _solo in parte_ le relazioni tra i corrispondenti fenomeni empirici.  --&gt;</span></span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>La teoria delle probabilità è cruciale per la scienza perché la ricerca procede mediante l'inferenza induttiva. Non siamo mai completamente sicuri della verità di una proposizione (ipotesi, teoria): al valore di verità di una proposizione possiamo solo assegnare un grado di certezza probabilistico. L'approccio bayesiano è una scuola di pensiero che usa la probabilità per quantificare il grado di fiducia che può essere attribuito ad una proposizione. L'inferenza statistica bayesiana è un tipo di inferenza induttiva che ha lo scopo di quantificare la fiducia che si ha nell'ipotesi $H$ dopo avere osservato il dato di evidenza $E$. Per quantificare un tale grado di fiducia l'inferenza statistica bayesiana utilizza, appunto, la teoria delle probabilità. Una comprensione dell'inferenza statistica bayesiana richiede dunque, preliminarmente, la conoscenza (di almeno gli elementi di base) della teoria delle probabilità.</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ## Proposizioni e modelli statistici --&gt;</span></span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Nell'inferenza bayesiana, le proposizioni di una teoria scientifica sono espresse nella forma di un modello statistico, ovvero mediante una legge generale che descrive il modo in cui un fenomeno si manifesta. Tale legge generale viene anche detta _processo generativo dei dati_.^[Per fare un esempio, consideriamo il quoziente d'intelligenza. Sappiamo che il punteggio totale della _Wechsler Adult Intelligence Scale_ ha, nella popolazione, media 100 e deviazione standard 15 (dato che il test WAIS è stato costruito in modo da avere una tale proprietà). Quindi, se prendiamo un campione abbastanza grande di persone, i valori del QI di tali persone avranno, circa, media uguale a 100 e deviazione standard uguale a 15. Se con tali dati costruiamo un istogramma, sappiamo anche che il profilo di tale istogramma sarà ben descritto da una funzione matematica che va sotto il nome di _legge gaussiana_. La rappresentazione grafica della funzione gaussiana è la classica curva a campana che sicuramente avrete già vista. La funzione gaussiana dipende da due parametri: la media (solitamente indicata con $\mu$) e la deviazione standard ($\sigma$). Se cambiamo questi parametri, ma usiamo sempre la stessa formula, otteniamo una curva diversa.  Per esempio, se consideriamo solo la sotto-popolazione dei bambini plus-dotati, la distribuzione dei punteggi QI sarà una gaussiana centrata su 130, con una qualche deviazione standard. In questo esempio, la gaussiana è il modello generatore dei dati e i parametri sono $\mu$ e $\sigma$. Per altri fenomeni, come ad esempio i tempi di reazione nel compito Stroop, o la gravità della sintomatologia ansiosa negli adulti misurata attraverso il test _Beck Anxiety Inventory_, il modello gaussiano non è più appropriato ed è necessario _ipotizzare_ un diverso processo generativo dei dati.] In generale, l'inferenza induttiva bayesiana procede _ipotizzando_ un modello generativo dei dati per poi, sulla base dei dati osservati in un campione e sulla base delle nostre credenze a priori, _inferire_ i valori plausibili dei parametri del modello. In questo processo inferenziale, possiamo individuare cinque fonti di incertezza: --&gt;</span></span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- 1. incertezza sui parametri dei modelli; --&gt;</span></span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- 2. incertezza su quale sia il modello migliore; --&gt;</span></span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- 3. incertezza su cosa fare con l'output dei (migliori) modelli; --&gt;</span></span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- 4. incertezza sul funzionamento del software che produce i risultati; --&gt;</span></span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- 5. incertezza sul fatto che il/i modello/i (migliore/i) siano coerenti con altri campioni di dati. --&gt;</span></span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- * L'approccio bayesiano usa la teoria delle probabilità per descrivere l'incertezza relativa ai punti (1) e (2); --&gt;</span></span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- * l'approccio bayesiano si collega alla teoria delle decisioni, che prescrive come affrontare il problema descritto nel punto (3); --&gt;</span></span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- * il software utilizzato (nel nostro caso, Stan) fa tutto ciò che è possibile per mitigare la preoccupazione (4); --&gt;</span></span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- * l'approccio bayesiano consente di quantificare l'incertezza descritta al punto (5). --&gt;</span></span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ## Oggettività e soggettività --&gt;</span></span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Facendo delle assunzioni non controverse, l'inferenza bayesiana consente di aggiornare le credenze a priori sui valori (sconosciuti) dei parametri $\theta$ di un modello statistico alla luce di nuovi dati $y_1, y_2, \dots, y_N$ che vengono osservati. L'approccio bayesiano è etichettato come "soggettivo" perché non ci dice quale valore dovrebbe essere assegnato ai parametri prima di avere osservato i dati $y_1, y_2, \dots, y_N$. In realtà, l'aggiornamento bayesiano è il modo più razionale di procedere: se, prima di avere osservato i dati, il ricercatore ha una credenza assurda relativamente al valore di $\theta$, dopo avere osservato $y_1, y_2, \dots, y_N$ le sue credenze _a posteriori_ su $\theta$, aggiornate secondo i principi bayesiani, saranno meno assurde. Il problema di questo modo di procedere non è che, a priori, i ricercatori (o chiunque altro) possono avere delle credenze sbagliate, ma bensì il fatto che, avendo osservato $y_1, y_2, \dots, y_N$, le credenze su $\theta$ non vengono aggiornate secondo i principi bayesiani. Infatti, in alcune situazioni, l'osservazione di dati che contraddicono le credenze pregresse non fa altro che rafforzare tali convinzioni errate -- il problema, dunque, non nasce dalla "soggettività" dell'approccio bayesiano, ma quanto dal fatto di non seguire un tale modo di procedere! --&gt;</span></span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Lo scopo delle prossime sezioni della dispensa è quello di introdurre quei concetti base della teoria delle probabilità che risultano necessari per una presentazione delle procedure dell'inferenza induttiva bayesiana. --&gt;</span></span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a><span class="fu">## Che cos'è la probabilità?</span></span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a>La definizione della probabilità è un problema estremamente dibattuto ed aperto. Sono state fornite due possibili soluzioni al problema di definire il concetto di probabilità.</span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a>(a) La natura della probabilità è "ontologica" (ovvero, basata sulla metafisica): la probabilità è una proprietà della della realtà, del mondo, di come sono le cose, indipendentemente dalla nostra esperienza. È una visione che qualcuno chiama "oggettiva".</span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a>(b) La natura della probabilità è "epistemica" (ovvero, basata sulla conoscenza): la probabilità si riferisce alla conoscenza che abbiamo del mondo, non al mondo in sé. Di conseguenza è detta, in contrapposizione alla precedente definizione, "soggettiva".</span>
<span id="cb13-80"><a href="#cb13-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-81"><a href="#cb13-81" aria-hidden="true" tabindex="-1"></a>In termini epistemici, la probabilità fornisce una misura della nostra incertezza sul verificarsi di un evento, alla luce delle informazioni disponibili. Potremmo dire che c'è una "scala" naturale che ha per estremi il vero (1: evento certo), da una parte, ed il falso (0: evento impossibile), dall'altra. La probabilità è la quantificazione di questa scala: descrive lo stato della nostra incertezza rispetto al contenuto di verità di una proposizione.</span>
<span id="cb13-82"><a href="#cb13-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-83"><a href="#cb13-83" aria-hidden="true" tabindex="-1"></a>L'incertezza nelle nostre previsioni può sorgere per due ragioni fondamentalmente diverse. La prima è dovuta alla nostra ignoranza relativamente alle cause nascoste sottostanti o dei meccanismi che generano i dati. Questa è, appunto, un'incertezza *epistemica*. Il secondo tipo di incertezza deriva invece dalla variabilità intrinseca dei fenomeni, che non può essere ridotta anche se raccogliamo più dati. Questa seconda forma di incertezza è talvolta chiamata *aleatoria*. Come esempio concreto, consideriamo il lancio di una moneta equilibrata. Sappiamo con certezza che la probabilità di testa è $P = 0.5$, quindi non c'è incertezza epistemica, ma non questo non è sufficiente per prevedere con certezza il risultato -- in altre parole, l'incertezza aleatoria persiste anche in assenza di incertezza epistemica.</span>
<span id="cb13-84"><a href="#cb13-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-85"><a href="#cb13-85" aria-hidden="true" tabindex="-1"></a>L'interpretazione bayesiana di probabilità si contrappone all'interpretazione frequentista. Nell'interpretazione frequentista, la probabilità $P(E)$ rappresenta la frequenza relativa a lungo termine di un grande numero di ripetizioni di un esperimento casuale sotto le medesime condizioni. Viene stressata qui l'idea che ciò di cui parliamo è qualcosa che emerge nel momento in cui è possibile ripetere l'esperimento casuale tante volte sotto le medesime condizioni -- sono invece esclusi gli eventi unici e irripetibili.</span>
<span id="cb13-86"><a href="#cb13-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-87"><a href="#cb13-87" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - Nell'interpretazione bayesiana della probabilità, $P(A)$ rappresenta il grado di fiducia che si ha relativamente al verificarsi dell'evento $A$. --&gt;</span></span>
<span id="cb13-88"><a href="#cb13-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-89"><a href="#cb13-89" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Possiamo citare De Finetti, ad esempio, il quale ha formulato la seguente definizione "soggettiva" di probabilità la quale risulta applicabile anche ad esperimenti casuali i cui eventi elementari non siano ritenuti ugualmente possibili e che non siano necessariamente ripetibili più volte sotto le stesse condizioni.  --&gt;</span></span>
<span id="cb13-90"><a href="#cb13-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-91"><a href="#cb13-91" aria-hidden="true" tabindex="-1"></a>L'interpretazione bayesiana della probabilità fa invece ricorso ad una concezione più ampia, non legata al solo evento in sé, ma che include anche il soggetto assegnante la funzione di probabilità. In pratica l'assegnazione di probabilità bayesiana viene effettuata dal decisore, in base alle proprie conoscenze a priori integrate con tutto il generico bagaglio culturale personale. In questo modo, la probabilità non sarà obbligatoriamente la stessa per tutti i soggetti, ma variarierà a seconda delle informazioni a disposizione, dell'esperienza personale e soprattutto del punto di vista proprio di ogni decisore ed è dunque assimilabile al "grado di fiducia" -- in inglese *degree of belief* -- di un dato soggetto, in un dato istante e con un dato insieme d'informazioni, circa l'accadere dell'evento $E$.</span>
<span id="cb13-92"><a href="#cb13-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-93"><a href="#cb13-93" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="sc">\[</span><span class="at">N</span><span class="sc">\]</span><span class="at">essuna scienza ci permetterà di dire: il tale fatto accadrà, andrà così e così, perché ciò è conseguenza di tale legge, e tale legge è una verità assoluta, ma tanto meno ci condurrà a concludere scetticamente: la verità assoluta non esiste, e quindi tale fatto può accadere e può non accadere, può andare così e può andare in tutt'altro modo, nulla io ne so. Quel che si potrà dire è questo: io prevedo che il tale fatto avverrà, e avverrà nel tal modo, perché l'esperienza del passato e l'elaborazione scientifica cui il pensiero dell'uomo l'ha sottoposta mi fanno sembrare ragionevole questa previsione" </span><span class="co">[</span><span class="ot">@definetti1931prob</span><span class="co">]</span><span class="at">.</span></span>
<span id="cb13-94"><a href="#cb13-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-95"><a href="#cb13-95" aria-hidden="true" tabindex="-1"></a>::: callout-note</span>
<span id="cb13-96"><a href="#cb13-96" aria-hidden="true" tabindex="-1"></a>La caratterizzazione 'epistemica' della nozione di probabilità può essere chiarita facendo riferimento all'esempio prototipico con il quale la probabilità viene solitamente descritta, ovvero come una frequenza relativa. A questo proposito, McElreath ci chiede di riflettere con maggiore attenzione sul fenomeno costituito da una sequenza di lanci di una moneta, ovvero l'esempio tradizionale con il quale si descrive un evento "aleatorio". Ingenuamente potremmo pensare che un tale fenomeno sia "casuale", nel senso che, all'interno della sequenza, non vi è alcuna informazione negli eventi (lanci) passati che sia utile per prevedere gli eventi futuri. Ma non è così. Il lancio di una moneta è un fenomeno deterministico, regolato dalle leggi fisiche. Infatti, sono state create delle macchine che, applicando la stessa forza ogni volta, sono in grado di ripetere lo stesso esito (testa o croce) in ogni prova. Questo significa che la "casualità" della sequenza di lanci non è una proprietà del fenomeno fisico che vorremmo descrivere (i fenomeni fisici sono sempre deterministici in quanto sono regolati dalle leggi della fisica) ma bensì è epistemica, ovvero riguarda lo stato dell'informazione disponibile all'osservatore.</span>
<span id="cb13-97"><a href="#cb13-97" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-98"><a href="#cb13-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-99"><a href="#cb13-99" aria-hidden="true" tabindex="-1"></a><span class="fu">### Formalizzazione dell'incertezza</span></span>
<span id="cb13-100"><a href="#cb13-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-101"><a href="#cb13-101" aria-hidden="true" tabindex="-1"></a>La caratterizzazione della probabilità quale rappresentazione dell'incertezza epistemica è stata formalizzata in ambito bayesiano da Ramsey e de Finetti. De Finetti riconduce l'assegnazione di probabilità allo scommettere sul verificarsi di un evento: la probabilità di un evento $E$ è la quota $p(E)$ che un individuo reputa di dover pagare ad un banco per ricevere "1" ovvero "0" verificandosi o non verificandosi $E$.</span>
<span id="cb13-102"><a href="#cb13-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-103"><a href="#cb13-103" aria-hidden="true" tabindex="-1"></a>In termini formali, secondo De Finetti, le valutazioni di probabilità degli eventi devono rispondere ai principi di equità e coerenza.</span>
<span id="cb13-104"><a href="#cb13-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-105"><a href="#cb13-105" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Una scommessa risponde al principio di *equità* se il ruolo di banco e giocatore sono scambiabili in ogni momento del gioco e sempre alle stesse condizioni.</span>
<span id="cb13-106"><a href="#cb13-106" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Una scommessa risponde al principio di *coerenza* se non vi sono combinazioni di scommesse che consentano (sia al banco che al giocatore) di realizzare perdite o vincite certe.</span>
<span id="cb13-107"><a href="#cb13-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-108"><a href="#cb13-108" aria-hidden="true" tabindex="-1"></a>L'approccio definettiano dell'impostazione della scommessa si basa dunque sulle assunzioni di razionalità e coerenza del decisore, al quale è fatto esplicito divieto di effettuare scommesse a perdita o guadagno certo. Il decisore, proponendo la scommessa, deve essere disposto a scambiare il posto dello scommettitore con quello del banco.</span>
<span id="cb13-109"><a href="#cb13-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-110"><a href="#cb13-110" aria-hidden="true" tabindex="-1"></a>Il metodo della scommessa, oltre che una definizione, fornisce un mezzo operativo di assegnazione della probabilità. Sulla base di questa definizione operativa, che si può ritenere ragionevolmente soddisfatta dal comportamento di un qualunque individuo che agisca in modo razionale in condizioni di incertezza, possono essere agevolmente dimostrate tutte le proprietà classiche della probabilità: essa non può assumere valori negativi, né può essere superiore all'unità; se $E$ è un evento certo, la sua probabilità è 1; se invece $E$ è un evento impossibile, la sua probabilità è 0.</span>
<span id="cb13-111"><a href="#cb13-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-112"><a href="#cb13-112" aria-hidden="true" tabindex="-1"></a>I problemi posti dall'approccio definettiano riguardano l'arbitrarietà dell'assegnazione soggettività di probabilità la quale sembra negare la validità dell'intero costrutto teorico. In risposta a tale critica, i bayesiani sostengono che gli approcci oggettivisti alla probabilità nascondono scelte arbitrarie preliminari e sono basate su assunzioni implausibili. È molto più onesto esplicitare subito tutte le scelte arbitrarie effettuate nel corso dell'analisi in modo da controllarne coerenza e razionalità.</span>
<span id="cb13-113"><a href="#cb13-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-114"><a href="#cb13-114" aria-hidden="true" tabindex="-1"></a>::: callout-note</span>
<span id="cb13-115"><a href="#cb13-115" aria-hidden="true" tabindex="-1"></a>Per chi desidera approfondire, un'introduzione molto leggibile alle tematiche della definizione della probabilità nella storia della scienza è fornita nel primo capitolo del testo *Bernoulli's fallacy* <span class="co">[</span><span class="ot">@clayton2021bernoulli</span><span class="co">]</span>.</span>
<span id="cb13-116"><a href="#cb13-116" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-117"><a href="#cb13-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-118"><a href="#cb13-118" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- In altri termini, de Finetti ritiene che la probabilità debba essere concepita non come una proprietà "oggettiva" dei fenomeni, ma bensì come il "grado di fiducia" -- in inglese _degree of belief_ -- di un dato soggetto, in un dato istante e con un dato insieme d'informazioni, riguardo al verificarsi di un evento. Per denotare sia la probabilità (soggettiva) di un evento sia il concetto di _valore atteso_ (che descriveremo in seguito), @definetti1970teoria utilizza il termine "previsione" (e lo stesso simbolo $P$): *"la previsione [$\dots$] consiste nel considerare ponderatamente tutte le alternative possibili per ripartire fra di esse nel modo che parrà più appropriato le proprie aspettative, le proprie sensazioni di probabilità."* --&gt;</span></span>
<span id="cb13-119"><a href="#cb13-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-120"><a href="#cb13-120" aria-hidden="true" tabindex="-1"></a><span class="fu">## Variabili casuali e probabilità di un evento</span></span>
<span id="cb13-121"><a href="#cb13-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-122"><a href="#cb13-122" aria-hidden="true" tabindex="-1"></a>Esaminiamo qui di seguito alcuni concetti di base della teoria delle probabilità, la quale può essere intesa come un'estensione della logica.</span>
<span id="cb13-123"><a href="#cb13-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-124"><a href="#cb13-124" aria-hidden="true" tabindex="-1"></a><span class="fu">### Eventi e probabilità</span></span>
<span id="cb13-125"><a href="#cb13-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-126"><a href="#cb13-126" aria-hidden="true" tabindex="-1"></a>Nella teoria delle probabilità il risultato "testa" nel lancio di una moneta è chiamato *evento*.<span class="ot">[^015_prob_intro-1]</span> Un evento, denotato da una variabile binaria, corrisponde ad uno stato del mondo che si verifica oppure no. Ad esempio, $Y$ = 1 può denotare l'evento per cui il lancio di una moneta produce il risultato testa. Il funzionale $P(Y)$ denota la probabilità con cui si ritiene che l'evento $Y$ sia vero (o la proporzione di volte che si verifica tale evento osservando a lungo termine delle ripetizioni indipendenti di un esperimento casuale). Ad esempio, per il lancio di una moneta equilibrata, la probabilità dell'evento "il risultato del lancio della moneta è testa" è scritta come $P(Y = 1) = 0.5.$</span>
<span id="cb13-127"><a href="#cb13-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-128"><a href="#cb13-128" aria-hidden="true" tabindex="-1"></a><span class="ot">[^015_prob_intro-1]: </span>Per un ripasso delle nozioni di base della teoria degli insiemi, si veda @sec-appendix-sets.</span>
<span id="cb13-129"><a href="#cb13-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-130"><a href="#cb13-130" aria-hidden="true" tabindex="-1"></a>Se la moneta è equilibrata dobbiamo anche avere $P(Y = 0) = 0.5$. I due eventi $Y$ = 1 e $Y$ = 0 sono *mutuamente esclusivi* nel senso che non possono entrambi verificarsi contemporaneamente: $P(Y = 1\; \land \; Y = 0) = 0.$ Gli eventi $Y$ = 1 e $Y$ = 0 di dicono *esaustivi*, nel senso che almeno uno di essi deve verificarsi e nessun altro tipo di evento è possibile. Nella notazione probabilistica, $P(Y = 1\; \lor \; Y = 0) = 1.$ Il connettivo logico "o" ($\lor$) specifica eventi *disgiunti*, ovvero eventi che non possono verificarsi contemporaneamente (eventi *incompatibili*) e per i quali, perciò, la probabilità della loro congiunzione è $P(A \; \land \; B) = 0$. Il connettivo logico "e" ($\land$), invece, specifica eventi *congiunti*, ovvero eventi che possono verificarsi contemporaneamente (eventi *compatibili*) e per i quali, perciò, la probabilità della loro congiunzione è $P(A \; \land \; B) &gt; 0$. La probabilità del verificarsi di due eventi congiunti $A$ e $B$ si può denotare, in maniera equivalente, con la notazione precedente, oppure con $P(A \cap B)$, oppure con $P(A, B)$.</span>
<span id="cb13-131"><a href="#cb13-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-132"><a href="#cb13-132" aria-hidden="true" tabindex="-1"></a>Si richiede che $0 \leq P(A) \leq 1$, dove $P(A) = 0$ denota l'evento impossibile e $P(A) = 1$ denota l'evento certo. Scriviamo $P(\lnot A)$ o $P(\bar{A})$ per denotare la probabilità che l'evento $A$ non avvenga; questa probabilità è definita come $P(\bar{A}) = 1 − P(A)$.</span>
<span id="cb13-133"><a href="#cb13-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-134"><a href="#cb13-134" aria-hidden="true" tabindex="-1"></a><span class="fu">### Spazio campione e risultati possibili</span></span>
<span id="cb13-135"><a href="#cb13-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-136"><a href="#cb13-136" aria-hidden="true" tabindex="-1"></a>Anche se il lancio di una moneta produce sempre uno specifico risultato nel mondo reale, possiamo anche immaginare i possibili risultati alternativi che si sarebbero potuti osservare. Quindi, anche se in uno specifico lancio la moneta dà testa ($Y$ = 1), possiamo immaginare la possibilità che il lancio possa avere prodotto croce ($Y$ = 0). Tale ragionamento controfattuale è la chiave per comprendere la teoria delle probabilità e l'inferenza statistica.</span>
<span id="cb13-137"><a href="#cb13-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-138"><a href="#cb13-138" aria-hidden="true" tabindex="-1"></a>I risultati possibili che si possono osservare come conseguenza del lancio di una moneta determinano i valori possibili che la variabile casuale può assumere. L'insieme $\Omega$ di tutti i risultati possibili è chiamato *spazio campione* (*sample space*). Lo spazio campione può essere concettualizzato come un'urna contenente una pallina per ogni possibile risultato del lancio della moneta. Su ogni pallina è scritto il valore della variabile casuale. Uno specifico lancio di una moneta -- ovvero, l'osservazione di uno specifico valore di una variabile casuale -- è chiamato *esperimento casuale*.</span>
<span id="cb13-139"><a href="#cb13-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-140"><a href="#cb13-140" aria-hidden="true" tabindex="-1"></a>Il lancio di un dado ci fornisce l'esempio di un altro esperimento casuale. Supponiamo di essere interessati all'evento "il lancio del dado produce un numero dispari". Un *evento* seleziona un sottoinsieme dello spazio campione: in questo caso, l'insieme dei risultati $<span class="sc">\{</span>1, 3, 5<span class="sc">\}</span>$. Se esce 3, per esempio, diciamo che si è verificato l'evento "dispari" (ma l'evento "dispari" si sarebbe anche verificato anche se fosse uscito 1 o 5).</span>
<span id="cb13-141"><a href="#cb13-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-142"><a href="#cb13-142" aria-hidden="true" tabindex="-1"></a><span class="fu">## Distribuzione di probabilità</span></span>
<span id="cb13-143"><a href="#cb13-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-144"><a href="#cb13-144" aria-hidden="true" tabindex="-1"></a>Sia $Y$ il risultato del lancio di moneta equilibrata; non di un generico lancio di una moneta, ma un'istanza specifica del lancio di una specifica moneta in un dato momento. Definita in questo modo, $Y$ è una *variabile casuale*, ovvero una variabile i cui valori non possono essere previsti con esattezza. Se la moneta è equilibrata, c'è una probabilità del 50% che il lancio della moneta dia come risultato "testa" e una probabilità del 50% che dia come risultato "croce". Per facilitare la trattazione, le variabili casuali assumono solo valori numerici. Per lo specifico lancio della moneta in questione, diciamo, ad esempio, che la variabile casuale $Y$ assume il valore 1 se esce testa e il valore 0 se esce croce.</span>
<span id="cb13-145"><a href="#cb13-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-146"><a href="#cb13-146" aria-hidden="true" tabindex="-1"></a>Una variabile casuale può essere *discreta* o *continua*. Una variabile casuale discreta può assumere un numero finito di valori $x_1, \dots ,x_n$, in corrispondenza degli eventi $E_i, \dots, E_n$ che si verificano con le rispettive probabilità $p_1, \dots, p_n$. Un esempio è il punteggio totale di un test psicometrico costituito da item su scala Likert. Invece un esempio di una variabile casuale continua è la distanza tra due punti, che può assumere infiniti valori all'interno di un certo intervallo. L'insieme $S$ dei valori che la variabile casuale può assumere è detto *spazio dei valori* o *spazio degli stati*.</span>
<span id="cb13-147"><a href="#cb13-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-148"><a href="#cb13-148" aria-hidden="true" tabindex="-1"></a>La caratteristica fondamentale di una variabile casuale è data dall'insieme delle probabilità dei suoi valori, detta *distribuzione di probabilità*. Nel seguito useremo la notazione $P(\cdot)$ per fare riferimento alle distribuzioni di probabilità delle variabili casuali discrete e $p(\cdot)$ per fare riferimento alla densità di probabilità delle variabili casuali continue. In questo contesto, l'insieme dei valori che la variabile casuale può assumere è detto *supporto* della sua distribuzione di probabilità. Il supporto di una variabile casuale può essere finito (come nel caso di una variabile casuale uniforme di supporto $<span class="co">[</span><span class="ot">a, b</span><span class="co">]</span>$) o infinito (nel caso di una variabile causale gaussiana il cui supporto coincide con la retta reale).</span>
<span id="cb13-149"><a href="#cb13-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-150"><a href="#cb13-150" aria-hidden="true" tabindex="-1"></a><span class="fu">## Usare la simulazione per stimare le probabilità</span></span>
<span id="cb13-151"><a href="#cb13-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-152"><a href="#cb13-152" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- I metodi basati sulla simulazione consentono di stimare le probabilità degli eventi in un modo diretto, se siamo in grado di generare molteplici e casuali realizzazioni delle variabili casuali coinvolte nelle definizioni degli eventi. --&gt;</span></span>
<span id="cb13-153"><a href="#cb13-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-154"><a href="#cb13-154" aria-hidden="true" tabindex="-1"></a>In questa dispensa verrà adottata l'interpretazione bayesiana delle probabilità. Tuttavia, le regole di base della teoria delle probabilità sono le stesse, indipendentemente dall'interpretazione adottata. Pertanto, negli esempi seguenti, possiamo utilizzare la simulazione per stimare le probabilità degli eventi in un modo diretto, ovvero mediante la generazione di molteplici osservazioni delle variabili casuali derivate dagli eventi di interesse.</span>
<span id="cb13-155"><a href="#cb13-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-156"><a href="#cb13-156" aria-hidden="true" tabindex="-1"></a>Simuliamo il lancio di una moneta equilibrata.</span>
<span id="cb13-157"><a href="#cb13-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-158"><a href="#cb13-158" aria-hidden="true" tabindex="-1"></a>Definiamo una variabile casuale bernoulliana $X$ con $\pi$ (probabilità di successo) uguale a 0.5.</span>
<span id="cb13-159"><a href="#cb13-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-162"><a href="#cb13-162" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-163"><a href="#cb13-163" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb13-164"><a href="#cb13-164" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> bernoulli(p)</span>
<span id="cb13-165"><a href="#cb13-165" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-166"><a href="#cb13-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-167"><a href="#cb13-167" aria-hidden="true" tabindex="-1"></a>Esaminiamo 20 realizzazioni di $X$.</span>
<span id="cb13-168"><a href="#cb13-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-171"><a href="#cb13-171" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-172"><a href="#cb13-172" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> X.rvs(<span class="dv">20</span>)</span>
<span id="cb13-173"><a href="#cb13-173" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x)</span>
<span id="cb13-174"><a href="#cb13-174" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-175"><a href="#cb13-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-176"><a href="#cb13-176" aria-hidden="true" tabindex="-1"></a>La stima della probabilità dell'evento $P(Y = 1)$ è data dalla frequenza relativa del numero di volte in cui abbiamo osservato l'evento di interesse ($Y = 1$).</span>
<span id="cb13-177"><a href="#cb13-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-180"><a href="#cb13-180" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-181"><a href="#cb13-181" aria-hidden="true" tabindex="-1"></a>np.mean(x)</span>
<span id="cb13-182"><a href="#cb13-182" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-183"><a href="#cb13-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-184"><a href="#cb13-184" aria-hidden="true" tabindex="-1"></a>Esaminiamo ora una simulazione nella quale facciamo variare l'ampiezza del campione usato per la stima empirica della probabilità. Nella simulazione vengono generati 10,000 campioni di ampiezza $n$. Di ciascuno di questi campioni casuali viene calcolata la media. Le 10,000 stime empiriche di $\pi$ vengono poi visualizzate con un istogramma.</span>
<span id="cb13-185"><a href="#cb13-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-188"><a href="#cb13-188" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-189"><a href="#cb13-189" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cointoss(p<span class="op">=</span><span class="fl">0.5</span>, size<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb13-190"><a href="#cb13-190" aria-hidden="true" tabindex="-1"></a>    niter <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb13-191"><a href="#cb13-191" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> bernoulli(p)</span>
<span id="cb13-192"><a href="#cb13-192" aria-hidden="true" tabindex="-1"></a>    random_samples <span class="op">=</span> []</span>
<span id="cb13-193"><a href="#cb13-193" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(niter):</span>
<span id="cb13-194"><a href="#cb13-194" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> X.rvs(size)</span>
<span id="cb13-195"><a href="#cb13-195" aria-hidden="true" tabindex="-1"></a>        random_samples.append(x)</span>
<span id="cb13-196"><a href="#cb13-196" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-197"><a href="#cb13-197" aria-hidden="true" tabindex="-1"></a>    rs <span class="op">=</span> np.array(random_samples)</span>
<span id="cb13-198"><a href="#cb13-198" aria-hidden="true" tabindex="-1"></a>    p_estimates <span class="op">=</span> np.mean(rs, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-199"><a href="#cb13-199" aria-hidden="true" tabindex="-1"></a>    sns.histplot(p_estimates, stat <span class="op">=</span> <span class="st">"density"</span>)<span class="op">;</span></span>
<span id="cb13-200"><a href="#cb13-200" aria-hidden="true" tabindex="-1"></a>    plt.xlim([<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb13-201"><a href="#cb13-201" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb13-202"><a href="#cb13-202" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-203"><a href="#cb13-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-204"><a href="#cb13-204" aria-hidden="true" tabindex="-1"></a>Iniziamo con campioni di ampiezza $n$ = 10.</span>
<span id="cb13-205"><a href="#cb13-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-208"><a href="#cb13-208" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-209"><a href="#cb13-209" aria-hidden="true" tabindex="-1"></a>cointoss(size<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb13-210"><a href="#cb13-210" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-211"><a href="#cb13-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-212"><a href="#cb13-212" aria-hidden="true" tabindex="-1"></a>Dato che la moneta è equilibrata, le stime empiriche della probabilità dell'evento $P(Y = 1)$ sono uguali, *in media*, al valore che ci aspettiamo, ovvero $P(Y = 1) = 0.5$, ma il risultato ottenuto varia di molto da campione a campione. Proviamo ora ad aumentare il numero di lanci in ciascun campione: $n$ = 100.</span>
<span id="cb13-213"><a href="#cb13-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-216"><a href="#cb13-216" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-217"><a href="#cb13-217" aria-hidden="true" tabindex="-1"></a>cointoss(size<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb13-218"><a href="#cb13-218" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-219"><a href="#cb13-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-220"><a href="#cb13-220" aria-hidden="true" tabindex="-1"></a>In questo secondo caso, gli errori tendono ad essere più piccoli che nel caso precedente. Usiamo ora 1000 lanci in ciascun campione.</span>
<span id="cb13-221"><a href="#cb13-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-224"><a href="#cb13-224" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-225"><a href="#cb13-225" aria-hidden="true" tabindex="-1"></a>cointoss(size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb13-226"><a href="#cb13-226" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-227"><a href="#cb13-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-228"><a href="#cb13-228" aria-hidden="true" tabindex="-1"></a>Ora le stime empiriche che otteniamo in ciascun campione sono tutte molto vicine alla probabilità vera (cioè 0.5, perché la moneta è equilibrata).</span>
<span id="cb13-229"><a href="#cb13-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-230"><a href="#cb13-230" aria-hidden="true" tabindex="-1"></a>I risultati delle simulazioni precedenti pongono dunque il problema di determinare quale sia il numero di lanci di cui abbiamo bisogno per assicurarci che le stime empiriche di $\pi$ siano accurate (ovvero, vicine al valore corretto della probabilità teorica).</span>
<span id="cb13-231"><a href="#cb13-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-232"><a href="#cb13-232" aria-hidden="true" tabindex="-1"></a><span class="fu">## La legge dei grandi numeri</span></span>
<span id="cb13-233"><a href="#cb13-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-234"><a href="#cb13-234" aria-hidden="true" tabindex="-1"></a>La visualizzazione mediante grafici contribuisce alla comprensione dei concetti della statistica e della teoria delle probabilità. Un modo per descrivere ciò che accade all'aumentare del numero $M$ di ripetizioni del lancio della moneta consiste nel registrare la stima della probabilità dell'evento $P(Y = 1)$ in funzione del numero di ripetizioni dell'esperimento casuale per ogni $m \in 1:M$. Possiamo ottenere un grafico dell'andamento della stima di $P(Y = 1)$ in funzione di $m$ nel modo seguente:</span>
<span id="cb13-235"><a href="#cb13-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-236"><a href="#cb13-236" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo=FALSE, fig-legge-grandi-n-1, cache=TRUE, fig.cap="Stima della probabilità di successo in funzione del numero dei lanci di una moneta."}</span></span>
<span id="cb13-237"><a href="#cb13-237" aria-hidden="true" tabindex="-1"></a>nrep <span class="ot">&lt;-</span> <span class="fl">1e4</span></span>
<span id="cb13-238"><a href="#cb13-238" aria-hidden="true" tabindex="-1"></a>estimate <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, nrep)</span>
<span id="cb13-239"><a href="#cb13-239" aria-hidden="true" tabindex="-1"></a>flip_coin <span class="ot">&lt;-</span> <span class="cf">function</span>(m) {</span>
<span id="cb13-240"><a href="#cb13-240" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(m, <span class="dv">1</span>, <span class="fl">0.5</span>)</span>
<span id="cb13-241"><a href="#cb13-241" aria-hidden="true" tabindex="-1"></a>  phat <span class="ot">&lt;-</span> <span class="fu">sum</span>(y) <span class="sc">/</span> m</span>
<span id="cb13-242"><a href="#cb13-242" aria-hidden="true" tabindex="-1"></a>  phat</span>
<span id="cb13-243"><a href="#cb13-243" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-244"><a href="#cb13-244" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nrep) {</span>
<span id="cb13-245"><a href="#cb13-245" aria-hidden="true" tabindex="-1"></a>  estimate[i] <span class="ot">&lt;-</span> <span class="fu">flip_coin</span>(i)</span>
<span id="cb13-246"><a href="#cb13-246" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-247"><a href="#cb13-247" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb13-248"><a href="#cb13-248" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> <span class="dv">1</span><span class="sc">:</span>nrep, </span>
<span id="cb13-249"><a href="#cb13-249" aria-hidden="true" tabindex="-1"></a>  estimate</span>
<span id="cb13-250"><a href="#cb13-250" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-251"><a href="#cb13-251" aria-hidden="true" tabindex="-1"></a>d <span class="sc">%&gt;%</span> </span>
<span id="cb13-252"><a href="#cb13-252" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> n, <span class="at">y =</span> estimate)) <span class="sc">+</span></span>
<span id="cb13-253"><a href="#cb13-253" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb13-254"><a href="#cb13-254" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb13-255"><a href="#cb13-255" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Numero di lanci della moneta"</span>, </span>
<span id="cb13-256"><a href="#cb13-256" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Stima di P(Y = 1)"</span></span>
<span id="cb13-257"><a href="#cb13-257" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-258"><a href="#cb13-258" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-259"><a href="#cb13-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-260"><a href="#cb13-260" aria-hidden="true" tabindex="-1"></a>La @fig-legge-grandi-n-1, quando è espressa su una scala lineare, non rivela chiaramente l'andamento della simulazione. Imponiamo dunque una scala logaritmica sull'asse delle ascisse ($x$). Su scala logaritmica, i valori tra 1 e 10 vengono tracciati all'incirca con la stessa ampiezza che si osserva tra i valori 50 e 700, eccetera.</span>
<span id="cb13-261"><a href="#cb13-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-262"><a href="#cb13-262" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo=FALSE, fig-legge-grandi-n-2, fig.cap="Stima della probabilità di successo in funzione del numero dei lanci di una moneta."}</span></span>
<span id="cb13-263"><a href="#cb13-263" aria-hidden="true" tabindex="-1"></a>d <span class="sc">%&gt;%</span></span>
<span id="cb13-264"><a href="#cb13-264" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> n, <span class="at">y =</span> estimate)) <span class="sc">+</span></span>
<span id="cb13-265"><a href="#cb13-265" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb13-266"><a href="#cb13-266" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(</span>
<span id="cb13-267"><a href="#cb13-267" aria-hidden="true" tabindex="-1"></a>    <span class="at">yintercept =</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">"gray"</span>, <span class="at">size =</span> <span class="dv">1</span></span>
<span id="cb13-268"><a href="#cb13-268" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb13-269"><a href="#cb13-269" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>(</span>
<span id="cb13-270"><a href="#cb13-270" aria-hidden="true" tabindex="-1"></a>    <span class="at">breaks =</span> <span class="fu">c</span>(</span>
<span id="cb13-271"><a href="#cb13-271" aria-hidden="true" tabindex="-1"></a>      <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">200</span>,</span>
<span id="cb13-272"><a href="#cb13-272" aria-hidden="true" tabindex="-1"></a>      <span class="dv">700</span>, <span class="dv">2500</span>, <span class="dv">10000</span></span>
<span id="cb13-273"><a href="#cb13-273" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb13-274"><a href="#cb13-274" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb13-275"><a href="#cb13-275" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb13-276"><a href="#cb13-276" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Numero dei lanci della moneta (scala logaritmica)"</span>,</span>
<span id="cb13-277"><a href="#cb13-277" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Stima di P(Y = 1)"</span></span>
<span id="cb13-278"><a href="#cb13-278" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb13-279"><a href="#cb13-279" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-280"><a href="#cb13-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-281"><a href="#cb13-281" aria-hidden="true" tabindex="-1"></a>La @fig-legge-grandi-n-2 fornisce una rappresentazione grafica della legge dei grandi numeri. La *legge dei grandi numeri* dice che, all'aumentare del numero di ripetizioni dell'esperimento casuale, la media dei risultati ottenuti tende al valore atteso, man mano che vengono eseguite più prove. Nella figura @fig-legge-grandi-n-2 vediamo infatti che, all'aumentare del numero *M* di lanci della moneta, la stima di $P(Y = 1)$ converge al valore 0.5.</span>
<span id="cb13-282"><a href="#cb13-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-283"><a href="#cb13-283" aria-hidden="true" tabindex="-1"></a><span class="fu">## Variabili casuali multiple</span></span>
<span id="cb13-284"><a href="#cb13-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-285"><a href="#cb13-285" aria-hidden="true" tabindex="-1"></a>Le variabili casuali non esistono isolatamente. Abbiamo iniziato con una sola variabile casuale $Y$ che rappresenta il risultato di un singolo, specifico lancio di una moneta equlibrata. Ma supponiamo ora di lanciare la moneta tre volte. I risultati di ciascuno dei tre lanci possono essere rappresentati da una diversa variabile casuale, ad esempio, $Y_1 , Y_2 , Y_3$. Possiamo assumere che ogni lancio sia indipendente, ovvero che non dipenda dal risultato degli altri lanci. Per ciascuna di queste variabili $Y_n$, con $n \in 1:3$, abbiamo che $P(Y_n =1)=0.5$ e $P(Y_n =0)=0.5$.</span>
<span id="cb13-286"><a href="#cb13-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-287"><a href="#cb13-287" aria-hidden="true" tabindex="-1"></a>È possibile combinare più variabili casuali usando le operazioni aritmetiche. Se $Y_1 , Y_2, Y_3$ sono variabili casuali che rappresentano tre lanci di una moneta equilibrata (o, in maniera equivalente, un lancio di tre monete equilibrate), possiamo definire la somma di tali variabili casuali come</span>
<span id="cb13-288"><a href="#cb13-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-289"><a href="#cb13-289" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-290"><a href="#cb13-290" aria-hidden="true" tabindex="-1"></a>Z = Y_1 + Y_2 + Y_3.</span>
<span id="cb13-291"><a href="#cb13-291" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-292"><a href="#cb13-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-293"><a href="#cb13-293" aria-hidden="true" tabindex="-1"></a>Possiamo simulare i valori assunti dalla variabile casuale $Z$ simulando i valori di $Y_1, Y_2, Y_3$ per poi sommarli.</span>
<span id="cb13-294"><a href="#cb13-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-297"><a href="#cb13-297" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-298"><a href="#cb13-298" aria-hidden="true" tabindex="-1"></a>size <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb13-299"><a href="#cb13-299" aria-hidden="true" tabindex="-1"></a>np.<span class="bu">sum</span>(X.rvs(size))</span>
<span id="cb13-300"><a href="#cb13-300" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-301"><a href="#cb13-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-302"><a href="#cb13-302" aria-hidden="true" tabindex="-1"></a>Ripetiamo questa simulazione $M$ = 10,000 volte.</span>
<span id="cb13-303"><a href="#cb13-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-306"><a href="#cb13-306" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-307"><a href="#cb13-307" aria-hidden="true" tabindex="-1"></a>size <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb13-308"><a href="#cb13-308" aria-hidden="true" tabindex="-1"></a>niter <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb13-309"><a href="#cb13-309" aria-hidden="true" tabindex="-1"></a>random_samples <span class="op">=</span> [X.rvs(size) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(niter)]</span>
<span id="cb13-310"><a href="#cb13-310" aria-hidden="true" tabindex="-1"></a>rs <span class="op">=</span> np.array(random_samples)</span>
<span id="cb13-311"><a href="#cb13-311" aria-hidden="true" tabindex="-1"></a>p_estimates <span class="op">=</span> np.mean(rs, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-312"><a href="#cb13-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-313"><a href="#cb13-313" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(p_estimates)</span>
<span id="cb13-314"><a href="#cb13-314" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-315"><a href="#cb13-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-316"><a href="#cb13-316" aria-hidden="true" tabindex="-1"></a>e calcoliamo poi una stima della probabilità che la variabile casuale $Z$ assuma ciascuno dei possibili valori 0, 1, 2, 3. Nel caso di 4 monete equilibrate, abbiamo:</span>
<span id="cb13-317"><a href="#cb13-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-320"><a href="#cb13-320" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-321"><a href="#cb13-321" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(p_estimates, columns<span class="op">=</span>[<span class="st">'p_est'</span>])</span>
<span id="cb13-322"><a href="#cb13-322" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span>
<span id="cb13-323"><a href="#cb13-323" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-324"><a href="#cb13-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-327"><a href="#cb13-327" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-328"><a href="#cb13-328" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'p_est'</span>].value_counts() <span class="op">/</span> niter</span>
<span id="cb13-329"><a href="#cb13-329" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-330"><a href="#cb13-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-331"><a href="#cb13-331" aria-hidden="true" tabindex="-1"></a>Una variabile casuale le cui modalità possono essere costituite solo da numeri interi è detta *variabile casuale discreta*:</span>
<span id="cb13-332"><a href="#cb13-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-333"><a href="#cb13-333" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-334"><a href="#cb13-334" aria-hidden="true" tabindex="-1"></a>\mathbb{Z} = \dots, -2, -1, 0, 1, 2, \dots</span>
<span id="cb13-335"><a href="#cb13-335" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-336"><a href="#cb13-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-337"><a href="#cb13-337" aria-hidden="true" tabindex="-1"></a><span class="fu">## Funzione di massa di probabilità {#sec-fun-mass-prob}</span></span>
<span id="cb13-338"><a href="#cb13-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-339"><a href="#cb13-339" aria-hidden="true" tabindex="-1"></a>È conveniente avere una funzione che associa una probabilità a ciascun possibile valore di una variabile casuale. In generale, ciò è possibile se e solo se la variabile casuale è discreta, così com'è stata definita nel paragrafo precedente. Ad esempio, se consideriamo $Z = Y_1 + \dots + Y_4$ come, ad esempio, il numero di risultati "testa" in 4 lanci della moneta, allora possiamo definire la seguente funzione:</span>
<span id="cb13-340"><a href="#cb13-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-341"><a href="#cb13-341" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-342"><a href="#cb13-342" aria-hidden="true" tabindex="-1"></a>\begin{array}{rclll}</span>
<span id="cb13-343"><a href="#cb13-343" aria-hidden="true" tabindex="-1"></a>p_Z(0) &amp; = &amp; 1/16 &amp; &amp; \mathrm{TTTT}</span>
<span id="cb13-344"><a href="#cb13-344" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span></span>
<span id="cb13-345"><a href="#cb13-345" aria-hidden="true" tabindex="-1"></a>p_Z(1) &amp; = &amp; 4/16 &amp; &amp; \mathrm{HTTT, THTT, TTHT, TTTH}</span>
<span id="cb13-346"><a href="#cb13-346" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span></span>
<span id="cb13-347"><a href="#cb13-347" aria-hidden="true" tabindex="-1"></a>p_Z(2) &amp; = &amp; 6/16 &amp; &amp; \mathrm{HHTT, HTHT, HTTH, THHT, THTH, TTTH}</span>
<span id="cb13-348"><a href="#cb13-348" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span></span>
<span id="cb13-349"><a href="#cb13-349" aria-hidden="true" tabindex="-1"></a>p_Z(3) &amp; = &amp; 4/16 &amp; &amp; \mathrm{HHHT, HHTH, HTHH, THHH}</span>
<span id="cb13-350"><a href="#cb13-350" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span></span>
<span id="cb13-351"><a href="#cb13-351" aria-hidden="true" tabindex="-1"></a>p_Z(4) &amp; = &amp; 1/16 &amp; &amp; \mathrm{HHHH}</span>
<span id="cb13-352"><a href="#cb13-352" aria-hidden="true" tabindex="-1"></a>\end{array}</span>
<span id="cb13-353"><a href="#cb13-353" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-354"><a href="#cb13-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-355"><a href="#cb13-355" aria-hidden="true" tabindex="-1"></a>Il lancio di quattro monete può produrre 16 risultati possibili. Dato che i lanci sono indipendenti, se le monete sono equilibrate ogni possibile risultato è ugualmente probabile. Nella tabella precedente, le sequenze dei risultati possibili del lancio delle 4 monete sono riportate nella colonna più a destra. Le probabilità si ottengono dividendo il numero di sequenze che producono lo stesso numero di eventi testa per il numero dei risultati possibili.</span>
<span id="cb13-356"><a href="#cb13-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-357"><a href="#cb13-357" aria-hidden="true" tabindex="-1"></a>Le sequenze come $\mathrm{TTTT}$, $\mathrm{HTTT}$, ecc. sono chiamate "eventi elementari" (ovvero, corrispondono ad un possibile esito dell'esperimento casuale). L'evento $Z = u$, con $u \in 0 \dots, 4$ è un "evento composto", il quale può essere costituito da più eventi elementari.</span>
<span id="cb13-358"><a href="#cb13-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-359"><a href="#cb13-359" aria-hidden="true" tabindex="-1"></a>La funzione $p_Z$ è stata costruita per associare a ciascun valore $u$ della variabile casuale $Z$ la probabilità dell'evento $Z = u$. Convenzionalmente, queste probabilità sono scritte come</span>
<span id="cb13-360"><a href="#cb13-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-361"><a href="#cb13-361" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-362"><a href="#cb13-362" aria-hidden="true" tabindex="-1"></a>P_Z(z) = P(Z = z).</span>
<span id="cb13-363"><a href="#cb13-363" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-364"><a href="#cb13-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-365"><a href="#cb13-365" aria-hidden="true" tabindex="-1"></a>La parte a destra dell'uguale si può leggere come: "la probabilità che la variabile casuale $Z$ assuma il valore $z$". Una funzione definita come sopra è detta *funzione di massa di probabilità* della variabile casuale $Z$. Ad ogni variabile casuale discreta è associata un'unica funzione di massa di probabilità.</span>
<span id="cb13-366"><a href="#cb13-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-367"><a href="#cb13-367" aria-hidden="true" tabindex="-1"></a>Una rappresentazione grafica della stima della funzione di massa di probabilità per l'esperimento casuale del lancio di quattro monete equilibrate è fornita nella @fig-barplot-mdf-4coins.</span>
<span id="cb13-368"><a href="#cb13-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-369"><a href="#cb13-369" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, fig-barplot-mdf-4coins, echo=FALSE, fig.cap="Grafico di $M = 100,000$ simulazioni della funzione di massa di probabilità di una variabile casuale definita come il numero di teste in quattro lanci di una moneta equilibrata."}</span></span>
<span id="cb13-370"><a href="#cb13-370" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb13-371"><a href="#cb13-371" aria-hidden="true" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="fl">1e5</span></span>
<span id="cb13-372"><a href="#cb13-372" aria-hidden="true" tabindex="-1"></a>nflips <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb13-373"><a href="#cb13-373" aria-hidden="true" tabindex="-1"></a>u <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(M, nflips, <span class="fl">0.5</span>)</span>
<span id="cb13-374"><a href="#cb13-374" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span>nflips</span>
<span id="cb13-375"><a href="#cb13-375" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, nflips <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb13-376"><a href="#cb13-376" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">0</span><span class="sc">:</span>nflips) {</span>
<span id="cb13-377"><a href="#cb13-377" aria-hidden="true" tabindex="-1"></a>  y[n <span class="sc">+</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">sum</span>(u <span class="sc">==</span> n) <span class="sc">/</span> M</span>
<span id="cb13-378"><a href="#cb13-378" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-379"><a href="#cb13-379" aria-hidden="true" tabindex="-1"></a>bar_plot <span class="ot">&lt;-</span></span>
<span id="cb13-380"><a href="#cb13-380" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>(<span class="at">Z =</span> x, <span class="at">count =</span> y) <span class="sc">%&gt;%</span></span>
<span id="cb13-381"><a href="#cb13-381" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Z, <span class="at">y =</span> count)) <span class="sc">+</span></span>
<span id="cb13-382"><a href="#cb13-382" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">"identity"</span>) <span class="sc">+</span></span>
<span id="cb13-383"><a href="#cb13-383" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(</span>
<span id="cb13-384"><a href="#cb13-384" aria-hidden="true" tabindex="-1"></a>    <span class="at">breaks =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">4</span>,</span>
<span id="cb13-385"><a href="#cb13-385" aria-hidden="true" tabindex="-1"></a>    <span class="at">labels =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb13-386"><a href="#cb13-386" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb13-387"><a href="#cb13-387" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb13-388"><a href="#cb13-388" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Probabilità stimata P(Z = z)"</span></span>
<span id="cb13-389"><a href="#cb13-389" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb13-390"><a href="#cb13-390" aria-hidden="true" tabindex="-1"></a>bar_plot</span>
<span id="cb13-391"><a href="#cb13-391" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-392"><a href="#cb13-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-393"><a href="#cb13-393" aria-hidden="true" tabindex="-1"></a>Se $A$ è un sottoinsieme della variabile casuale $Z$, allora denotiamo con $P_{z}(A)$ la probabilità assegnata ad $A$ dalla distribuzione $P_{z}$. Mediante una distribuzione di probabilità $P_{z}$ è possibile determinare la probabilità di ciascun sottoinsieme $A \subset Z$ come</span>
<span id="cb13-394"><a href="#cb13-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-395"><a href="#cb13-395" aria-hidden="true" tabindex="-1"></a><span class="in">```{=tex}</span></span>
<span id="cb13-396"><a href="#cb13-396" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{equation}</span></span>
<span id="cb13-397"><a href="#cb13-397" aria-hidden="true" tabindex="-1"></a><span class="in">P_{z}(A) = \sum_{z \in A} P_{z}(Z = z).</span></span>
<span id="cb13-398"><a href="#cb13-398" aria-hidden="true" tabindex="-1"></a><span class="in">\end{equation}</span></span>
<span id="cb13-399"><a href="#cb13-399" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-400"><a href="#cb13-400" aria-hidden="true" tabindex="-1"></a>Una funzione di massa di probabilità soddisfa le proprietà</span>
<span id="cb13-401"><a href="#cb13-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-402"><a href="#cb13-402" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$0 \leq P(X=x) \leq 1$,</span>
<span id="cb13-403"><a href="#cb13-403" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$\sum_{x \in X} P(x) = 1$.</span>
<span id="cb13-404"><a href="#cb13-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-405"><a href="#cb13-405" aria-hidden="true" tabindex="-1"></a>::: {#exr-prob-4-coins-1}</span>
<span id="cb13-406"><a href="#cb13-406" aria-hidden="true" tabindex="-1"></a>Per l'esempio discusso nella @sec-fun-mass-prob, la probabilità che la variabile casuale $Z$ sia un numero dispari è</span>
<span id="cb13-407"><a href="#cb13-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-408"><a href="#cb13-408" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-409"><a href="#cb13-409" aria-hidden="true" tabindex="-1"></a>P(\text{Z è un numero dispari}) = P_{z}(Z = 1) + P_{z}(Z = 3) = \frac{4}{16} + \frac{4}{16} = \frac{1}{2}.</span>
<span id="cb13-410"><a href="#cb13-410" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-411"><a href="#cb13-411" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-412"><a href="#cb13-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-413"><a href="#cb13-413" aria-hidden="true" tabindex="-1"></a><span class="fu">### Funzione di ripartizione</span></span>
<span id="cb13-414"><a href="#cb13-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-415"><a href="#cb13-415" aria-hidden="true" tabindex="-1"></a>Data una variabile casuale discreta $X$ possiamo calcolare la probabilità che $X$ non superi un certo valore $x$, ossia la sua *funzione di ripartizione*. Poichè $X$ assume valori discreti possiamo cumulare le probabilità mediante una somma:</span>
<span id="cb13-416"><a href="#cb13-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-417"><a href="#cb13-417" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-418"><a href="#cb13-418" aria-hidden="true" tabindex="-1"></a>F(x_k) = P(X \leq x_k) = \sum_{x \leq x_k} P(x).</span>
<span id="cb13-419"><a href="#cb13-419" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-420"><a href="#cb13-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-421"><a href="#cb13-421" aria-hidden="true" tabindex="-1"></a>::: {#exr-prob-4-coins-2}</span>
<span id="cb13-422"><a href="#cb13-422" aria-hidden="true" tabindex="-1"></a>Per l'esempio discusso nella @sec-fun-mass-prob, la funzione di ripartizione della variabile casuale $Z$ è fornita nella tabella seguente.</span>
<span id="cb13-423"><a href="#cb13-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-424"><a href="#cb13-424" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo=FALSE}</span></span>
<span id="cb13-425"><a href="#cb13-425" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">4</span></span>
<span id="cb13-426"><a href="#cb13-426" aria-hidden="true" tabindex="-1"></a>pz <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">16</span>, <span class="dv">4</span><span class="sc">/</span><span class="dv">16</span>, <span class="dv">6</span><span class="sc">/</span><span class="dv">16</span>, <span class="dv">4</span><span class="sc">/</span><span class="dv">16</span>, <span class="dv">1</span><span class="sc">/</span><span class="dv">16</span>)</span>
<span id="cb13-427"><a href="#cb13-427" aria-hidden="true" tabindex="-1"></a>cum_prob <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(pz)</span>
<span id="cb13-428"><a href="#cb13-428" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(z, pz, cum_prob) <span class="sc">|&gt;</span> </span>
<span id="cb13-429"><a href="#cb13-429" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kbl</span>() <span class="sc">|&gt;</span> </span>
<span id="cb13-430"><a href="#cb13-430" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_paper</span>(<span class="st">"hover"</span>, <span class="at">full_width =</span> <span class="cn">FALSE</span>) <span class="sc">|&gt;</span> </span>
<span id="cb13-431"><a href="#cb13-431" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">bootstrap_options =</span> <span class="fu">c</span>(<span class="st">"striped"</span>, <span class="st">"hover"</span>, <span class="st">"condensed"</span>))</span>
<span id="cb13-432"><a href="#cb13-432" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-433"><a href="#cb13-433" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-434"><a href="#cb13-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-435"><a href="#cb13-435" aria-hidden="true" tabindex="-1"></a><span class="fu">## Commenti e considerazioni finali {.unnumbered}</span></span>
<span id="cb13-436"><a href="#cb13-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-437"><a href="#cb13-437" aria-hidden="true" tabindex="-1"></a>In questo capitolo abbiamo visto come si costruisce lo spazio campione di un esperimento casuale, quali sono le proprietà di base della probabilità e come si assegnano le probabilità agli eventi definiti sopra uno spazio campione discreto. Abbiamo anche introdotto le nozioni di variabile casuale, ovvero di una variabile che assume i suoi valori in maniera casuale. Abbiamo descritto il modo di specificare la probabilità con cui sono una variabile casuale assume i suoi differenti valori, ovvero la funzione di ripartizione $F(X) = P(X &lt; x)$ e la funzione di massa di probabilità.</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>