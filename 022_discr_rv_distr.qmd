# Distribuzioni di v.c. discrete {#ch-distr-rv-discr}

```{r, include = FALSE}
source("_common.R")
```

In questo Capitolo verranno esaminate le principali distribuzioni di probabilità delle variabili casuali discrete. Un esperimento casuale che può dare luogo a solo due possibili esiti (successo, insuccesso) è modellabile con una variabile casuale di Bernoulli. Una sequenza di prove di Bernoulli costituisce un processo Bernoulliano. Il numero di successi dopo $n$ prove di Bernoulli corrisponde ad una variabile casuale che segue la legge binomiale. La distribuzione binomiale risulta da un insieme di prove di Bernoulli solo se il numero totale $n$ è fisso per disegno. Se il numero di prove è esso stesso una variabile casuale, allora il numero di successi nella corrispondente sequenza di prove bernoulliane segue al distribuzione di Poisson. Concluderemo con la distribuzione discreta uniforme.

## Una prova Bernoulliana

Se un esperimento casuale ha solo due esiti possibili, allora le repliche indipendenti di questo esperimento sono chiamate "prove Bernoulliane" (il lancio di una moneta è il tipico esempio).

::: {#def-bernoulli-var}
Viene detta variabile di Bernoulli una variabile casuale discreta $Y = \{0, 1\}$ con la seguente distribuzione di probabilità:

$$
P(Y \mid \theta) =
  \begin{cases}
    \theta     & \text{se $Y = 1$}, \\
    1 - \theta & \text{se $Y = 0$},
  \end{cases}
$$

con $0 \leq \theta \leq 1$. Convenzionalmente l'evento $\{Y = 1\}$ con probabilità $\theta$ viene chiamato "successo" mentre l'evento $\{Y = 0\}$ con probabilità $1-\theta$ viene chiamato "insuccesso".
:::

Applicando l'operatore di valore atteso e di varianza, otteniamo

$$
\begin{align}
\mathbb{E}(Y) &= 0 \cdot P(Y=0) + 1 \cdot P(Y=1) = \theta, \\
\mathbb{V}(Y) &= (0 - \theta)^2 \cdot P(Y=0) + (1 - \theta)^2 \cdot P(Y=1) = \theta(1-\theta).
\end{align}
$$ {#eq-ev-var-bern}

Scriviamo $Y \sim \mbox{Bernoulli}(\theta)$ per indicare che la variabile casuale $Y$ ha una distribuzione Bernoulliana di parametro $\theta$.

::: {#exr-bernoulli-var-1}
Nel caso del lancio di una moneta equilibrata la variabile casuale di Bernoulli assume i valori $0$ e $1$. La distribuzione di massa di probabilità è pari a $\frac{1}{2}$ in corrispondenza di entrambi i valori. La funzione di distribuzione vale $\frac{1}{2}$ per $Y = 0$ e $1$ per $Y = 1$.
:::

## Una sequenza di prove Bernoulliane

La distribuzione binomiale è rappresentata dall'elenco di tutti i possibili numeri di successi $Y = \{0, 1, 2, \dots n\}$ che possono essere osservati in $n$ prove Bernoulliane indipendenti di probabilità $\theta$, a ciascuno dei quali è associata la relativa probabilità. Esempi di una distribuzione binomiale sono i risultati di una serie di lanci di una stessa moneta o di una serie di estrazioni da un'urna (con reintroduzione). La distribuzione binomiale di parametri $n$ e $\theta$ è in realtà una famiglia di distribuzioni: al variare dei parametri $\theta$ e $n$ variano le probabilità.

::: {#def-binomial-distrib}
La probabilità di ottenere $y$ successi e $n-y$ insuccessi in $n$ prove Bernoulliane è data dalla distribuzione binomiale:

$$
\begin{align}
P(Y=y) &= \binom{n}{y} \theta^{y} (1-\theta)^{n-y} \notag \\
&= \frac{n!}{y!(n-y)!} \theta^{y} (1-\theta)^{n-y}, 
\end{align}
$$ {#eq-binomialdistribution}

dove $n$ = numero di prove Bernoulliane, $\theta$ = probabilità di successo in ciascuna prova e $y$ = numero di successi.
:::

::: proof
L'@eq-binomialdistribution può essere derivata nel modo seguente. Indichiamo con $S$ il successo e con $I$ l'insuccesso di ciascuna prova. Una sequenza di $n$ prove Bernoulliane darà come esito una sequenza di $n$ elementi $S$ e $I$. Ad esempio, una sequenza che contiene $y$ successi è la seguente:

$$
\overbrace{SS\dots S}^\text{$y$ volte} \overbrace{II\dots I}^\text{$n-y$ volte}
$$

Essendo $\theta$ la probabilità di $S$ e $1-\theta$ la probabilità di $I$, la probabilità di ottenere la specifica sequenza riportata sopra è

$$
\begin{equation}
\overbrace{\theta \theta\dots \theta}^\text{$y$ volte} \overbrace{(1-\theta)(1-\theta)\dots (1-\theta)}^\text{$n-y$ volte} = \theta^y \cdot (1-\theta)^{n-y}.
\end{equation}
$$ {#eq-demo-bino-kernel}

Non siamo però interessati alla probabilità di una *specifica* sequenza di $S$ e $I$ ma, bensì, alla probabilità di osservare una *qualsiasi* sequenza di $y$ successi in $n$ prove. In altre parole, vogliamo la probabilità dell'unione di tutti gli eventi corrispondenti a $y$ successi in $n$ prove.

È immediato notare che una qualsiasi altra sequenza contenente esattamente $y$ successi avrà sempre come probabilità $\theta^y \cdot (1-\theta)^{n-y}$: il prodotto infatti resta costante anche se cambia l'ordine dei fattori.[^022_discr_rv_distr-1] Per trovare il risultato cercato dobbiamo moltiplicare l'@eq-demo-bino-kernel per il numero di sequenze possibili di $y$ successi in $n$ prove.

Il numero di sequenze che contengono esattamente $y$ successi in $n$ prove. La risposta è fornita dal coefficiente binomiale:

$$
\begin{equation}
\binom{n}{y} = \frac{n!}{y!(n-y)!},
\end{equation}
$$ {#eq-binomial-coefficient}

dove il simbolo $n!$ si legge $n$ fattoriale ed è uguale al prodotto di $n$ numeri interi decrescenti a partire da $n$. Per definizione $0! = 1$.

Essendo la probabilità dell'unione di $K$ elementi incompatibili uguale alla somma delle loro rispettive probabilità, e dato che le sequenze di $y$ successi in $n$ prove hanno tutte la stessa probabilità, per trovare la formula della distributione binomiale @eq-binomialdistribution è sufficiente moltiplicare l'@eq-demo-bino-kernel per l'@eq-binomial-coefficient.
:::

[^022_discr_rv_distr-1]: Viene detta *scambiabilità* la proprietà per cui l'ordine con cui compiamo le osservazioni è irrilevante per l'assegnazione delle probabilità.

La distribuzione di probabilità di alcune distribuzioni binomiali, per due valori di $n$ e $\theta$, è fornita nella @fig-example-binomial-distr.

```{r fig-example-binomial-distr, echo=FALSE, message=FALSE, warning=FALSE, error=FALSE, fig.cap = "Alcune distribuzioni binomiali. Nella figura, il parametro $\\theta$ è indicato con $p$."}
library("scales")
distribution_plot <- function(dset, dfun, darg, name) {
  theme_update(legend.position = c(0.8, 0.8))
  gg <- ggplot(dset, aes(x = x))
  for (index in 1:length(darg)) {
    iarg <- darg[[index]]
    params <- names(iarg)
    params <- sapply(params,
      switch,
      "size" = "n",
      "prob" = "p",
      "lambda" = "lambda",
      "shape" = "alpha",
      "rate" = "beta",
      "mean" = "mu",
      "sd" = "sigma",
      "meanlog" = "mu",
      "sdlog" = "sigma"
    )
    label <- paste(paste(params, unlist(iarg), sep = "=="), 
                   collapse = "*','~~")
    gg <- gg + stat_function(
      data = dset %>% mutate(label = label),
      aes(colour = label),
      size = 0.75,
      fun = dfun,
      n = nrow(dset),
      args = iarg,
      geom = "path"
    )
    if (grepl("poisson|binomial", name)) {
      gg <- gg + stat_function(
        data = dset %>% mutate(label = label),
        aes(colour = label),
        fun = dfun,
        n = nrow(dset),
        args = iarg,
        geom = "point"
      )
    }
  }
  gg <- gg + scale_fill_viridis(
    option = "E",
    name = "Parameters",
    labels = parse_format()
  )
  gg <- gg + labs(x = "\ny", y = "f(y)\n")
  print(gg)
}

binomial_plot <- function() {
  dset <- tibble(x = seq(0, 20))
  darg <- list(
    list(size = 10, prob = 0.5),
    list(size = 10, prob = 0.1),
    list(size = 20, prob = 0.5),
    list(size = 20, prob = 0.1)
  )
  distribution_plot(dset, dbinom, darg, "binomial")
}
binomial_plot()
```

::: {#exr-binomial-distr-1}
Usando l'@eq-binomialdistribution, si trovi la probabilità di $y = 2$ successi in $n = 4$ prove Bernoulliane indipendenti con $\theta = 0.2$
:::

::: solution
Abbiamo

$$
\begin{aligned}
P(Y=2) &= \frac{4!}{2!(4-2)!} 0.2^{2} (1-0.2)^{4-2} \notag  \\
 &= \frac{4 \cdot 3 \cdot 2 \cdot 1}{(2 \cdot 1)(2 \cdot 1)}
0.2^{2} 0.8^{2} = 0.1536. \notag
\end{aligned}
$$

Ripetendo i calcoli per i valori $y = 0, \dots, 4$ troviamo la distribuzione binomiale di parametri $n = 4$ e $\theta = 0.2$:

|  y  | P(Y = y) |
|:---:|:--------:|
|  0  |  0.4096  |
|  1  |  0.4096  |
|  2  |  0.1536  |
|  3  |  0.0256  |
|  4  |  0.0016  |
| sum |   1.0    |

Lo stesso risultato si può trovare in Python.

```{python}
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import binom
```

```{python}
n = 4
p = 0.2
x = np.arange(0, n+1)
x
```

```{python}
binomial_pmf = binom.pmf(x, n, p)
print(binomial_pmf)
```
:::

::: {#exr-binomial-distr-2}
Lanciando $5$ volte una moneta onesta, qual è la probabilità che esca testa almeno tre volte?
:::

::: solution
In Python, la soluzione si trova con

```{python}
binom.pmf(3, n=5, p=0.5) + binom.pmf(4, n=5, p=0.5) +  binom.pmf(5, n=5, p=0.5)
```

Alternativamente, possiamo trovare la probabilità dell'evento complementare a quello definito dalla funzione di ripartizione.

```{python}
1 - binom.cdf(2, n=5, p=0.5)
```
:::

### Valore atteso e deviazione standard

La media (numero atteso di successi in $n$ prove) e la deviazione standard di una distribuzione binomiale si trovano nel modo seguente:

$$
\begin{align}
\mu    &= n\theta,  \notag \\
\sigma &= \sqrt{n\theta(1-\theta)}.
\end{align}
$$ {#eq-mean-var-binomial}

::: proof
Essendo $Y$ la somma di $n$ prove Bernoulliane indipendenti $Y_i$, è facile vedere che

```{=tex}
\begin{align}
\mathbb{E}(Y) &= \mathbb{E}\left( \sum_{i=1}^n Y_i \right) = \sum_{i=1}^n \mathbb{E}(Y_i) = n\theta, \\
\mathbb{V}(Y) &= \mathbb{V} \left( \sum_{i=1}^n Y_i \right) = \sum_{i=1}^n \mathbb{V}(Y_i) = n \theta (1-\theta).
\end{align}
```
:::

::: {#exr-mean-var-binomial-1}
Si trovino il valore atteso e la varianza del lancio di quattro monete con probabilità di successo pari a $\theta = 0.2$.
:::

::: solution
Il valore atteso è $\mu = n \theta = 4 \cdot 0.2 = 0.8.$ Ciò significa che, se l'esperimento casuale venisse ripetuto infinite volte, l'esito testa verrebbe osservato un numero medio di volte pari a 0.8. La varianza è $n \theta (1-\theta) = 4 \cdot 0.2 \cdot (1 - 0.2) = 0.64$.
:::

## Distribuzione discreta uniforme

Una distribuzione discreta uniforme è una distribuzione di probabilità discreta che è uniforme su un insieme, ovvero che attribuisce ad ogni elemento dell'insieme discreto e finito $S$ su cui è definita la stessa probabilità $p$ di verificarsi.

Si consideri una variabile casuale $X$ con supporto $1, 2, \dots, m$. Un esperimento casuale in cui si verifica questa distribuzione è la scelta casuale di un intero compreso tra 1 e $m$ inclusi. Sia $X$ il numero scelto. Allora

$$
P(X = x) = \frac{1}{m}, \quad x = 1, \dots, m.
$$

Il valore atteso di $X$ è

$$
\mathbb{E}(X) = \sum_{x=1}^m x f_X(x) = \sum_{x=1}^m x \frac{1}{m} = \frac{1}{m} (1 + 2 + \dots + m) = \frac{m+1}{2},
$$

dove abbiamo utilizzato l'identità $1+2+···+m = m(m+1)/2$.

Per trovare la varianza, prima calcoliamo

$$
\mathbb{E}(X^2) = \frac{1}{m} \sum_{x=1}^m x^2,
$$ e poi troviamo

$$
\mathbb{V}(X) = \mathbb{E}(X^2) - \left[\mathbb{E}(X)\right]^2.
$$

## Distribuzione beta-binomiale

La distribuzione beta-binomiale di parametri $N$, $\alpha$ e $\beta$ è una distribuzione discreta con una funzione di massa di probabilità uguale a

$$
\mbox{BetaBinomial}(y \mid N, \alpha, \beta) = \binom{N}{y} \frac{B(y + \alpha, N-y+\beta)}{B(\alpha, \beta)},
$$ {#eq-beta-binom-formula}

dove la funzione beta è $B(u, v) = \frac{\Gamma(u)\Gamma(v)}{\Gamma(u+v)}$.

## Commenti e considerazioni finali {.unnumbered}

La distribuzione binomiale è una distribuzione di probabilità discreta che descrive il numero di successi in un processo di Bernoulli, ovvero la variabile aleatoria $Y = Y_1 + \dots + Y_n$ che somma $n$ variabili casuali indipendenti di uguale distribuzione di Bernoulli $\mathcal{B}(\theta)$, ognuna delle quali può fornire due soli risultati: il successo con probabilità $\theta$ e l'insuccesso con probabilità $1 - \theta$.

La distribuzione binomiale è molto importante per le sue molte applicazioni. Nelle presenti dispense dedicate all'analisi bayesiana, è importante perché costituisce il fondamento del caso più semplice dell'*aggiornamento bayesiano*, ovvero il caso Beta-Binomiale. Il modello Beta-Binomiale fornisce un esempio paradigmatico dell'approccio bayesiano all'inferenza e sarà trattato in maniera analitica nei capitoli successivi. È dunque importante che le proprietà della distribuzione binomiale risultino ben chiare.
